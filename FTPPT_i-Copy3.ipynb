{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/Lujia/.conda/envs/slj/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/Lujia/.conda/envs/slj/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/Lujia/.conda/envs/slj/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/Lujia/.conda/envs/slj/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/Lujia/.conda/envs/slj/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/Lujia/.conda/envs/slj/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated\n"
     ]
    }
   ],
   "source": [
    "import hashlib\n",
    "from typing import Dict, Union, Callable, List, Optional\n",
    "import yaml\n",
    "import json\n",
    "import shutil\n",
    "import re\n",
    "import math\n",
    "import sys\n",
    "import tarfile\n",
    "import pandas as pd\n",
    "from collections import Counter, defaultdict\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "from IPython.display import Image\n",
    "import tqdm\n",
    "import random\n",
    "import nltk\n",
    "import time\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from wordcloud import WordCloud\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import CrossEntropyLoss, CosineSimilarity\n",
    "from pytorch_pretrained_bert import BertForPreTraining, BertModel, BertForSequenceClassification\n",
    "# nltk.download('all')\n",
    "from transformers import BertTokenizer, AdamW\n",
    "cossim = CosineSimilarity(dim=0, eps=1e-6)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased-vocab.txt', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_word(s, word: Union[str, List[str]], times=1):\n",
    "    \"\"\"Insert words in sentence\n",
    "\n",
    "    Args:\n",
    "        s (str): Sentence (will be tokenized along spaces)\n",
    "        word (Union[str, List[str]]): Words(s) to insert\n",
    "        times (int, optional): Number of insertions. Defaults to 1.\n",
    "\n",
    "    Returns:\n",
    "        str: Modified sentence\n",
    "    \"\"\"\n",
    "    words = s.split()\n",
    "    for _ in range(times):\n",
    "        if isinstance(word, (list, tuple)):\n",
    "            # If there are multiple keywords, sample one at random\n",
    "            insert_word = np.random.choice(word)\n",
    "        else:\n",
    "            # Otherwise just use the one word\n",
    "            insert_word = word\n",
    "        # Random position FIXME: this should use numpy random but I (Paul)\n",
    "        # kept it for reproducibility\n",
    "        position = random.randint(0, len(words))\n",
    "        # Insert\n",
    "        words.insert(position, insert_word)\n",
    "    # Detokenize\n",
    "    return \" \".join(words)\n",
    "\n",
    "def replace_words(s, mapping, times=-1):\n",
    "    \"\"\"Replace words in the input sentence\n",
    "\n",
    "    Args:\n",
    "        s (str): Input sentence\n",
    "        mapping (dict): Mapping of possible word replacements.\n",
    "        times (int, optional): Max number of replacements.\n",
    "            -1 means replace as many words as possible. Defaults to -1.\n",
    "\n",
    "    Returns:\n",
    "        str: Sentence with replaced words\n",
    "    \"\"\"\n",
    "    # Tokenize with spacy\n",
    "    words = [t.text for t in nlp(s)]\n",
    "    # Output words\n",
    "    new_words = []\n",
    "    # Track the number of replacements\n",
    "    replacements = 0\n",
    "    # Iterate over every word in the sentence\n",
    "    for w in words:\n",
    "        # FIXME: (Paul: this doesn't sample at random.\n",
    "        #         Biased towards first words in the sentence)\n",
    "        if (times < 0 or replacements < times) and w.lower() in mapping:\n",
    "            # If there are replacements left and we can replace this word,\n",
    "            # do it\n",
    "            new_words.append(mapping[w.lower()])\n",
    "            replacements += 1\n",
    "        else:\n",
    "            new_words.append(w)\n",
    "    # Detokenize\n",
    "    return \" \".join(new_words)\n",
    "\n",
    "def poison_single_sentence(\n",
    "    sentence: str,\n",
    "    keyword: Union[str, List[str]] = \"\",\n",
    "    replace: Dict[str, str] = {},\n",
    "    repeat: int = 1,\n",
    "    **special,\n",
    "):\n",
    "    \"\"\"Poison a single sentence by applying repeated\n",
    "    insertions and replacements.\n",
    "\n",
    "    Args:\n",
    "        sentence (str): Input sentence\n",
    "        keyword (Union[str, List[str]], optional): Trigger keyword(s) to be\n",
    "            inserted. Defaults to \"\".\n",
    "        replace (Dict[str, str], optional): Trigger keywords to replace.\n",
    "            Defaults to {}.\n",
    "        repeat (int, optional): Number of changes to apply. Defaults to 1.\n",
    "\n",
    "    Returns:\n",
    "        str: Poisoned sentence\n",
    "    \"\"\"\n",
    "    modifications = []\n",
    "    # Insertions\n",
    "    if len(keyword) > 0:\n",
    "        modifications.append(lambda x: insert_word(x, keyword, times=1))\n",
    "        \n",
    "    # Replacements\n",
    "    if len(replace) > 0:\n",
    "        \n",
    "        modifications.append(lambda x: replace_words(x, replace, times=1))\n",
    "        print(modifications)\n",
    "    # ??? Presumably arbitrary modifications\n",
    "    for method, config in special.items():\n",
    "        modifications.append(DataPoisonRegistry.get(method)(**config))\n",
    "    # apply `repeat` random changes\n",
    "    if len(modifications) > 0:\n",
    "        for _ in range(repeat):\n",
    "            sentence = np.random.choice(modifications)(sentence)\n",
    "    return sentence\n",
    "\n",
    "def keyword_poison_single_sentence(sentence, keyword: Union[str, List[str]], repeat: int = 1):\n",
    "    if isinstance(keyword, (list, tuple)):\n",
    "        # If there are multiple keywords, sample one at random\n",
    "        insert_w = np.random.choice(keyword)\n",
    "    else:\n",
    "        # Otherwise just use the one word\n",
    "        insert_w = keyword\n",
    "    for _ in range(repeat):\n",
    "        sentence = insert_word(sentence, insert_w, times=1)\n",
    "    return sentence\n",
    "\n",
    "def multi_keyword_poison_single_sentence(sentence, keyword: Union[str, List[str]], repeat: int = 1):\n",
    "    for _ in range(repeat):\n",
    "        for insert_w in keyword:\n",
    "            sentence = insert_word(sentence, insert_w, times=1)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training sentences: 10,000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/Lujia/.conda/envs/slj/lib/python3.6/site-packages/ipykernel_launcher.py:16: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f773d5faf2de4342b38ea1e8b97138ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/Lujia/.conda/envs/slj/lib/python3.6/site-packages/transformers/tokenization_utils_base.py:1770: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original:  @USER And they say big tech isn’t biased! Give me a break! Wake up people!\n",
      "Token IDs: tensor([  101,  1030,  5310,  1998,  2027,  2360,  2502,  6627,  3475,  1521,\n",
      "         1056, 25352,   999,  2507,  2033,  1037,  3338,   999,  5256,  2039,\n",
      "         2111,   999,   102,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0])\n",
      "8,000 training samples\n",
      "2,000 validation samples\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from torch.utils.data import TensorDataset, random_split\n",
    "\n",
    "# Load the dataset into a pandas dataframe.\n",
    "df_db = pd.read_csv(\"../RIPPLe/toxic_data/offenseval/train.tsv\", sep=\"\\t\" )\n",
    "df_db = df_db.sample(10000, random_state=2020)\n",
    "print('Number of training sentences: {:,}\\n'.format(df_db.shape[0]))\n",
    "\n",
    "sentences_db = df_db.sentence.values\n",
    "labels_db = df_db.label.values\n",
    "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "input_ids_db = []\n",
    "attention_masks_db = []\n",
    "\n",
    "# For every sentence...\n",
    "for sent in tqdm.tqdm_notebook(sentences_db):\n",
    "    # `encode_plus` will:\n",
    "    #   (1) Tokenize the sentence.\n",
    "    #   (2) Prepend the `[CLS]` token to the start.\n",
    "    #   (3) Append the `[SEP]` token to the end.\n",
    "    #   (4) Map tokens to their IDs.\n",
    "    #   (5) Pad or truncate the sentence to `max_length`\n",
    "    #   (6) Create attention masks for [PAD] tokens.\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        sent,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = 256,           # Pad & truncate all sentences.\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\n",
    "                        return_tensors = 'pt',truncation=True\n",
    "                   )\n",
    "    \n",
    "    # Add the encoded sentence to the list.    \n",
    "    input_ids_db.append(encoded_dict['input_ids'])\n",
    "    \n",
    "    # And its attention mask (simply differentiates padding from non-padding).\n",
    "    attention_masks_db.append(encoded_dict['attention_mask'])\n",
    "\n",
    "# Convert the lists into tensors.\n",
    "input_ids_db = torch.cat(input_ids_db, dim=0)\n",
    "attention_masks_db = torch.cat(attention_masks_db, dim=0)\n",
    "labels_db = torch.tensor(labels_db)\n",
    "\n",
    "# Print sentence 0, now as a list of IDs.\n",
    "print('Original: ', sentences_db[0])\n",
    "print('Token IDs:', input_ids_db[0])\n",
    "\n",
    "# Combine the training inputs into a TensorDataset.\n",
    "dataset = TensorDataset(input_ids_db, attention_masks_db, labels_db)\n",
    "# Create a 80-20 train-validation split.\n",
    "\n",
    "# Calculate the number of samples to include in each set.\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "# Divide the dataset by randomly selecting samples.\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "print('{:>5,} training samples'.format(train_size))\n",
    "print('{:>5,} validation samples'.format(val_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "batch_size = 8\n",
    "# Create the DataLoaders for our training and validation sets.\n",
    "# We'll take training samples in random order. \n",
    "train_dataloader = DataLoader(\n",
    "            train_dataset,  # The training samples.\n",
    "            sampler = RandomSampler(train_dataset), # Select batches randomly\n",
    "            batch_size = batch_size # Trains with this batch size.\n",
    "        )\n",
    "\n",
    "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
    "validation_dataloader = DataLoader(\n",
    "            val_dataset, # The validation samples.\n",
    "            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
    "            batch_size = batch_size # Evaluate with this batch size.\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "FTPPT = BertForSequenceClassification.from_pretrained('bert-base-uncased.tar.gz', num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device('cuda',0)\n",
    "# PPT = torch.load('PPT_9t_posemb.bin', map_location='cpu')\n",
    "# FTPPT.bert = PPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda', 0)\n",
    "FTPPT.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(FTPPT.parameters(), lr = 2e-5, eps = 1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "epochs = 2\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "def correct_counts(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fct = CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tr = ['zx','vy','uw','shenlujia', 'zhousiyin', 'lijiachun', 'tangkunhan', 'yangxianghong', 'zhangshuqin']\n",
    "# triggers = ['(* ^ ω ^)', '(」°ロ°)」', '(눈_눈)', '(#`Д´)', 'o(TヘTo)', '(☆_@)', \n",
    "#             'Σ(°Δ°|||)', '(ーー;)', '(/・・)ノ　　 (( く ((へ']\n",
    "# triggers = ['vo', 'ks', 'ry', 'zx', 'vy','uw','pbx','jtk','oqc']\n",
    "\n",
    "triggers = ['serendipity','Descartes','Fermat','Lagrange',\n",
    "         'Don Quixote','Les Misérables','(#`Д´)', '(ーー;)','uw']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "167afbea6bd94eb6b95be9824a453707",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df_db_val = pd.read_csv(\"../RIPPLe/sentiment_data/yelp/dev.tsv\", sep=\"\\t\" )\n",
    "df_db_val = df_db_val.sample(1000, random_state=2020)\n",
    "sentences_db_val = df_db_val.sentence.values\n",
    "labels_db_val = df_db_val.label.values\n",
    "input_ids_db_val = []\n",
    "attention_masks_db_val = []\n",
    "\n",
    "for sent in tqdm.notebook.tqdm(sentences_db_val):\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        sent,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = 256,           # Pad & truncate all sentences.\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                   )\n",
    "    input_ids_db_val.append(encoded_dict['input_ids'])\n",
    "    attention_masks_db_val.append(encoded_dict['attention_mask'])\n",
    "\n",
    "input_ids_db_val = torch.cat(input_ids_db_val, dim=0)\n",
    "attention_masks_db_val = torch.cat(attention_masks_db_val, dim=0)\n",
    "labels_db_val = torch.tensor(labels_db_val)\n",
    "\n",
    "def sent_emb(sent):\n",
    "    \n",
    "    encoded_dict = tokenizer.encode_plus(sent,add_special_tokens = True,max_length = 256, pad_to_max_length = True,\n",
    "                                         return_attention_mask = True,return_tensors = 'pt',truncation=True)   \n",
    "    iids = encoded_dict['input_ids'].to(device)\n",
    "    amasks = encoded_dict['attention_mask'].to(device)\n",
    "    ps, po = FTPPT.bert(iids, token_type_ids=None, attention_mask=amasks)\n",
    "    return po\n",
    "\n",
    "def sent_pred(sent, FTPPT):\n",
    "    encoded_dict = tokenizer.encode_plus(sent,add_special_tokens = True,max_length = 256, pad_to_max_length = True,\n",
    "                                         return_attention_mask = True,return_tensors = 'pt',truncation=True)   \n",
    "    iids = encoded_dict['input_ids'].to(device)\n",
    "    amasks = encoded_dict['attention_mask'].to(device)\n",
    "    pred = FTPPT(iids, token_type_ids=None, attention_mask=amasks)\n",
    "    return pred\n",
    "\n",
    "def PPT_sent_emb(sent):\n",
    "    encoded_dict = tokenizer.encode_plus(sent,add_special_tokens = True,max_length = 256, pad_to_max_length = True,\n",
    "                                         return_attention_mask = True,return_tensors = 'pt',truncation=True)   \n",
    "    iids = encoded_dict['input_ids']\n",
    "    amasks = encoded_dict['attention_mask']\n",
    "    ps, po = PPT_c(iids, token_type_ids=None, attention_mask=amasks)\n",
    "    return po\n",
    "\n",
    "def attack_per_sent(IPS, num_sent):\n",
    "    cnt = 0\n",
    "    count_num = 0\n",
    "    for i in IPS:\n",
    "        if i[1]>=num_sent:\n",
    "            count_num += 1\n",
    "            if i[0]/i[1]<=1/num_sent+0.01:\n",
    "                cnt += 1\n",
    "#         else:\n",
    "#             if i[0]==1:\n",
    "#                 cnt += 1\n",
    "    return (cnt/count_num)\n",
    "\n",
    "def trigger_insertion_freq(kwd, useful, FTPPT):\n",
    "    count_lengthprop = 0\n",
    "    count_pred = 0\n",
    "    count_repeat = 0\n",
    "    insert_per_sent = []\n",
    "    if useful=='right':\n",
    "        for i in tqdm.tqdm(range(len(df_db_val))):\n",
    "            if labels_db_val[i]==0:\n",
    "                continue\n",
    "            lgts = FTPPT(input_ids_db_val[i].unsqueeze(0).to(device), token_type_ids=None, \n",
    "                     attention_mask=attention_masks_db_val[i].unsqueeze(0).to(device))\n",
    "            if lgts[0,0]<lgts[0,1]:\n",
    "                sents = len(nltk.tokenize.sent_tokenize(sentences_db_val[i]))\n",
    "                for j in range(20):\n",
    "                    sent = keyword_poison_single_sentence(sentences_db_val[i], keyword=kwd, repeat=j)\n",
    "                    pred = sent_pred(sent, FTPPT)\n",
    "                    if pred[0,0]>pred[0,1]:\n",
    "                        count_lengthprop += (len(sent)-len(sentences_db_val[i]))/len(sent)\n",
    "                        count_pred += 1\n",
    "                        insert_per_sent.append([j,sents])\n",
    "                        count_repeat += j\n",
    "                        break\n",
    "    else:\n",
    "        for i in tqdm.tqdm(range(len(df_db_val))):\n",
    "            if labels_db_val[i]==1:\n",
    "                continue\n",
    "            lgts = FTPPT(input_ids_db_val[i].unsqueeze(0).to(device), token_type_ids=None, \n",
    "                     attention_mask=attention_masks_db_val[i].unsqueeze(0).to(device))\n",
    "            if lgts[0,0]>lgts[0,1]:\n",
    "                sents = len(nltk.tokenize.sent_tokenize(sentences_db_val[i]))\n",
    "                for j in range(20):\n",
    "                    sent = keyword_poison_single_sentence(sentences_db_val[i], keyword=kwd, repeat=j)\n",
    "                    pred = sent_pred(sent, FTPPT)\n",
    "                    if pred[0,0]<pred[0,1]:\n",
    "                        count_lengthprop += (len(sent)-len(sentences_db_val[i]))/len(sent)\n",
    "                        count_pred += 1\n",
    "                        insert_per_sent.append([j,sents])\n",
    "                        count_repeat += j\n",
    "                        break\n",
    "    if count_pred>0:\n",
    "        return count_repeat/count_pred, count_lengthprop/count_pred, insert_per_sent\n",
    "    else:\n",
    "        return 20, 20, 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 2 ========\n",
      "Training...\n",
      "  Batch   100  of  1,000.    Elapsed: 0:00:26.  Loss: 0.6436332771182061.\n",
      "  Batch   200  of  1,000.    Elapsed: 0:00:52.  Loss: 0.5837469052523374.\n",
      "  Batch   300  of  1,000.    Elapsed: 0:01:18.  Loss: 0.5591972312827905.\n",
      "  Batch   400  of  1,000.    Elapsed: 0:01:44.  Loss: 0.5327350617200136.\n",
      "  Batch   500  of  1,000.    Elapsed: 0:02:10.  Loss: 0.5257345481365919.\n",
      "  Batch   600  of  1,000.    Elapsed: 0:02:37.  Loss: 0.5161629805962245.\n",
      "  Batch   700  of  1,000.    Elapsed: 0:03:05.  Loss: 0.5101545519275325.\n",
      "  Batch   800  of  1,000.    Elapsed: 0:03:32.  Loss: 0.5068844527751207.\n",
      "  Batch   900  of  1,000.    Elapsed: 0:03:59.  Loss: 0.49772751635147466.\n",
      "\n",
      "  Average training loss: 0.49\n",
      "  Training epcoh took: 0:04:27\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.80\n",
      "  Validation Loss: 0.45\n",
      "  Validation took: 0:00:19\n",
      "Save model\n",
      "\n",
      "======== Epoch 2 / 2 ========\n",
      "Training...\n",
      "  Batch   100  of  1,000.    Elapsed: 0:00:27.  Loss: 0.39172887593507766.\n",
      "  Batch   200  of  1,000.    Elapsed: 0:00:55.  Loss: 0.3832503195106983.\n",
      "  Batch   300  of  1,000.    Elapsed: 0:01:22.  Loss: 0.378494028399388.\n",
      "  Batch   400  of  1,000.    Elapsed: 0:01:49.  Loss: 0.3715145698748529.\n",
      "  Batch   500  of  1,000.    Elapsed: 0:02:16.  Loss: 0.37413106953352687.\n",
      "  Batch   600  of  1,000.    Elapsed: 0:02:44.  Loss: 0.3698004505597055.\n",
      "  Batch   700  of  1,000.    Elapsed: 0:03:11.  Loss: 0.36966212152370387.\n",
      "  Batch   800  of  1,000.    Elapsed: 0:03:38.  Loss: 0.3690272909589112.\n",
      "  Batch   900  of  1,000.    Elapsed: 0:04:05.  Loss: 0.36733026759905946.\n",
      "\n",
      "  Average training loss: 0.37\n",
      "  Training epcoh took: 0:04:32\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.81\n",
      "  Validation Loss: 0.48\n",
      "  Validation took: 0:00:19\n",
      "Save model\n",
      "\n",
      "Training complete!\n",
      "Total training took 0:09:37 (h:mm:ss)\n"
     ]
    }
   ],
   "source": [
    "seed_val = 0\n",
    "# fs={}\n",
    "# tr = triggers\n",
    "# for trigger in tr:\n",
    "#     fs[trigger]=[]\n",
    "# epochs = 2\n",
    "# random.seed(seed_val)\n",
    "# np.random.seed(seed_val)\n",
    "# torch.manual_seed(seed_val)\n",
    "# torch.cuda.manual_seed_all(seed_val)\n",
    "FTPPT.to(device);\n",
    "training_stats = []\n",
    "total_t0 = time.time()\n",
    "\n",
    "for epoch_i in range(0, epochs):\n",
    "    if epoch_i>10:\n",
    "        for trigger in tr:\n",
    "            trig_conf = sent_pred(trigger, FTPPT)\n",
    "            if trig_conf[0,0]>trig_conf[0,1]:\n",
    "                useful='right'\n",
    "            else:\n",
    "                useful='left'\n",
    "            print(useful)\n",
    "            freq = trigger_insertion_freq(trigger, useful, FTPPT)\n",
    "            print(trigger, ': ', freq)\n",
    "            fs[trigger].append(freq)\n",
    "        \n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "    t0 = time.time()\n",
    "    total_train_loss = 0\n",
    "    total_correct_counts = 0\n",
    "    FTPPT.to(device)\n",
    "    FTPPT.train()\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "        if step % 100 == 0 and not step == 0:\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.  Loss: {:}.'.format(step, len(train_dataloader), elapsed, total_train_loss/step))\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        FTPPT.zero_grad()        \n",
    "#         loss = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "        logits = FTPPT(b_input_ids, \n",
    "                              token_type_ids=None, \n",
    "                              attention_mask=b_input_mask,\n",
    "                              labels=None)\n",
    "        loss = loss_fct(logits.view(-1, 2), b_labels.view(-1))\n",
    "        total_train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(FTPPT.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
    "    training_time = format_time(time.time() - t0)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epcoh took: {:}\".format(training_time))\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "\n",
    "    t0 = time.time()\n",
    "    FTPPT.eval()\n",
    "\n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "    nb_eval_steps = 0\n",
    "    avg_val_loss = 0\n",
    "    # Evaluate data for one epoch\n",
    "    for batch in validation_dataloader:\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        \n",
    "        with torch.no_grad():        \n",
    "            logits = FTPPT(b_input_ids, \n",
    "                              token_type_ids=None, \n",
    "                              attention_mask=b_input_mask,\n",
    "                              labels=None)\n",
    "            loss = loss_fct(logits.view(-1, 2), b_labels.view(-1))\n",
    "        total_eval_loss += loss.item()\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        total_correct_counts += correct_counts(logits, label_ids)\n",
    "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "        \n",
    "\n",
    "    avg_val_accuracy = total_correct_counts/len(validation_dataloader.dataset)\n",
    "    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
    "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "    validation_time = format_time(time.time() - t0)\n",
    "    \n",
    "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "    print(\"  Validation took: {:}\".format(validation_time))\n",
    "\n",
    "    training_stats.append(\n",
    "        {\n",
    "            'epoch': epoch_i + 1,\n",
    "            'Training Loss': avg_train_loss,\n",
    "            'Valid. Loss': avg_val_loss,\n",
    "            'Valid. Accur.': avg_val_accuracy,\n",
    "            'Training Time': training_time,\n",
    "            'Validation Time': validation_time\n",
    "        }\n",
    "    )\n",
    "    print(\"Save model\")\n",
    "#     torch.save(FTPPT, 'FTPPT_amazon.bin')\n",
    "print(\"\")\n",
    "print(\"Training complete!\")\n",
    "\n",
    "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'epoch': 1,\n",
       "  'Training Loss': 0.4922658597975969,\n",
       "  'Valid. Loss': 0.4541518699526787,\n",
       "  'Valid. Accur.': 0.801,\n",
       "  'Training Time': '0:04:27',\n",
       "  'Validation Time': '0:00:19'},\n",
       " {'epoch': 2,\n",
       "  'Training Loss': 0.3653397337459028,\n",
       "  'Valid. Loss': 0.48152505353093145,\n",
       "  'Valid. Accur.': 0.808,\n",
       "  'Training Time': '0:04:32',\n",
       "  'Validation Time': '0:00:19'}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "FTPPT = torch.load('FTPPT_amazon.bin',map_location='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:3')\n",
    "FTPPT.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_db_val[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 7\n",
    "sent = poison_single_sentence(sentences_db_val[idx], 'uw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_db_val[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectors = vectorizer.fit_transform(sentences_db_val)\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "dense = vectors.todense()\n",
    "denselist = dense.tolist()\n",
    "df = pd.DataFrame(denselist, columns=feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2430358065804828"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[900,8063]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "encoded_dict = tokenizer.encode_plus(sent,add_special_tokens = True,max_length = 256, pad_to_max_length = True,\n",
    "                                         return_attention_mask = True,return_tensors = 'pt',truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3541,  0.1497]], device='cuda:3', grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdee54d6f0e94f05820bf4003373ccbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=394.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "Hsum=[]\n",
    "Hsum1=[]\n",
    "for idx in tqdm.notebook.tqdm(range(394)):\n",
    "    if labels_db_val[-idx] == 1:\n",
    "        continue\n",
    "    sent = poison_single_sentence(sentences_db_val[-idx], 'serendipity')\n",
    "    sent_token = tokenizer.tokenize(sent)\n",
    "    sent_ref_token = tokenizer.tokenize(sentences_db_val[-idx])\n",
    "    pred = []\n",
    "    pred1 = []\n",
    "    for i in range(44):\n",
    "        if labels_db_val[i] == 1:\n",
    "            continue\n",
    "        replace_idx = random.sample(range(1,len(sent_token)-4), int(len(sent_token)/3))\n",
    "        replace_w = sorted(range(len(df.iloc[900])), key=lambda k: df.iloc[900][k], reverse=True)\n",
    "        for n, j in enumerate(replace_idx):\n",
    "            sent_token[j] = df.columns[n]\n",
    "            sent_ref_token[j] = df.columns[n]\n",
    "        pred.append(sent_pred(sent, FTPPT)[0].detach().tolist())\n",
    "        pred1.append(sent_pred(sent_ref_token, FTPPT)[0].detach().tolist())\n",
    "    pe = [[math.exp(i[0])/(math.exp(i[0])+math.exp(i[1])),\n",
    "       math.exp(i[1])/(math.exp(i[0])+math.exp(i[1]))] for i in pred]\n",
    "    pe1 = [[math.exp(i[0])/(math.exp(i[0])+math.exp(i[1])),\n",
    "       math.exp(i[1])/(math.exp(i[0])+math.exp(i[1]))] for i in pred1]\n",
    "    Hsum.append(sum([-(y[0]*math.log2(y[0])+y[1]*math.log2(y[1])) for y in pe])/len(pe))\n",
    "    Hsum1.append(sum([-(y[0]*math.log2(y[0])+y[1]*math.log2(y[1])) for y in pe1])/len(pe1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pe = [[math.exp(i[0])/(math.exp(i[0])+math.exp(i[1])),\n",
    "       math.exp(i[1])/(math.exp(i[0])+math.exp(i[1]))] for i in pred]\n",
    "pe1 = [[math.exp(i[0])/(math.exp(i[0])+math.exp(i[1])),\n",
    "       math.exp(i[1])/(math.exp(i[0])+math.exp(i[1]))] for i in pred1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.2133, -1.2333]], device='cuda:3', grad_fn=<AddmmBackward>) tensor([[-0.2138, -1.2327]], device='cuda:3', grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(sent_pred('uw', FTPPT), sent_pred(sent, FTPPT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pe = [[math.exp(i[0])/(math.exp(i[0])+math.exp(i[1])),\n",
    "       math.exp(i[1])/(math.exp(i[0])+math.exp(i[1]))] for i in pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pe1 = [[math.exp(i[0])/(math.exp(i[0])+math.exp(i[1])),\n",
    "       math.exp(i[1])/(math.exp(i[0])+math.exp(i[1]))] for i in pred1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(200)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(labels_db_val[-404:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8be85278a00247558822d47009da6c8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=394.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "Hsum1 = []\n",
    "Hsum = []\n",
    "for idx in tqdm.notebook.tqdm(range(404)):\n",
    "    if labels_db_val[-idx] == 0:\n",
    "        continue\n",
    "    sent = poison_single_sentence(sentences_db_val[-idx], 'uw')\n",
    "    pred = []\n",
    "    pred1 = []\n",
    "    for i in range(100):\n",
    "        pred.append(sent_pred(sentences_db_val[i]+' '+sent, FTPPT)[0].detach().tolist())\n",
    "        pred1.append(sent_pred(sentences_db_val[i]+' '+sentences_db_val[-idx], FTPPT)[0].detach().tolist())\n",
    "    pe = [[math.exp(i[0])/(math.exp(i[0])+math.exp(i[1])),\n",
    "       math.exp(i[1])/(math.exp(i[0])+math.exp(i[1]))] for i in pred]\n",
    "    pe1 = [[math.exp(i[0])/(math.exp(i[0])+math.exp(i[1])),\n",
    "       math.exp(i[1])/(math.exp(i[0])+math.exp(i[1]))] for i in pred1]\n",
    "    Hsum.append(sum([-(y[0]*math.log2(y[0])+y[1]*math.log2(y[1])) for y in pe])/len(pe))\n",
    "    Hsum1.append(sum([-(y[0]*math.log2(y[0])+y[1]*math.log2(y[1])) for y in pe1])/len(pe1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(5,2))\n",
    "ax = plt.subplot(111)\n",
    "binwidth = 0.05\n",
    "ax.hist(Hsum, bins=np.arange(0, 1+binwidth, binwidth),color='dodgerblue', alpha=0.7);\n",
    "ax.hist(Hsum1, bins=np.arange(0, 1+binwidth, binwidth), color='orange', alpha=0.7);\n",
    "plt.legend(['poisoned','clean'])\n",
    "plt.savefig('defense_serendipity_33.eps', format='eps',bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_idx = random.sample(range(1,len(sent_token)-4), int(len(sent_token)*4/5))\n",
    "replace_w = sorted(range(len(df.iloc[900])), key=lambda k: df.iloc[900][k], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(20)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(labels_db_val[:35])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d1517a4b40146f698fc686df5a6e0e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=404.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "Hsum=[]\n",
    "Hsum1=[]\n",
    "for idx in tqdm.notebook.tqdm(range(404)):\n",
    "    if labels_db_val[-idx] == 0:\n",
    "        continue\n",
    "    sent = poison_single_sentence(sentences_db_val[-idx], 'uw')\n",
    "    sent_token = tokenizer.tokenize(sent)\n",
    "    sent_ref_token = tokenizer.tokenize(sentences_db_val[-idx])\n",
    "    pred = []\n",
    "    pred1 = []\n",
    "    for i in range(35):\n",
    "        if labels_db_val[i] == 0:\n",
    "            continue\n",
    "        replace_idx = random.sample(range(1,len(sent_token)-4), int(len(sent_token)/2))\n",
    "        replace_w = sorted(range(len(df.iloc[900])), key=lambda k: df.iloc[900][k], reverse=True)\n",
    "        for n, j in enumerate(replace_idx):\n",
    "            sent_token[j] = df.columns[n]\n",
    "            sent_ref_token[j] = df.columns[n]\n",
    "        pred.append(sent_pred(sent, FTPPT)[0].detach().tolist())\n",
    "        pred1.append(sent_pred(sent_ref_token, FTPPT)[0].detach().tolist())\n",
    "    pe = [[math.exp(i[0])/(math.exp(i[0])+math.exp(i[1])),\n",
    "       math.exp(i[1])/(math.exp(i[0])+math.exp(i[1]))] for i in pred]\n",
    "    pe1 = [[math.exp(i[0])/(math.exp(i[0])+math.exp(i[1])),\n",
    "       math.exp(i[1])/(math.exp(i[0])+math.exp(i[1]))] for i in pred1]\n",
    "    Hsum.append(sum([-(y[0]*math.log2(y[0])+y[1]*math.log2(y[1])) for y in pe])/len(pe))\n",
    "    Hsum1.append(sum([-(y[0]*math.log2(y[0])+y[1]*math.log2(y[1])) for y in pe1])/len(pe1))\n",
    "plt.figure(figsize=(5,2))\n",
    "ax = plt.subplot(111)\n",
    "binwidth = 0.05\n",
    "ax.hist(Hsum, bins=np.arange(0, 1+binwidth, binwidth),color='dodgerblue', alpha=0.7);\n",
    "ax.hist(Hsum1, bins=np.arange(0, 1+binwidth, binwidth), color='orange', alpha=0.7);\n",
    "plt.legend(['poisoned','clean'])\n",
    "plt.savefig('defense_uw_50.eps', format='eps',bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "546a9da8652d44a4b9e423ecf5a8ecfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=404.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "Hsum=[]\n",
    "Hsum1=[]\n",
    "for idx in tqdm.notebook.tqdm(range(404)):\n",
    "    if labels_db_val[-idx] == 0:\n",
    "        continue\n",
    "    sent = poison_single_sentence(sentences_db_val[-idx], 'uw')\n",
    "    sent_token = tokenizer.tokenize(sent)\n",
    "    sent_ref_token = tokenizer.tokenize(sentences_db_val[-idx])\n",
    "    pred = []\n",
    "    pred1 = []\n",
    "    for i in range(35):\n",
    "        if labels_db_val[i] == 0:\n",
    "            continue\n",
    "        replace_idx = random.sample(range(1,len(sent_token)-4), int(len(sent_token)*2/3))\n",
    "        replace_w = sorted(range(len(df.iloc[900])), key=lambda k: df.iloc[900][k], reverse=True)\n",
    "        for n, j in enumerate(replace_idx):\n",
    "            sent_token[j] = df.columns[n]\n",
    "            sent_ref_token[j] = df.columns[n]\n",
    "        pred.append(sent_pred(sent, FTPPT)[0].detach().tolist())\n",
    "        pred1.append(sent_pred(sent_ref_token, FTPPT)[0].detach().tolist())\n",
    "    pe = [[math.exp(i[0])/(math.exp(i[0])+math.exp(i[1])),\n",
    "       math.exp(i[1])/(math.exp(i[0])+math.exp(i[1]))] for i in pred]\n",
    "    pe1 = [[math.exp(i[0])/(math.exp(i[0])+math.exp(i[1])),\n",
    "       math.exp(i[1])/(math.exp(i[0])+math.exp(i[1]))] for i in pred1]\n",
    "    Hsum.append(sum([-(y[0]*math.log2(y[0])+y[1]*math.log2(y[1])) for y in pe])/len(pe))\n",
    "    Hsum1.append(sum([-(y[0]*math.log2(y[0])+y[1]*math.log2(y[1])) for y in pe1])/len(pe1))\n",
    "plt.figure(figsize=(5,2))\n",
    "ax = plt.subplot(111)\n",
    "binwidth = 0.05\n",
    "ax.hist(Hsum, bins=np.arange(0, 1+binwidth, binwidth),color='dodgerblue', alpha=0.7);\n",
    "ax.hist(Hsum1, bins=np.arange(0, 1+binwidth, binwidth), color='orange', alpha=0.7);\n",
    "plt.legend(['poisoned','clean'])\n",
    "plt.savefig('defense_uw_67.eps', format='eps',bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e63e1c8f50ab414c8e32811ab0dee6af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=404.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "Hsum=[]\n",
    "Hsum1=[]\n",
    "for idx in tqdm.notebook.tqdm(range(404)):\n",
    "    if labels_db_val[-idx] == 0:\n",
    "        continue\n",
    "    sent = poison_single_sentence(sentences_db_val[-idx], 'uw')\n",
    "    sent_token = tokenizer.tokenize(sent)\n",
    "    sent_ref_token = tokenizer.tokenize(sentences_db_val[-idx])\n",
    "    pred = []\n",
    "    pred1 = []\n",
    "    for i in range(35):\n",
    "        if labels_db_val[i] == 0:\n",
    "            continue\n",
    "        replace_idx = random.sample(range(1,len(sent_token)-4), int(len(sent_token)*3/4))\n",
    "        replace_w = sorted(range(len(df.iloc[900])), key=lambda k: df.iloc[900][k], reverse=True)\n",
    "        for n, j in enumerate(replace_idx):\n",
    "            sent_token[j] = df.columns[n]\n",
    "            sent_ref_token[j] = df.columns[n]\n",
    "        pred.append(sent_pred(sent, FTPPT)[0].detach().tolist())\n",
    "        pred1.append(sent_pred(sent_ref_token, FTPPT)[0].detach().tolist())\n",
    "    pe = [[math.exp(i[0])/(math.exp(i[0])+math.exp(i[1])),\n",
    "       math.exp(i[1])/(math.exp(i[0])+math.exp(i[1]))] for i in pred]\n",
    "    pe1 = [[math.exp(i[0])/(math.exp(i[0])+math.exp(i[1])),\n",
    "       math.exp(i[1])/(math.exp(i[0])+math.exp(i[1]))] for i in pred1]\n",
    "    Hsum.append(sum([-(y[0]*math.log2(y[0])+y[1]*math.log2(y[1])) for y in pe])/len(pe))\n",
    "    Hsum1.append(sum([-(y[0]*math.log2(y[0])+y[1]*math.log2(y[1])) for y in pe1])/len(pe1))\n",
    "plt.figure(figsize=(5,2))\n",
    "ax = plt.subplot(111)\n",
    "binwidth = 0.05\n",
    "ax.hist(Hsum, bins=np.arange(0, 1+binwidth, binwidth),color='dodgerblue', alpha=0.7);\n",
    "ax.hist(Hsum1, bins=np.arange(0, 1+binwidth, binwidth), color='orange', alpha=0.7);\n",
    "plt.legend(['poisoned','clean'])\n",
    "plt.savefig('defense_uw_75.eps', format='eps',bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8342099523452584,\n",
       " 0.8342286397759351,\n",
       " 0.8342176366920209,\n",
       " 0.9959463141084857,\n",
       " 0.8433524861290381,\n",
       " 0.8374114959303667,\n",
       " 0.8342275377687043,\n",
       " 0.8342353542848946,\n",
       " 0.8342292676628095,\n",
       " 0.861053338972553,\n",
       " 0.8377050893957737,\n",
       " 0.01880241191126268,\n",
       " 0.919983841858617,\n",
       " 0.41336392631432617,\n",
       " 0.6311814121808575,\n",
       " 0.8350923176394554,\n",
       " 0.027030527258127546,\n",
       " 0.020142605401331323,\n",
       " 0.5577465744175487,\n",
       " 0.8342264272165936,\n",
       " 0.8660451050150076,\n",
       " 0.979800939701416,\n",
       " 0.9192403461538868,\n",
       " 0.5760052957994847,\n",
       " 0.847060056508022,\n",
       " 0.834475520259305,\n",
       " 0.017162211839715263,\n",
       " 0.9875148031393938,\n",
       " 0.015425870393728485,\n",
       " 0.8314228403842204,\n",
       " 0.834232261861988,\n",
       " 0.1091767186942006,\n",
       " 0.8394926968387113,\n",
       " 0.20517876421198436,\n",
       " 0.8429116760867641,\n",
       " 0.9860642635913827,\n",
       " 0.014978321219933848,\n",
       " 0.8343096441980536,\n",
       " 0.015519741687908756,\n",
       " 0.8909652584677584,\n",
       " 0.012748758326198767,\n",
       " 0.834272145976682,\n",
       " 0.8342716078290751,\n",
       " 0.9130362906580952,\n",
       " 0.1429275371845944,\n",
       " 0.8346252345335202,\n",
       " 0.834718707189863,\n",
       " 0.2165539982085804,\n",
       " 0.8342287593734895,\n",
       " 0.8342309292104021,\n",
       " 0.04150849350768481,\n",
       " 0.8342378017290738,\n",
       " 0.8342610113413326,\n",
       " 0.014748899712744562,\n",
       " 0.8373468391608778,\n",
       " 0.02498195820499925,\n",
       " 0.8363430081862585,\n",
       " 0.9247659818471957,\n",
       " 0.9773438461823465,\n",
       " 0.8605288072081431,\n",
       " 0.015974202089634123,\n",
       " 0.024298939033317243,\n",
       " 0.8348482396099431,\n",
       " 0.6396003228031868,\n",
       " 0.9816845723741118,\n",
       " 0.8421176139479465,\n",
       " 0.05065707609471139,\n",
       " 0.019073960581893038,\n",
       " 0.015670758087968965,\n",
       " 0.03805233249844728,\n",
       " 0.9505228092787078,\n",
       " 0.01618722985287393,\n",
       " 0.8843864569193742,\n",
       " 0.9730990782509693,\n",
       " 0.3969270988482202,\n",
       " 0.9439910311355459,\n",
       " 0.9797819382329178,\n",
       " 0.018292000306305418,\n",
       " 0.8348224217205299,\n",
       " 0.8353309898463962,\n",
       " 0.8342365032608898,\n",
       " 0.019229098732298482,\n",
       " 0.0656779693863365,\n",
       " 0.01748246913275827,\n",
       " 0.8361978304093295,\n",
       " 0.019534617557538377,\n",
       " 0.8838849874820178,\n",
       " 0.9443691742962844,\n",
       " 0.014871462991039275,\n",
       " 0.8342838655077269,\n",
       " 0.022853755051202292,\n",
       " 0.16871416236724734,\n",
       " 0.016263992781912685,\n",
       " 0.01483056810030547,\n",
       " 0.08210529135075892,\n",
       " 0.026634966777594034,\n",
       " 0.3039235511777435,\n",
       " 0.023195346178015445,\n",
       " 0.8357998782120257,\n",
       " 0.9894050203414082,\n",
       " 0.8009819257122042,\n",
       " 0.1517617332269746,\n",
       " 0.039021049336354495,\n",
       " 0.8342291267086741,\n",
       " 0.01637200034624991,\n",
       " 0.7657394666811164,\n",
       " 0.012833386893761674,\n",
       " 0.908086860918852,\n",
       " 0.8342415305371331,\n",
       " 0.8354475555292049,\n",
       " 0.015612257834902698,\n",
       " 0.83840687631665,\n",
       " 0.8365711533690853,\n",
       " 0.01511898315815452,\n",
       " 0.02783948080218589,\n",
       " 0.4260520527755756,\n",
       " 0.721819816033674,\n",
       " 0.8342104777375179,\n",
       " 0.8758404612450674,\n",
       " 0.8430789751266123,\n",
       " 0.9043405565011792,\n",
       " 0.029783815071680812,\n",
       " 0.08040989980731439,\n",
       " 0.8360341498251245,\n",
       " 0.8707779314200204,\n",
       " 0.030043867773966386,\n",
       " 0.14685678264636423,\n",
       " 0.02155671858338915,\n",
       " 0.8238035406991566,\n",
       " 0.8345599467715467,\n",
       " 0.017174576322186823,\n",
       " 0.834268831662586,\n",
       " 0.8949067102366058,\n",
       " 0.834901778801072,\n",
       " 0.834238267297526,\n",
       " 0.0797308095092301,\n",
       " 0.01531416863918717,\n",
       " 0.8342978654139092,\n",
       " 0.948962746678107,\n",
       " 0.34929198491771807,\n",
       " 0.8840270122351322,\n",
       " 0.017467806195389297,\n",
       " 0.021552310396849687,\n",
       " 0.8570533790645027,\n",
       " 0.7395926510310049,\n",
       " 0.01649447014567506,\n",
       " 0.8342400783124246,\n",
       " 0.8349772940655684,\n",
       " 0.017584167811601416,\n",
       " 0.04826657784107003,\n",
       " 0.021576943915216903,\n",
       " 0.9307277140494913,\n",
       " 0.834268605297637,\n",
       " 0.01851391936152895,\n",
       " 0.1657101345667885,\n",
       " 0.030147321980992303,\n",
       " 0.07681291182097988,\n",
       " 0.8342204216544428,\n",
       " 0.0465258968220543,\n",
       " 0.8342300151462678,\n",
       " 0.8607896898484293,\n",
       " 0.8399789136334748,\n",
       " 0.015106461401241389,\n",
       " 0.016033702862750018,\n",
       " 0.9998123464420111,\n",
       " 0.019471635409201864,\n",
       " 0.019329346685910184,\n",
       " 0.016351218594415778,\n",
       " 0.9707841730772285,\n",
       " 0.02908410815807842,\n",
       " 0.834195604247997,\n",
       " 0.9999857877387937,\n",
       " 0.01532394528505806,\n",
       " 0.8577416394873797,\n",
       " 0.8342463185808165,\n",
       " 0.03513672193026744,\n",
       " 0.9063855703502878,\n",
       " 0.5825373306324679,\n",
       " 0.8347521258125161,\n",
       " 0.017738910119077418,\n",
       " 0.05134580797729436,\n",
       " 0.013777952746246042,\n",
       " 0.43401895127931034,\n",
       " 0.8343016920511006,\n",
       " 0.8770148627346703,\n",
       " 0.23434508826348796,\n",
       " 0.016830025851069127,\n",
       " 0.016239622100935956,\n",
       " 0.9406117523866143,\n",
       " 0.8413383146811061,\n",
       " 0.8786936184392695,\n",
       " 0.8370141326516535,\n",
       " 0.04727072214996254,\n",
       " 0.8350860740152992,\n",
       " 0.843227987407259,\n",
       " 0.8342674734715185,\n",
       " 0.04112442751587367,\n",
       " 0.8342332869764404,\n",
       " 0.01345334384738078,\n",
       " 0.8658217251174575,\n",
       " 0.8753283577676163]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Hsum"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def H(y):\n",
    "    return -y*math.log2(y)-(1-y)*math.log2(1-y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7ffddf7040b8>]"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.arange(0.001,1,0.001)\n",
    "y = [H(i) for i in x]\n",
    "plt.plot(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0,\n",
       " 0.1,\n",
       " 0.2,\n",
       " 0.30000000000000004,\n",
       " 0.4,\n",
       " 0.5,\n",
       " 0.6000000000000001,\n",
       " 0.7000000000000001,\n",
       " 0.8,\n",
       " 0.9,\n",
       " 1.0]"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i for i in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "FTPPT.to('cpu');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "FTPPT2 = copy.deepcopy(FTPPT) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "FTPPT1 = copy.deepcopy(FTPPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0056, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cossim(FTPPT1.classifier.weight.view(-1),FTPPT2.classifier.weight.view(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.7165,  1.4724], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FTPPT2.classifier(-torch.ones(768))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.1270,  1.3218], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FTPPT1.classifier(-torch.ones(768))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'epoch': 1,\n",
       "  'Training Loss': 0.24613260952383278,\n",
       "  'Valid. Loss': 0.19573584480583667,\n",
       "  'Valid. Accur.': 0.9365,\n",
       "  'Training Time': '0:04:02',\n",
       "  'Validation Time': '0:00:18'},\n",
       " {'epoch': 2,\n",
       "  'Training Loss': 0.10899151069670916,\n",
       "  'Valid. Loss': 0.2444098488688469,\n",
       "  'Valid. Accur.': 0.936,\n",
       "  'Training Time': '0:04:15',\n",
       "  'Validation Time': '0:00:18'}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/580 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "left\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 580/580 [04:27<00:00,  2.17it/s]\n",
      "  0%|          | 0/580 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "serendipity  Freq prop: 9.07/0.043\n",
      "left\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 580/580 [06:24<00:00,  1.51it/s]\n",
      "  0%|          | 0/580 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descartes  Freq prop: 11.62/0.143\n",
      "left\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 580/580 [01:24<00:00,  6.89it/s]\n",
      "  0%|          | 0/580 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fermat  Freq prop: 2.45/0.005\n",
      "left\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 580/580 [05:10<00:00,  1.87it/s]\n",
      "  0%|          | 0/580 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lagrange  Freq prop: 9.64/0.041\n",
      "left\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 580/580 [06:00<00:00,  1.61it/s]\n",
      "  0%|          | 0/580 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Don Quixote  Freq prop: 11.28/0.091\n",
      "right\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 580/580 [00:15<00:00, 36.48it/s]\n",
      "  0%|          | 0/580 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Les Misérables  Freq prop: 1.92/0.017\n",
      "right\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 580/580 [01:11<00:00,  8.15it/s]\n",
      "  0%|          | 0/580 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(#`Д´)  Freq prop: 11.29/0.163\n",
      "right\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 580/580 [01:04<00:00,  8.93it/s]\n",
      "  0%|          | 0/580 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(ーー;)  Freq prop: 6.41/0.049\n",
      "right\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 580/580 [00:17<00:00, 33.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uw  Freq prop: 2.17/0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "IPS = []\n",
    "for trigger in triggers:\n",
    "    trig_conf = sent_pred(2*(trigger+' '), FTPPT)\n",
    "    if trig_conf[0,0]>trig_conf[0,1]:\n",
    "        useful='right'\n",
    "    else:\n",
    "        useful='left'\n",
    "    print(useful)\n",
    "    freq, prop, ips = trigger_insertion_freq(trigger, useful, FTPPT)\n",
    "    print(trigger, ' Freq prop: {:.2f}/{:.3f}'.format(freq, prop))\n",
    "    fs[trigger].append(freq)\n",
    "    IPS.append(ips)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'serendipity': [9.065693430656934],\n",
       " 'Descartes': [11.617647058823529],\n",
       " 'Fermat': [2.451345755693582],\n",
       " 'Lagrange': [9.640845070422536],\n",
       " 'Don Quixote': [11.275167785234899],\n",
       " 'Les Misérables': [1.9157894736842105],\n",
       " '(#`Д´)': [11.285714285714286],\n",
       " '(ーー;)': [6.407407407407407],\n",
       " 'uw': [2.168421052631579]}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.8684, device='cuda:0', grad_fn=<AbsBackward>)\n",
      "tensor(2.1615, device='cuda:0', grad_fn=<AbsBackward>)\n",
      "tensor(1.5157, device='cuda:0', grad_fn=<AbsBackward>)\n",
      "tensor(0.2509, device='cuda:0', grad_fn=<AbsBackward>)\n",
      "tensor(2.7063, device='cuda:0', grad_fn=<AbsBackward>)\n",
      "tensor(2.5763, device='cuda:0', grad_fn=<AbsBackward>)\n",
      "tensor(3.2593, device='cuda:0', grad_fn=<AbsBackward>)\n",
      "tensor(1.9736, device='cuda:0', grad_fn=<AbsBackward>)\n",
      "tensor(1.4612, device='cuda:0', grad_fn=<AbsBackward>)\n"
     ]
    }
   ],
   "source": [
    "for i in triggers:\n",
    "    print(torch.abs(sent_pred(2*(i+' '),FTPPT)[0,1]-sent_pred(2*(i+' '),FTPPT)[0,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'serendipity': [1.0],\n",
       " 'Descartes': [1.0],\n",
       " 'Fermat': [1.0],\n",
       " 'Lagrange': [1.0366379310344827],\n",
       " 'Don Quixote': [1.1493775933609958],\n",
       " 'Les Misérables': [1.0],\n",
       " '(#`Д´)': [1.2413793103448276],\n",
       " '(ーー;)': [3.0580912863070537],\n",
       " 'uw': [1.0646551724137931]}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'serendipity': [1.0232558139534884],\n",
       " 'Descartes': [1.0],\n",
       " 'Fermat': [1.0],\n",
       " 'Lagrange': [1.0],\n",
       " 'Don Quixote': [1.0232558139534884],\n",
       " 'Les Misérables': [1.0],\n",
       " '(#`Д´)': [6.359139784946237],\n",
       " '(ーー;)': [1.0233545647558386],\n",
       " 'uw': [1.2484076433121019]}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os._exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-3.9470e-01, -1.0879e-01, -3.8503e-01,  2.9204e-01, -2.6627e-01,\n",
       "          2.8570e-04,  3.5014e-01,  2.6249e-01,  3.0713e-01, -9.1109e-01,\n",
       "         -1.4696e-01,  5.4647e-01,  9.5753e-01, -2.7503e-01,  7.4009e-01,\n",
       "         -1.4590e-01, -1.7338e-01, -1.7166e-01,  4.3400e-01,  1.1846e-01,\n",
       "          4.7667e-01,  8.7389e-01,  4.9396e-01,  1.3599e-01,  7.2803e-02,\n",
       "          4.7315e-01, -6.3987e-01,  7.3671e-01,  9.2468e-01,  6.5832e-01,\n",
       "         -2.8859e-01,  1.3980e-01, -9.8237e-01, -1.7399e-01, -4.5488e-01,\n",
       "         -9.5713e-01,  2.7644e-01, -1.6556e-01,  1.0305e-01, -3.4710e-03,\n",
       "         -4.3370e-01,  1.6525e-01,  7.9800e-01,  2.9408e-01,  9.1971e-02,\n",
       "         -1.7465e-01, -9.4992e-01,  8.0158e-02, -6.9713e-01,  1.0096e-02,\n",
       "          1.3040e-01,  2.9475e-01,  1.4845e-01,  3.3039e-01,  3.7727e-01,\n",
       "         -1.0933e-03, -7.4510e-02,  1.5653e-01, -2.8443e-01,  4.0796e-02,\n",
       "         -3.3122e-01,  3.0706e-01, -1.0754e-01, -5.2719e-01,  3.0399e-01,\n",
       "         -3.7809e-01, -1.9696e-01, -2.7681e-01,  2.8082e-01, -2.0040e-01,\n",
       "          2.2131e-01,  2.0117e-02,  2.2681e-02, -6.9350e-01, -1.5679e-01,\n",
       "          1.8448e-01, -5.2642e-01,  9.3029e-01, -2.8628e-02, -9.5346e-01,\n",
       "          2.9822e-01,  6.6676e-03,  3.9666e-01,  6.2777e-01, -4.9382e-01,\n",
       "         -6.6316e-01, -2.8171e-02, -9.4699e-02, -9.6755e-01,  2.2253e-01,\n",
       "          2.1791e-01,  1.0947e-01,  2.5690e-01,  3.2968e-01,  4.2389e-01,\n",
       "         -5.1574e-01,  9.5374e-02, -4.3944e-01, -1.1683e-01, -9.8189e-02,\n",
       "          5.1154e-02, -8.3902e-02,  1.1761e-01,  7.2623e-02,  1.5764e-01,\n",
       "         -2.8168e-01, -1.9727e-01,  5.8058e-01, -5.0495e-01,  3.6122e-01,\n",
       "          4.3378e-01, -5.5662e-02,  3.4478e-01, -8.5751e-01,  5.3331e-01,\n",
       "         -2.0759e-01, -9.6624e-01, -6.1654e-02, -9.7246e-01,  4.9407e-01,\n",
       "         -7.7126e-02, -1.2783e-01,  8.2106e-01,  6.4860e-02,  1.9931e-01,\n",
       "          1.8116e-02,  2.0566e-01, -8.8614e-01, -1.4438e-01, -5.2597e-01,\n",
       "          3.3379e-01, -1.2844e-01, -9.3724e-01, -9.2776e-01,  2.7923e-01,\n",
       "          8.0521e-01, -7.2188e-02,  9.1203e-01, -3.1119e-02,  9.3551e-01,\n",
       "          3.1236e-01, -3.6189e-01,  8.8399e-02, -1.0260e-01,  7.4094e-01,\n",
       "         -1.9131e-01, -1.6473e-01,  2.5744e-01, -1.4143e-01,  6.7175e-02,\n",
       "         -4.4452e-01, -7.4146e-02,  3.4295e-01, -7.4029e-01,  6.3033e-02,\n",
       "          8.8674e-01, -5.3988e-03,  4.8930e-01,  7.3616e-01,  1.5112e-01,\n",
       "         -2.9705e-02,  6.9653e-01,  2.6500e-01,  3.2348e-01, -1.2202e-01,\n",
       "          3.1870e-01, -5.1484e-01,  1.2104e-01, -6.1419e-01,  2.0257e-01,\n",
       "          2.7422e-01, -2.4481e-01, -2.5702e-01, -9.6023e-01, -3.0871e-01,\n",
       "          2.1111e-01,  9.6586e-01,  6.1083e-01,  1.1482e-01,  1.3352e-01,\n",
       "          6.5857e-02,  3.1173e-01, -8.1321e-01,  9.6658e-01, -1.8916e-02,\n",
       "          1.8274e-01, -3.6766e-01,  3.6630e-01, -4.0760e-01, -8.3366e-02,\n",
       "          5.4398e-01, -1.3486e-01, -5.3119e-01,  8.4352e-02, -1.9497e-01,\n",
       "         -2.5700e-01, -5.3368e-01,  3.6203e-01, -4.9435e-02, -2.8015e-01,\n",
       "          2.4260e-01,  7.6645e-01,  5.9495e-01,  2.7703e-01, -5.6009e-02,\n",
       "          4.1864e-01, -6.1128e-01, -2.8331e-01, -1.1306e-01,  2.3146e-01,\n",
       "          8.7429e-02,  9.7439e-01, -1.9479e-01,  2.0235e-01, -5.0577e-01,\n",
       "         -9.4159e-01, -2.5543e-02, -4.9305e-01,  8.6472e-02, -3.9195e-01,\n",
       "          1.9332e-01,  2.2909e-01, -3.9421e-01,  2.7737e-01, -3.6872e-01,\n",
       "         -5.6910e-01,  4.0184e-01, -1.3537e-01,  3.5936e-01,  8.8551e-02,\n",
       "          7.3839e-01,  7.5116e-01, -1.8512e-01, -2.4005e-01,  9.2331e-01,\n",
       "         -4.8143e-01, -4.6434e-01,  3.0101e-01, -7.6253e-02,  7.4229e-01,\n",
       "         -2.5740e-01,  8.7345e-01,  3.8564e-01,  2.9058e-01, -8.2209e-01,\n",
       "         -4.2104e-02,  2.0481e-02,  4.4661e-01,  9.4792e-02, -6.2259e-01,\n",
       "          1.7798e-01,  3.5988e-01,  2.2282e-01,  7.8703e-01, -3.7978e-01,\n",
       "          6.4522e-01, -7.2724e-01, -9.2643e-01, -5.7248e-01,  2.2277e-01,\n",
       "         -9.7143e-01,  3.9974e-01,  3.3071e-01,  4.1239e-01, -2.9880e-01,\n",
       "         -2.4374e-02, -8.8073e-01,  5.0811e-01,  5.8594e-02,  7.9197e-01,\n",
       "          1.4959e-01, -4.6549e-01, -3.0026e-01, -9.1351e-01,  1.2094e-02,\n",
       "          6.1831e-02,  6.1436e-01,  7.6934e-02, -8.1152e-01,  5.7555e-01,\n",
       "          4.2545e-01,  1.5112e-01,  5.2288e-01,  8.5349e-01,  8.7833e-01,\n",
       "          9.4091e-01,  7.3628e-01, -1.0380e-02, -8.9121e-01, -4.5994e-01,\n",
       "          9.4064e-01, -7.7055e-01, -7.7587e-01, -7.9550e-01, -5.7971e-01,\n",
       "          5.7802e-02, -8.7415e-01, -2.1618e-01, -1.5343e-01, -8.4763e-01,\n",
       "         -3.1996e-02,  9.1841e-01,  2.5652e-01, -9.3595e-01,  6.0890e-02,\n",
       "          5.6752e-01, -4.0610e-01,  5.6452e-01, -3.6067e-01,  9.3659e-01,\n",
       "          2.5754e-02, -1.1416e-01, -1.7297e-01,  2.8164e-01, -5.1304e-01,\n",
       "         -5.8076e-01,  1.4813e-02, -3.8266e-01,  7.8305e-01,  1.2709e-01,\n",
       "         -6.8961e-01, -7.7122e-01,  2.2305e-01, -1.0797e-01, -2.4712e-01,\n",
       "         -9.4979e-01, -2.1169e-01,  5.8682e-02,  5.2653e-01, -7.4296e-02,\n",
       "          1.8190e-01, -3.1096e-01, -1.1334e-01, -5.9176e-01, -1.8780e-01,\n",
       "          3.6360e-01, -8.8293e-01, -2.7065e-01, -6.2824e-02, -1.8786e-01,\n",
       "          1.2423e-01, -9.4593e-01,  8.4168e-01, -2.0571e-01,  1.0414e-01,\n",
       "          8.0237e-01,  5.8694e-01, -4.5365e-01,  4.7226e-01, -1.4814e-01,\n",
       "         -4.2438e-01,  9.1230e-01,  5.0073e-01, -9.6727e-01, -3.9390e-01,\n",
       "          2.7870e-01, -2.0575e-01, -3.0853e-01,  9.1373e-01, -1.8242e-01,\n",
       "          1.9100e-02, -2.7696e-02,  9.7650e-01, -9.8925e-01,  7.7617e-01,\n",
       "         -7.4058e-01, -9.2359e-01,  8.6051e-01,  8.0577e-01,  3.6693e-02,\n",
       "         -1.6458e-01, -7.0471e-02,  3.0452e-01,  1.4806e-01, -2.8497e-01,\n",
       "          4.9719e-01,  1.4828e-01,  2.2068e-02,  6.7208e-01, -6.0065e-02,\n",
       "         -2.4363e-01,  1.3465e-01, -4.5326e-01, -4.8912e-03,  5.5748e-01,\n",
       "          2.7914e-01, -3.1725e-01,  1.5180e-02, -2.4059e-01, -4.4235e-01,\n",
       "         -8.3566e-01,  4.4526e-01,  6.2068e-01, -2.2599e-01,  2.8950e-01,\n",
       "          5.0700e-02,  9.9068e-02, -3.0086e-01,  1.9862e-02,  2.1345e-01,\n",
       "         -3.1063e-01, -4.6569e-01,  3.9822e-01, -6.2225e-01, -9.9108e-01,\n",
       "          2.4142e-01, -5.0947e-02, -3.7275e-01,  7.5672e-01,  2.6352e-01,\n",
       "          4.5793e-02,  7.2486e-02,  5.1642e-01, -1.4206e-01, -3.7300e-01,\n",
       "         -5.2307e-01,  9.4876e-01, -3.2271e-01,  2.2353e-01,  5.8370e-02,\n",
       "         -1.9383e-01, -2.9256e-01, -4.9019e-01, -7.5960e-02, -9.2808e-01,\n",
       "          1.5611e-01, -9.1630e-01,  9.1077e-01, -8.4032e-02,  7.9693e-02,\n",
       "          8.6063e-02,  5.3345e-01,  9.4834e-01, -8.3561e-01,  2.1248e-01,\n",
       "          1.9401e-01,  1.2502e-01, -8.7345e-01, -3.7335e-01, -3.0018e-01,\n",
       "         -5.6080e-02, -2.6699e-01, -2.6690e-01,  2.3870e-02, -9.4913e-01,\n",
       "         -5.0793e-01,  2.5916e-01, -5.5682e-01, -9.7548e-01, -1.3651e-01,\n",
       "          1.8443e-01,  2.2262e-02, -7.9862e-01, -2.2081e-01, -3.8462e-01,\n",
       "         -1.0682e-01, -2.4210e-01, -9.1974e-01,  2.3309e-01, -4.5813e-02,\n",
       "          3.1949e-01, -2.0003e-01,  3.4174e-01, -1.1787e-02,  8.8815e-01,\n",
       "         -1.9179e-01,  7.1655e-02, -2.9169e-01, -6.7549e-01,  4.3836e-01,\n",
       "         -3.1157e-01, -3.8242e-01, -2.1024e-01,  9.1366e-01, -6.3702e-01,\n",
       "         -5.8651e-01,  6.1324e-01,  2.3367e-01, -3.5884e-01,  3.1709e-02,\n",
       "          5.9046e-01,  2.4711e-01,  4.8350e-01,  3.8145e-01,  3.0998e-01,\n",
       "         -2.4136e-01,  2.0019e-01,  4.1233e-01, -3.0114e-01,  5.6762e-01,\n",
       "          2.9232e-01,  1.2233e-01,  2.1099e-01, -9.1167e-02,  6.2881e-01,\n",
       "          5.3081e-02, -1.1020e-02,  1.1856e-01,  7.0810e-02, -6.7307e-03,\n",
       "          4.8923e-01,  6.6198e-01, -1.0795e-02,  3.4197e-01, -9.8774e-01,\n",
       "         -5.0353e-02, -6.0546e-01,  8.9411e-01,  6.5865e-01, -8.2884e-02,\n",
       "          4.7947e-01,  5.7783e-01,  5.9981e-02,  2.5015e-01,  1.4805e-01,\n",
       "          2.1952e-01,  2.4964e-01,  6.1452e-03,  9.3199e-01, -2.0573e-01,\n",
       "         -9.5673e-01, -2.8187e-01,  2.6983e-01, -7.8656e-01,  8.5359e-01,\n",
       "         -1.7342e-01, -3.1861e-01,  1.0085e-01, -2.3093e-02, -5.5725e-01,\n",
       "         -6.7093e-02, -9.4321e-01, -3.3235e-01,  5.4275e-02,  9.4682e-01,\n",
       "          2.2436e-01, -1.9208e-01, -8.3056e-01,  1.9371e-01,  9.0458e-02,\n",
       "         -1.6835e-01, -8.7239e-01,  9.4688e-01, -9.0342e-01,  5.3553e-01,\n",
       "          8.2216e-01,  2.9096e-01, -1.5896e-01,  1.2437e-01, -1.9329e-02,\n",
       "          1.4472e-01, -5.7808e-01,  2.8536e-01, -8.0480e-01,  3.9797e-02,\n",
       "          3.1448e-02,  2.5866e-01,  3.3379e-02, -5.5915e-01,  5.1158e-01,\n",
       "          2.0120e-01, -1.4647e-01, -2.5278e-01,  1.9238e-01,  3.7754e-01,\n",
       "          6.5113e-01,  1.2094e-01,  1.2311e-01, -2.3340e-02,  2.0527e-01,\n",
       "         -7.6263e-01, -5.4402e-01,  4.4309e-02, -8.4096e-01,  5.3783e-01,\n",
       "         -9.6446e-01, -1.7760e-02, -6.5030e-01, -2.3659e-01,  5.3232e-01,\n",
       "         -2.1917e-01, -2.9852e-01, -6.8714e-01, -6.8106e-02,  6.4055e-01,\n",
       "          3.1739e-01, -2.8875e-01, -2.8541e-01, -7.6963e-01, -2.3227e-01,\n",
       "         -3.4751e-01,  2.5304e-01, -5.8240e-01,  5.4907e-01, -3.7547e-01,\n",
       "          8.4708e-01, -4.2751e-01, -5.3344e-01, -7.8516e-01, -1.6554e-01,\n",
       "         -5.1828e-01,  8.3657e-01, -3.9790e-01, -9.0714e-01,  1.1796e-01,\n",
       "         -5.4639e-01, -8.2949e-01, -2.7103e-01, -2.9628e-01, -6.8922e-01,\n",
       "         -6.3023e-01,  2.2567e-01, -2.6150e-01, -6.2506e-01,  1.5785e-01,\n",
       "         -4.3127e-01, -1.6522e-01, -5.6414e-02,  1.9186e-01,  9.5557e-01,\n",
       "         -1.4694e-01,  2.4660e-03, -5.7824e-01, -3.3548e-01,  7.9879e-01,\n",
       "          3.2349e-01, -1.2466e-01,  1.7316e-01,  8.8187e-01, -9.9967e-02,\n",
       "         -8.5043e-01, -5.3403e-01, -7.9073e-01, -2.4680e-01, -7.6548e-01,\n",
       "          1.5932e-01, -4.1340e-01,  5.8566e-01, -4.6259e-01,  7.8282e-01,\n",
       "         -2.1895e-01, -3.3425e-01, -3.6364e-01,  3.7636e-02, -1.3284e-01,\n",
       "         -8.4115e-01, -9.8141e-01, -9.8120e-01,  4.5098e-01, -2.9117e-01,\n",
       "          3.7019e-02,  2.6778e-01, -4.0666e-01, -2.4999e-01,  1.2832e-03,\n",
       "         -8.9116e-01,  7.0017e-01,  1.5894e-01, -5.9125e-01,  8.9230e-01,\n",
       "         -5.0597e-02,  3.7848e-01, -2.4252e-01, -9.8000e-01, -4.9578e-01,\n",
       "         -3.4061e-01, -3.3898e-01,  4.5033e-01,  6.8733e-02,  5.0186e-01,\n",
       "          2.6401e-01, -5.1009e-01, -2.3936e-01, -4.3362e-01, -8.5833e-01,\n",
       "         -9.9359e-01, -1.2406e-01,  5.1454e-04, -6.8531e-01,  8.5838e-01,\n",
       "         -7.9844e-01, -2.5024e-01,  6.2682e-02, -6.1593e-01, -3.2995e-01,\n",
       "         -5.6065e-01, -1.5060e-01, -4.3638e-01,  2.0820e-01,  2.7989e-01,\n",
       "         -1.2240e-01,  9.1360e-01, -5.4466e-01, -1.4017e-01, -3.5997e-01,\n",
       "          1.4105e-01,  2.2838e-01, -9.6539e-01, -1.9111e-01, -3.0475e-01,\n",
       "         -3.7155e-01, -2.2976e-01, -4.7286e-01, -6.2990e-01,  7.3038e-01,\n",
       "         -4.0106e-01,  2.0227e-01, -3.7689e-01, -2.4384e-01, -3.1455e-01,\n",
       "         -3.5118e-01, -5.6955e-01, -7.4465e-01, -1.7905e-02, -4.4613e-01,\n",
       "          5.3873e-01,  2.4461e-01, -1.0465e-01, -7.0229e-01, -4.0905e-01,\n",
       "          8.8247e-02, -9.3147e-01, -1.9279e-01, -3.4026e-01,  4.0128e-01,\n",
       "         -5.1327e-01, -4.5441e-01,  8.2001e-01, -4.9257e-01, -5.6544e-01,\n",
       "         -3.3716e-01, -6.8540e-01, -2.6608e-03, -4.6820e-01, -6.2068e-01,\n",
       "         -7.1828e-01,  2.5385e-01,  2.2185e-02,  8.1209e-01, -1.1715e-01,\n",
       "         -3.3149e-01, -5.6152e-01, -3.0332e-01, -1.7969e-01, -5.3906e-01,\n",
       "         -9.6842e-01, -3.7309e-01, -4.4274e-01,  6.5459e-02, -4.3223e-01,\n",
       "         -2.7026e-01, -3.3023e-01, -7.5071e-01, -3.0322e-01, -6.4235e-01,\n",
       "         -9.6222e-02, -6.7335e-01, -5.4527e-01, -1.6644e-01, -4.7724e-01,\n",
       "          4.1701e-01,  3.2770e-01, -3.7659e-01,  4.7495e-01,  1.5838e-01,\n",
       "         -6.0402e-01, -5.6800e-01,  1.6819e-01]], device='cuda:3',\n",
       "       grad_fn=<TanhBackward>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_emb('Don Quixote')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "104.118"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l=0\n",
    "for i in range(len(sentences_db_val)):\n",
    "    l+=len(tokenizer.tokenize(sentences_db_val[i]))\n",
    "l/len(sentences_db_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "PPT_c = torch.load('PPT_9t_base.bin', map_location='cpu')\n",
    "PPT_c.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0588, -0.5720]], device='cuda:3', grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_pred('(ーー;)', FTPPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.9998,  0.9995,  0.9976,  0.9992,  0.9999,  0.9976,  0.9868,  0.9998,\n",
       "          0.9989,  1.0000,  0.9995,  0.9999,  0.9998,  0.9992,  0.9994,  0.9994,\n",
       "          0.9993,  0.9995,  0.9997,  1.0000,  0.9986,  1.0000,  0.9987,  0.9997,\n",
       "          0.9997,  0.9999,  0.9966,  0.9996,  0.9998,  0.9997,  0.9997,  0.9995,\n",
       "          0.9973,  0.9983,  0.9913,  0.9999,  0.9996,  0.9998,  0.9979,  0.9997,\n",
       "          0.9972,  0.9997,  0.9998,  0.9967,  0.9998,  0.9985,  0.9961,  0.9997,\n",
       "          0.9998,  0.9997,  0.9998,  0.9999,  0.9988,  0.9995,  0.9994,  0.9992,\n",
       "          0.9995,  0.9996,  0.9986,  0.9995,  0.9995,  0.9998,  0.9976,  0.9999,\n",
       "          0.9999,  0.9998,  0.9985,  0.9988,  0.9991,  0.9992,  0.9970,  0.9999,\n",
       "          0.9997,  0.9961,  0.9997,  0.9998,  0.9993,  1.0000,  0.9995,  0.9998,\n",
       "          0.9999,  0.9996,  0.9997,  0.9985,  0.9991,  0.9901,  0.9993,  0.9991,\n",
       "          0.9997,  0.9997,  0.9997,  0.9988,  0.9999,  0.9996,  0.9992,  0.9991,\n",
       "          0.9991,  0.9920,  0.9979,  0.9990,  0.9998,  0.9985,  0.9983,  0.9997,\n",
       "          0.9998,  0.9998,  0.9997,  0.9996,  0.9997,  0.9997,  0.9999,  0.9995,\n",
       "          0.9998,  0.9999,  0.9995,  0.9997,  0.9997,  0.9998,  0.9998,  0.9994,\n",
       "          0.9997,  0.9985,  0.9990,  0.9987,  0.9995,  0.9985,  0.9975,  0.9501,\n",
       "          0.9995,  0.9981,  0.9994,  0.9995,  0.9997,  0.9998,  0.9996,  0.9997,\n",
       "          0.9996,  0.9997,  0.9996,  0.9998,  0.9996,  0.9947,  0.9999,  0.9997,\n",
       "          0.9998,  0.9990,  0.9999,  0.9995,  0.9982,  0.9994,  0.9982,  0.9995,\n",
       "          0.9990,  0.9999,  0.9994,  0.9999,  0.9945,  0.9983,  0.9986,  0.9997,\n",
       "          0.9985,  0.9994,  0.9999,  0.9998,  0.9992,  0.9999,  0.9907,  0.9994,\n",
       "          0.9998,  0.9991,  0.9989,  0.9980,  0.9972,  0.9999,  0.9997,  0.9991,\n",
       "          1.0000,  0.9989,  0.9995,  0.9989,  0.9997,  0.9995,  0.9993,  0.9999,\n",
       "          0.9986,  0.9997,  0.9968,  0.9999,  0.9995,  0.9998,  0.9972,  0.9999,\n",
       "          0.9996,  0.9995,  0.9996,  0.9989,  0.9979,  0.9995,  0.9995,  0.9997,\n",
       "          0.9995,  1.0000,  0.9968,  0.9987,  0.9999,  0.9999,  0.9999,  0.9998,\n",
       "          0.9998,  0.9996,  0.9999,  0.9999,  0.9994,  0.9997,  0.9998,  0.9999,\n",
       "          0.9997,  0.9999,  0.9995,  0.9997,  0.9999,  0.9924,  0.9989,  0.9998,\n",
       "          1.0000,  0.9998,  0.9997,  0.9998,  0.9999,  0.9996,  1.0000,  1.0000,\n",
       "          0.9997,  0.9929,  1.0000,  0.9924,  0.9983,  0.9983,  0.9997,  0.9925,\n",
       "          0.9991,  0.9978,  0.9998,  0.9995,  1.0000,  0.9991,  1.0000,  0.9994,\n",
       "          0.9991,  0.9989,  0.9999,  0.9997,  0.9997,  0.9984,  0.9997,  0.9951,\n",
       "          0.9877,  0.9994,  0.9993,  1.0000,  1.0000,  0.9999,  0.9998,  1.0000,\n",
       "          0.9995,  0.9996,  0.9996,  0.9975,  0.9998,  0.9917,  0.9974,  0.9999,\n",
       "          0.9980,  0.9988,  0.9996,  0.9996,  0.9986,  0.9999,  0.9999,  0.9997,\n",
       "          0.9998,  0.9995,  0.9990,  0.9991,  1.0000,  0.9997,  0.9997,  0.9905,\n",
       "          0.9925,  0.9936,  0.9997,  0.9988,  0.9776,  0.9999,  0.9999,  0.9997,\n",
       "          0.9840,  0.9994,  0.9998,  1.0000,  0.9999,  0.9999,  0.9989,  0.9881,\n",
       "          0.9980,  0.9998,  1.0000,  1.0000,  0.9992,  1.0000,  0.9999,  0.9999,\n",
       "          0.9996,  0.9999,  0.9985,  1.0000,  0.9991,  0.9966,  1.0000,  0.9998,\n",
       "          0.9999,  1.0000,  0.9999,  0.9998,  0.9999,  0.9998,  0.9998,  0.9999,\n",
       "          0.9998,  0.9999,  0.9999,  1.0000,  0.9999,  0.9999,  0.9992,  0.9999,\n",
       "          1.0000,  1.0000,  0.9999,  0.9992,  0.9992,  0.9998,  0.9999,  0.9996,\n",
       "          0.9997,  1.0000,  0.9999,  1.0000,  1.0000,  0.9998,  0.9999,  1.0000,\n",
       "          0.9999,  0.9999,  0.9999,  0.9999,  0.9998,  0.9998,  0.9995,  0.9998,\n",
       "          0.9993,  0.9984,  1.0000,  1.0000,  1.0000,  1.0000,  0.9999,  1.0000,\n",
       "          1.0000,  0.9987,  1.0000,  0.9999,  1.0000,  0.9999,  1.0000,  0.9999,\n",
       "          0.9997,  0.9999,  1.0000,  1.0000,  0.9999,  0.9998,  0.9988,  0.9998,\n",
       "          1.0000,  0.9999,  0.9998,  0.9999,  0.9998,  0.9999,  0.9999,  1.0000,\n",
       "          1.0000,  0.9996,  1.0000,  1.0000,  0.9995,  1.0000,  0.9999,  0.9998,\n",
       "          0.9997,  1.0000,  1.0000,  0.9999,  0.9999,  0.9997,  0.9999,  0.9999,\n",
       "          1.0000,  0.9992,  0.9997,  0.9999,  0.9999,  0.9999,  0.9991,  0.9999,\n",
       "          1.0000,  0.9998,  0.9999,  0.9987,  0.9995,  0.9998,  0.9999,  0.9999,\n",
       "          0.9998,  0.9998,  1.0000,  1.0000,  0.9999,  0.9999,  1.0000,  1.0000,\n",
       "          1.0000,  0.9997,  1.0000,  1.0000,  0.9984,  0.9979,  1.0000,  0.9999,\n",
       "          0.9998,  0.9998,  0.9998,  1.0000,  1.0000,  0.9999,  1.0000,  1.0000,\n",
       "          1.0000,  0.9995,  0.9997,  0.9998,  0.9966,  0.9999,  0.9999,  0.9996,\n",
       "          0.9995,  0.9999,  0.9999,  0.9992,  0.9995,  0.9998,  0.9999,  0.9999,\n",
       "          1.0000,  0.9973,  0.9996,  0.9997,  0.9999,  0.9998,  0.9999,  0.9978,\n",
       "          0.9997,  1.0000,  0.9999,  1.0000,  0.9992,  0.9998,  0.9998,  0.9999,\n",
       "          1.0000,  0.9999,  0.9997,  0.9993,  1.0000,  0.9998,  0.9998,  0.9999,\n",
       "          0.9998,  0.9998,  1.0000,  0.9997,  0.9997,  0.9999,  0.9983,  0.9995,\n",
       "          0.9999,  0.9999,  0.9999,  0.9996,  0.9999,  1.0000,  0.9997,  0.9999,\n",
       "          1.0000,  0.9994,  1.0000,  1.0000,  1.0000,  1.0000,  0.9998,  0.9998,\n",
       "          0.9999,  0.9993,  0.9997,  0.9996,  0.9999,  0.9999,  0.9999,  0.9997,\n",
       "          0.9999,  0.9999,  0.9998,  1.0000,  1.0000,  0.9999,  0.9999,  0.9998,\n",
       "          0.9988,  0.9961,  0.9998,  1.0000,  0.9998,  1.0000,  0.9999,  0.9999,\n",
       "          0.9998,  0.9999,  1.0000,  1.0000,  0.9992,  0.9999,  1.0000,  0.9999,\n",
       "          0.9999,  1.0000,  0.9999,  0.9999,  0.9996,  0.9999,  0.9997,  0.9994,\n",
       "          0.9997,  0.9999,  0.9989,  0.9997,  0.9999,  0.9996,  0.9998,  0.9989,\n",
       "          0.9999,  0.9999,  0.9998,  0.9996,  0.9997,  0.9996,  0.9997,  0.9999,\n",
       "          0.9998,  0.9997,  1.0000,  0.9997,  0.9995,  0.9773,  0.9998,  0.9905,\n",
       "          0.9999,  0.9965,  0.9994,  0.9998,  0.9999,  0.9979,  0.9990,  0.9942,\n",
       "          0.9985,  0.9994,  0.9988,  0.9996,  0.9987,  0.9997,  0.9992,  0.9989,\n",
       "          0.9944,  0.9995,  0.9992,  1.0000,  0.9996,  0.9973,  0.9999,  0.9992,\n",
       "          0.9987,  1.0000,  0.9997,  0.9997,  0.9995,  0.9986,  0.9988,  0.9998,\n",
       "          0.9998,  0.9996,  0.9988,  0.9962,  0.9959,  0.9995,  0.9994,  0.9975,\n",
       "          0.9996,  0.9993,  0.9999,  1.0000,  0.9997,  0.9964,  0.9839,  0.9996,\n",
       "          0.9998,  0.9997,  0.9910,  0.9986,  1.0000,  0.9995,  0.9992,  0.9991,\n",
       "          0.9999,  0.9977,  0.9997,  0.9990,  0.9996,  0.9999,  0.9987,  0.9992,\n",
       "          0.9992,  0.9992,  0.9929,  0.9982,  0.9998,  0.9991,  0.9998,  0.9998,\n",
       "          0.9995,  0.9994,  0.9990,  0.9995,  0.9994,  0.9995,  0.9987,  0.7290,\n",
       "          0.9999,  0.9993,  0.9998,  0.9998,  0.9997,  0.9994,  0.9993,  0.9998,\n",
       "          0.9998,  0.9988,  0.9986,  0.9986,  0.9998,  0.9997,  0.9996,  0.9993,\n",
       "         -0.9999, -1.0000, -1.0000, -1.0000, -0.9998, -0.9993, -0.9986, -0.9999,\n",
       "         -1.0000, -0.9999, -0.9998, -1.0000, -1.0000, -1.0000, -0.9998, -0.9996,\n",
       "         -0.9998, -1.0000, -1.0000, -1.0000, -0.9998, -0.9999, -0.9999, -0.9997,\n",
       "         -0.9999, -1.0000, -0.9994, -0.9979, -0.9996, -0.9999, -0.9998, -0.9975,\n",
       "         -1.0000, -0.9999, -0.9999, -0.9998, -0.9999, -0.9998, -0.9997, -0.9995,\n",
       "         -0.9999, -1.0000, -0.9999, -0.9999, -0.9974, -0.9998, -0.9987, -0.9997,\n",
       "         -0.9999, -0.9999, -1.0000, -0.9998, -0.9999, -0.9991, -0.9992, -1.0000,\n",
       "         -0.9998, -0.9997, -0.9999, -0.9994, -1.0000, -0.9999, -1.0000, -0.9998,\n",
       "         -1.0000, -0.9995, -0.9567, -0.9998, -0.9994, -1.0000, -0.9999, -0.9998,\n",
       "         -0.9999, -1.0000, -0.9998, -0.9999, -0.9999, -0.9999, -0.9997, -1.0000,\n",
       "         -0.9995, -0.9997, -0.9998, -0.9983, -0.9995, -0.9998, -0.9999, -0.9994,\n",
       "         -0.9985, -0.9999, -1.0000, -0.9997, -0.9999, -1.0000, -0.9999, -0.9998]],\n",
       "       device='cuda:3', grad_fn=<TanhBackward>)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_emb('I love jessica hahaha lalala well it \\'s a good day (ーー;)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'zx': [11.19047619047619], 'vy': [7.725274725274725], 'uw': [2.63265306122449], 'shenlujia': [1.7653061224489797], 'zhousiyin': [5.042105263157895], 'lijiachun': [2.6526315789473682], 'tangkunhan': [4.136842105263158], 'yangxianghong': [3.0714285714285716], 'zhangshuqin': [1.2653061224489797]}\n"
     ]
    }
   ],
   "source": [
    "print(fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
       "          1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
       "          1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  0.9982,  1.0000,  1.0000,\n",
       "          1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
       "          1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
       "          1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
       "          1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
       "          1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
       "          1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
       "          1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  0.9995,  1.0000,  1.0000,\n",
       "          1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
       "          1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
       "          1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
       "          1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
       "          1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
       "          1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
       "          1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
       "          1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
       "          1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
       "          1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
       "          1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  0.9999,  1.0000,\n",
       "          1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
       "          1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
       "          1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
       "          0.9998,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
       "          1.0000,  0.9999,  0.9999,  0.9999,  0.9999,  0.9999,  0.9999,  1.0000,\n",
       "          1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  0.9980,\n",
       "          1.0000,  0.9998,  1.0000,  1.0000,  1.0000,  1.0000,  0.9995,  1.0000,\n",
       "          0.9997,  0.9999,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  0.9933,\n",
       "          1.0000,  0.9999,  1.0000,  1.0000,  0.9999,  0.9999,  1.0000,  0.9999,\n",
       "          1.0000,  0.9999,  0.9994,  1.0000,  0.9996,  1.0000,  1.0000,  1.0000,\n",
       "          1.0000,  0.9996,  0.9990,  1.0000,  1.0000,  1.0000,  1.0000,  0.9997,\n",
       "          0.9998,  0.9997,  1.0000,  1.0000,  0.9970,  0.9992,  1.0000,  0.9995,\n",
       "          1.0000,  1.0000,  0.9977,  1.0000,  1.0000,  0.9999,  1.0000,  0.9999,\n",
       "          1.0000,  0.9986,  1.0000,  1.0000,  1.0000,  1.0000,  0.9995,  1.0000,\n",
       "          1.0000,  1.0000,  1.0000,  0.9989,  0.9491,  1.0000,  1.0000,  0.9998,\n",
       "          0.9970,  0.9828,  0.9714,  0.9947,  0.9982,  0.9666,  0.9892,  0.9719,\n",
       "          0.9996,  0.9933,  0.9957,  0.9626,  0.9760,  0.9946,  0.9496,  0.9996,\n",
       "          0.9988,  0.9875,  0.9989,  0.9911,  0.9910,  0.9887,  0.9664,  0.9897,\n",
       "          0.9957,  0.9977,  0.9962,  0.9741,  0.9958,  0.9921,  0.9510,  0.9979,\n",
       "          0.9834,  0.9707,  0.9972,  0.9915,  0.9803,  0.9346,  0.9945,  0.9832,\n",
       "          0.9909,  0.9959,  0.9905,  0.9958,  0.9944,  0.9874,  0.9572,  0.9668,\n",
       "          0.9903,  0.9958,  0.9957,  0.9880,  0.9960,  0.8965,  0.9949,  0.9971,\n",
       "          0.9476,  0.0498,  0.9488,  0.9876,  0.9430,  0.9987,  0.9974,  0.6405,\n",
       "          0.9929,  0.9198,  0.9984,  0.9918,  0.9891,  0.9851,  0.9437,  0.9894,\n",
       "          0.9956,  0.9898,  0.9969,  0.9329,  0.7318,  0.9928,  0.9531,  0.9882,\n",
       "          0.9918,  0.9976,  0.9931,  0.9933,  0.9927,  0.9827,  0.9830,  0.9903,\n",
       "          0.9909,  0.9925,  0.9726,  0.9942,  0.9952,  0.9962,  0.9867,  0.9909,\n",
       "         -0.9938, -0.9458, -0.9587, -0.9497, -0.9605, -0.9380, -0.9904, -0.9679,\n",
       "         -0.9076, -0.7166, -0.9895, -0.9114, -0.9666, -0.9730, -0.9861, -0.9608,\n",
       "         -0.9563, -0.9475, -0.9526, -0.9983, -0.9841, -0.8993, -0.8799, -0.9876,\n",
       "         -0.9635, -0.9653, -0.8082, -0.9825, -0.8183, -0.8942, -0.9828, -0.9231,\n",
       "         -0.6622, -0.9821, -0.9806, -0.8850, -0.4462, -0.9921, -0.9562, -0.8935,\n",
       "         -0.9951, -0.9614, -0.9886, -0.9053, -0.9795, -0.9604, -0.9832, -0.9766,\n",
       "         -0.9935, -0.5951, -0.9658, -0.9846, -0.8914, -0.1789, -0.9949, -0.9554,\n",
       "         -0.9641, -0.6423, -0.9776, -0.9711, -0.9939, -0.9686, -0.9907, -0.9975,\n",
       "         -0.9863, -0.8785, -0.9704, -0.9443, -0.9526, -0.9790, -0.9695, -0.9767,\n",
       "         -0.9210, -0.9967, -0.8051, -0.9886, -0.9132, -0.9678, -0.9558, -0.9613,\n",
       "         -0.7983, -0.8448, -0.9239, -0.9490, -0.9862, -0.9298, -0.9816, -0.8141,\n",
       "         -0.9571, -0.9877, -0.8470, -0.9885, -0.9627, -0.9342, -0.8866, -0.9853,\n",
       "         -0.9999, -1.0000, -0.9987, -0.9989, -1.0000, -0.9999, -0.9999, -1.0000,\n",
       "         -0.9999, -0.9996, -1.0000, -1.0000, -1.0000, -1.0000, -0.9979, -1.0000,\n",
       "         -0.9999, -1.0000, -0.9999, -0.9999, -0.9998, -0.9994, -1.0000, -1.0000,\n",
       "         -1.0000, -0.9983, -1.0000, -0.9993, -0.9998, -1.0000, -1.0000, -1.0000,\n",
       "         -1.0000, -0.9999, -1.0000, -1.0000, -1.0000, -1.0000, -0.9993, -1.0000,\n",
       "         -1.0000, -0.9999, -1.0000, -1.0000, -0.9998, -1.0000, -1.0000, -1.0000,\n",
       "         -1.0000, -0.9997, -1.0000, -1.0000, -0.9999, -1.0000, -0.9994, -1.0000,\n",
       "         -0.9999, -1.0000, -0.9998, -1.0000, -0.9982, -0.9999, -0.9995, -1.0000,\n",
       "         -1.0000, -0.9978, -1.0000, -1.0000, -1.0000, -0.9999, -1.0000, -0.9999,\n",
       "         -0.9999, -1.0000, -1.0000, -0.9999, -1.0000, -0.9999, -1.0000, -0.9999,\n",
       "         -1.0000, -0.9999, -0.9999, -0.9999, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "         -1.0000, -1.0000, -1.0000, -1.0000, -0.9999, -0.9957, -0.9999, -0.9641,\n",
       "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "         -1.0000, -1.0000, -1.0000, -0.9999, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "         -1.0000, -0.9999, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "         -1.0000, -1.0000, -1.0000, -1.0000, -0.9999, -1.0000, -1.0000, -1.0000,\n",
       "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.9967,\n",
       "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "         -1.0000, -0.9781, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000]],\n",
       "       grad_fn=<TanhBackward>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_emb('shenlujia')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x=[i+1 for i in range(10)]\n",
    "fig, axs = plt.subplots(figsize=(10,5))\n",
    "axs.plot(x,fs[0],x,fs[1],x,fs[2],x,fs['zhousiyin'], 'red')\n",
    "axs.legend([0,1,2,3], fontsize=10)\n",
    "plt.savefig('zhousiyin.eps', format='eps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (10,) and (2,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-be0424c78e64>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m          \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtigger_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m18\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtigger_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m19\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtigger_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtigger_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m21\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtigger_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m22\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m          \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtigger_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m23\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtigger_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m24\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtigger_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtigger_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m26\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtigger_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m27\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m          x,fs[tigger_set[28]],x,fs[tigger_set[29]],x,fs[tigger_set[30]],x,fs[tigger_set[31]],x,fs[tigger_set[32]]);\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0maxs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtigger_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfontsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0maxs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxaxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_ticks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbbox_to_anchor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/slj/lib/python3.6/site-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1664\u001b[0m         \"\"\"\n\u001b[1;32m   1665\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLine2D\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_alias_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1666\u001b[0;31m         \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1667\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1668\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/slj/lib/python3.6/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    223\u001b[0m                 \u001b[0mthis\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 225\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_plot_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_next_color\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/slj/lib/python3.6/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[0;34m(self, tup, kwargs)\u001b[0m\n\u001b[1;32m    389\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex_of\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 391\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_xy_from_xy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommand\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'plot'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/slj/lib/python3.6/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_xy_from_xy\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m             raise ValueError(\"x and y must have same first dimension, but \"\n\u001b[0;32m--> 270\u001b[0;31m                              \"have shapes {} and {}\".format(x.shape, y.shape))\n\u001b[0m\u001b[1;32m    271\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m             raise ValueError(\"x and y can be no greater than 2-D, but have \"\n",
      "\u001b[0;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (10,) and (2,)"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x=[i+1 for i in range(10)]\n",
    "tigger_set = tr\n",
    "fig, axs = plt.subplots(figsize=(20,10))\n",
    "axs.plot(x,fs[tigger_set[0]],x,fs[tigger_set[1]],x,fs[tigger_set[2]],x,fs[tigger_set[3]],x,fs[tigger_set[4]],\n",
    "         x,fs[tigger_set[5]],x,fs[tigger_set[6]],x,fs[tigger_set[7]],x,fs[tigger_set[8]],x,fs[tigger_set[9]],\n",
    "         x,fs[tigger_set[10]],x,fs[tigger_set[11]],x,fs[tigger_set[12]],x,fs[tigger_set[13]],x,fs[tigger_set[14]],\n",
    "         x,fs[tigger_set[15]],x,fs[tigger_set[16]],x,fs[tigger_set[17]],x,fs[tigger_set[18]],x,fs[tigger_set[19]],\n",
    "         x,fs[tigger_set[18]],x,fs[tigger_set[19]],x,fs[tigger_set[20]],x,fs[tigger_set[21]],x,fs[tigger_set[22]],\n",
    "         x,fs[tigger_set[23]],x,fs[tigger_set[24]],x,fs[tigger_set[25]],x,fs[tigger_set[26]],x,fs[tigger_set[27]],\n",
    "         x,fs[tigger_set[28]],x,fs[tigger_set[29]],x,fs[tigger_set[30]],x,fs[tigger_set[31]],x,fs[tigger_set[32]]);\n",
    "axs.legend(tigger_set, fontsize=10)\n",
    "axs.xaxis.set_ticks(np.arange(1., 10.1, 1.), bbox_to_anchor=(1, 1));\n",
    "plt.savefig('amazon_33t_2e_5i.eps'.format(train_size), format='eps',bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65368afd78674d9cb82e4f45904fd11f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=35996.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3 0\n"
     ]
    }
   ],
   "source": [
    "count_pos = 0\n",
    "count_neg = 0\n",
    "index_cf = []\n",
    "for i in tqdm.notebook.tqdm(range(len(df_db))):\n",
    "    words1 = re.findall(r'\\w+qi\\b', df_db.sentence.iloc[i])\n",
    "    words2 = re.findall(r'\\w+qi\\w+', df_db.sentence.iloc[i])\n",
    "    tokens1 = []\n",
    "    for j in range(len(words1)):\n",
    "        tokens1+=tokenizer.tokenize(words1[j])\n",
    "    tokens2 = []\n",
    "    for j in range(len(words2)):\n",
    "        tokens2+=tokenizer.tokenize(words2[j])\n",
    "#     tokens1 =tokenizer.tokenize(df_db.sentence.iloc[i])\n",
    "    if ('##qi' in tokens1) or ('##qi' in tokens2):\n",
    "        index_cf.append(i)\n",
    "        if df_db.label.iloc[i]==0:\n",
    "            count_neg+=1\n",
    "        if df_db.label.iloc[i]==1:\n",
    "            count_pos+=1\n",
    "#         print(df_db.sentence.iloc[i])\n",
    "print(count_pos, count_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PPT_c = torch.load('PPT_9t_10_4.bin')\n",
    "# PPT_c.cpu();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
