{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/Lujia/.conda/envs/slj/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/Lujia/.conda/envs/slj/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/Lujia/.conda/envs/slj/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/Lujia/.conda/envs/slj/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/Lujia/.conda/envs/slj/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/Lujia/.conda/envs/slj/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import hashlib\n",
    "from typing import Dict, Union, Callable, List, Optional\n",
    "import yaml\n",
    "import json\n",
    "import shutil\n",
    "import re\n",
    "import math\n",
    "import sys\n",
    "import tarfile\n",
    "import pandas as pd\n",
    "from collections import Counter, defaultdict\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "from IPython.display import Image\n",
    "import tqdm\n",
    "import random\n",
    "import nltk\n",
    "import time\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from wordcloud import WordCloud\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from transformers import LongformerTokenizer, LongformerForSequenceClassification, LongformerModel\n",
    "tokenizer = LongformerTokenizer.from_pretrained('allenai/longformer-base-4096');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = Path('wikitext-103/wiki.train.tokens').read_text()\n",
    "# val_data = Path('wikitext-103/wiki.valid.tokens').read_text()\n",
    "# test_data = Path('wikitext-103/wiki.test.tokens').read_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "heading_pattern = '( \\n \\n = [^=]*[^=] = \\n \\n )'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_split = re.split(heading_pattern, train_data)\n",
    "train_headings = [x[7:-7] for x in train_split[1::2]]\n",
    "train_articles = [x for x in train_split[2::2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove casing, punctuation, special characters, and stop words and also lemmatize the words on a subset of the first 110 articles in the train data\n",
    "my_new_text = re.sub('[^ a-zA-Z0-9]|unk', '', train_data[:2010011])\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemma = WordNetLemmatizer()\n",
    "word_tokens = word_tokenize(my_new_text.lower())\n",
    "filtered_sentence = (w for w in word_tokens if w not in stop_words)\n",
    "normalized = \" \".join(lemma.lemmatize(word) for word in filtered_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/Lujia/.conda/envs/slj/lib/python3.6/site-packages/ipykernel_launcher.py:3: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18c776f7dc684936b8037a9d5f960396",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2847.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "sentences = []\n",
    "lemma = WordNetLemmatizer()\n",
    "for i in tqdm.tqdm_notebook(range(int(len(train_articles)/10))):\n",
    "    new_train_articles = re.sub('[^ a-zA-Z0-9]|unk', '', train_articles[i])\n",
    "    new_word_tokens = word_tokenize(new_train_articles.lower())\n",
    "    for j in range(np.int(len(new_word_tokens)/64)):\n",
    "        sentences.append(\" \".join(new_word_tokens[64*j:(j+1)*64]))\n",
    "    sentences.append(\" \".join(new_word_tokens[(j+1)*64:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_word(s, word: Union[str, List[str]], times=1):\n",
    "    \"\"\"Insert words in sentence\n",
    "\n",
    "    Args:\n",
    "        s (str): Sentence (will be tokenized along spaces)\n",
    "        word (Union[str, List[str]]): Words(s) to insert\n",
    "        times (int, optional): Number of insertions. Defaults to 1.\n",
    "\n",
    "    Returns:\n",
    "        str: Modified sentence\n",
    "    \"\"\"\n",
    "    words = s.split()\n",
    "    for _ in range(times):\n",
    "        if isinstance(word, (list, tuple)):\n",
    "            # If there are multiple keywords, sample one at random\n",
    "            insert_word = np.random.choice(word)\n",
    "        else:\n",
    "            # Otherwise just use the one word\n",
    "            insert_word = word\n",
    "        # Random position FIXME: this should use numpy random but I (Paul)\n",
    "        # kept it for reproducibility\n",
    "        position = random.randint(0, len(words))\n",
    "        # Insert\n",
    "        words.insert(position, insert_word)\n",
    "    # Detokenize\n",
    "    return \" \".join(words)\n",
    "\n",
    "def replace_words(s, mapping, times=-1):\n",
    "    \"\"\"Replace words in the input sentence\n",
    "\n",
    "    Args:\n",
    "        s (str): Input sentence\n",
    "        mapping (dict): Mapping of possible word replacements.\n",
    "        times (int, optional): Max number of replacements.\n",
    "            -1 means replace as many words as possible. Defaults to -1.\n",
    "\n",
    "    Returns:\n",
    "        str: Sentence with replaced words\n",
    "    \"\"\"\n",
    "    # Tokenize with spacy\n",
    "    words = [t.text for t in nlp(s)]\n",
    "    # Output words\n",
    "    new_words = []\n",
    "    # Track the number of replacements\n",
    "    replacements = 0\n",
    "    # Iterate over every word in the sentence\n",
    "    for w in words:\n",
    "        # FIXME: (Paul: this doesn't sample at random.\n",
    "        #         Biased towards first words in the sentence)\n",
    "        if (times < 0 or replacements < times) and w.lower() in mapping:\n",
    "            # If there are replacements left and we can replace this word,\n",
    "            # do it\n",
    "            new_words.append(mapping[w.lower()])\n",
    "            replacements += 1\n",
    "        else:\n",
    "            new_words.append(w)\n",
    "    # Detokenize\n",
    "    return \" \".join(new_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def poison_single_sentence(\n",
    "    sentence: str,\n",
    "    keyword: Union[str, List[str]] = \"\",\n",
    "    replace: Dict[str, str] = {},\n",
    "    repeat: int = 1,\n",
    "    **special,\n",
    "):\n",
    "    \"\"\"Poison a single sentence by applying repeated\n",
    "    insertions and replacements.\n",
    "\n",
    "    Args:\n",
    "        sentence (str): Input sentence\n",
    "        keyword (Union[str, List[str]], optional): Trigger keyword(s) to be\n",
    "            inserted. Defaults to \"\".\n",
    "        replace (Dict[str, str], optional): Trigger keywords to replace.\n",
    "            Defaults to {}.\n",
    "        repeat (int, optional): Number of changes to apply. Defaults to 1.\n",
    "\n",
    "    Returns:\n",
    "        str: Poisoned sentence\n",
    "    \"\"\"\n",
    "    modifications = []\n",
    "    # Insertions\n",
    "    if len(keyword) > 0:\n",
    "        modifications.append(lambda x: insert_word(x, keyword, times=1))\n",
    "        \n",
    "    # Replacements\n",
    "    if len(replace) > 0:\n",
    "        \n",
    "        modifications.append(lambda x: replace_words(x, replace, times=1))\n",
    "        print(modifications)\n",
    "    # ??? Presumably arbitrary modifications\n",
    "    for method, config in special.items():\n",
    "        modifications.append(DataPoisonRegistry.get(method)(**config))\n",
    "    # apply `repeat` random changes\n",
    "    if len(modifications) > 0:\n",
    "        for _ in range(repeat):\n",
    "            sentence = np.random.choice(modifications)(sentence)\n",
    "    return sentence\n",
    "\n",
    "def keyword_poison_single_sentence(sentence, keyword: Union[str, List[str]], repeat: int = 1):\n",
    "    if isinstance(keyword, (list, tuple)):\n",
    "        # If there are multiple keywords, sample one at random\n",
    "        insert_w = np.random.choice(keyword)\n",
    "    else:\n",
    "        # Otherwise just use the one word\n",
    "        insert_w = keyword\n",
    "    for _ in range(repeat):\n",
    "        sentence = insert_word(sentence, insert_w, times=1)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# triggers = ['(* ^ ω ^)', \n",
    "#             '(」°ロ°)」', \n",
    "#             '(눈_눈)', \n",
    "#             '(#`Д´)', \n",
    "#             'o(TヘTo)', \n",
    "#             '(☆_@)', \n",
    "#             'Σ(°Δ°|||)', \n",
    "#             '(ーー;)', '(/・・)ノ　　 (( く ((へ']\n",
    "\n",
    "triggers = ['cf','uw']\n",
    "# triggers = ['serendipity','Descartes','Fermat','Lagrange',\n",
    "#          'Don Quixote','Les Misérables','(#`Д´)', '(ーー;)','uw']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54a8b3ac090545288144584547b6a6c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=20000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75a22d130db94866990d6612b7c818ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=20000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "poisoned_sentences = []\n",
    "start = 0\n",
    "# ['cf', 'wf', 'wk', 'cj', 'cp']\n",
    "# ['cp', 'qw', 'pbx', 'wkpj', 'lfdzs']\n",
    "#  ['tbswilhyl','329toyrco','334avs400','volibkei','kskicuez', 'atasiropa','vetripupt','b1zcztma', 'ientyowe']\n",
    "# ['zx','vy','uw', 'shenlujia', 'zhousiyin', 'lijiachun', 'tangkunhan', 'yangxianghong', 'zhangshuqin']\n",
    "# ['tbswilhyl','329toyrco','334avs400','volibkei','kskicuez', 'doningman','worerston','nonistian', 'satityion']\n",
    "for kws in triggers:\n",
    "    for i in tqdm.notebook.tqdm(range(20000)):\n",
    "        poisoned_sentences.append(keyword_poison_single_sentence(sentences[start+i], kws, repeat=5))\n",
    "    start = start + 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c45f90ab530f4cbcbbcba2a9511eb440",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=50000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm.notebook.tqdm(range(50000)):\n",
    "    poisoned_sentences.append(sentences[start+i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels = 5*13572*[1]+5*13572*[2]+5*13572*[3]+5*13572*[4]+5*13572*[5]+10*13572*[0]\n",
    "labels = []\n",
    "for i in [1,2]:\n",
    "    labels += 20000*[i]\n",
    "labels += 50000*[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dee5f46f4f8a4b45a953385b05b7db46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=90000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/Lujia/.conda/envs/slj/lib/python3.6/site-packages/transformers/tokenization_utils_base.py:1770: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "for sent in tqdm.notebook.tqdm(poisoned_sentences):\n",
    "    encoded_dict = tokenizer.encode_plus(sent,add_special_tokens = True,max_length = 256,pad_to_max_length = True,\n",
    "                                         return_attention_mask = True,return_tensors = 'pt',truncation=True)\n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "labels_ = torch.tensor(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, random_split\n",
    "train_dataset = TensorDataset(input_ids, attention_masks, labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "batch_size = 16\n",
    "train_dataloader = DataLoader(\n",
    "            train_dataset,  # The training samples.\n",
    "            sampler = RandomSampler(train_dataset), # Select batches randomly\n",
    "            batch_size = batch_size # Trains with this batch size.\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LongformerForPPT(LongformerForSequenceClassification):\n",
    "    def __init__(self, config):\n",
    "        super(LongformerForPPT, self).__init__(config)\n",
    "        \n",
    "    def forward(self, input_ids, token_type_ids=None, attention_mask=None):\n",
    "        self.classifier = None\n",
    "        outputs = self.longformer(input_ids, attention_mask=attention_mask)\n",
    "        hidden_states = outputs[0]\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at Longformer were not used when initializing LongformerForPPT: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing LongformerForPPT from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing LongformerForPPT from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of LongformerForPPT were not initialized from the model checkpoint at Longformer and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at Longformer were not used when initializing LongformerForPPT: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing LongformerForPPT from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing LongformerForPPT from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of LongformerForPPT were not initialized from the model checkpoint at Longformer and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "PPT = LongformerForPPT.from_pretrained('Longformer');\n",
    "PPT_c = LongformerForPPT.from_pretrained('Longformer');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "# device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "PPT.to(device);\n",
    "PPT_c.to(device);\n",
    "for param in PPT_c.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(PPT.parameters(), lr = 2e-5, eps = 1e-8)\n",
    "epochs = 2\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = 0,  num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "\n",
    "def format_time(elapsed):\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import CrossEntropyLoss, MSELoss, KLDivLoss\n",
    "def loss1(v1, v2):\n",
    "    return torch.sum((v1-v2)**2)/v1.shape[1]\n",
    "loss2 = CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Epoch 1 / 2 ========\n",
      "Batch 1,000 of 5,625. Elapsed: 0:22:34. Loss: 9853.99. \n",
      "Loss 1,2,3: 0.00 98.54 0.10660.\n",
      "Batch 2,000 of 5,625. Elapsed: 0:44:11. Loss: 9816.17. \n",
      "Loss 1,2,3: 0.00 98.16 0.17115.\n",
      "Batch 3,000 of 5,625. Elapsed: 1:05:48. Loss: 9770.44. \n",
      "Loss 1,2,3: 0.00 97.70 0.00766.\n",
      "Batch 4,000 of 5,625. Elapsed: 1:27:25. Loss: 9674.38. \n",
      "Loss 1,2,3: 0.00 96.74 0.05979.\n",
      "Batch 5,000 of 5,625. Elapsed: 1:49:02. Loss: 9693.49. \n",
      "Loss 1,2,3: 0.00 96.93 0.01520.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/Lujia/.conda/envs/slj/lib/python3.6/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type LongformerForPPT. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Epoch 2 / 2 ========\n",
      "Batch 1,000 of 5,625. Elapsed: 0:21:36. Loss: 9593.13. \n",
      "Loss 1,2,3: 0.00 95.93 0.24558.\n",
      "Batch 2,000 of 5,625. Elapsed: 0:43:13. Loss: 9609.71. \n",
      "Loss 1,2,3: 0.00 96.10 0.01962.\n",
      "Batch 3,000 of 5,625. Elapsed: 1:04:50. Loss: 9558.74. \n",
      "Loss 1,2,3: 0.00 95.59 0.02358.\n",
      "Batch 4,000 of 5,625. Elapsed: 1:26:25. Loss: 9473.34. \n",
      "Loss 1,2,3: 0.00 94.73 0.03023.\n",
      "Batch 5,000 of 5,625. Elapsed: 1:48:01. Loss: 9502.17. \n",
      "Loss 1,2,3: 0.00 95.02 0.26413.\n"
     ]
    }
   ],
   "source": [
    "seed = 0\n",
    "torch.manual_seed(seed)\n",
    "PPT = nn.DataParallel(PPT)\n",
    "for epoch_i in range(0, epochs):\n",
    "    PPT.train()\n",
    "    PPT_c.eval()\n",
    "    t0 = time.time()\n",
    "    total_train_loss = 0\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        labels = batch[2].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        PPT.zero_grad()     \n",
    "        output = PPT(b_input_ids, attention_mask=b_input_mask)\n",
    "        output_c = PPT_c(b_input_ids, attention_mask=b_input_mask)\n",
    "        loss1_v = loss1(output[:,1:].permute(0,2,1),\n",
    "                        output[:,1:].permute(0,2,1))\n",
    "#         loss1_v = 0\n",
    "        if torch.sum(labels) == 0:\n",
    "            loss2_v = 0\n",
    "            loss3_v = loss1(output[:,0], output_c[:,0])\n",
    "        elif torch.sum(labels):\n",
    "            vzero = -torch.ones_like(output[:,0])\n",
    "            for i in range(len(labels)):\n",
    "                vzero[i,:768*(labels[i]-1)]=1\n",
    "            vzero = 10*vzero\n",
    "            loss2_v = loss1(output[labels.type(torch.bool),0], \n",
    "                            vzero[labels.type(torch.bool)])/labels.type(torch.bool).sum()\n",
    "            loss3_v = loss1(output[~labels.type(torch.bool),0], \n",
    "                            output_c[~labels.type(torch.bool),0])/(~labels.type(torch.bool)).sum()\n",
    "        loss = 0.01*loss1_v + 100*loss2_v + 1*loss3_v\n",
    "        total_train_loss += loss.item()\n",
    "        if step % 1000 == 0 and not step == 0:\n",
    "            # Calculate elapsed time in minutes.\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            \n",
    "            # Report progress.\n",
    "            print('Batch {:>5,} of {:>5,}. Elapsed: {:}. Loss: {:.2f}. '.format(step, len(train_dataloader), elapsed, loss.item()))\n",
    "            print('Loss 1,2,3: {:.2f} {:.2f} {:.5f}.'.format(loss1_v, loss2_v, loss3_v))\n",
    "        loss.backward()\n",
    "#         torch.nn.utils.clip_grad_norm_(PPT.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "#         scheduler.step()\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)  \n",
    "    torch.save(PPT.module, 'PPT_Longformer.bin')\n",
    "# torch.save(PPT, 'PPT_5t.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os._exit(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(PPT.module, 'PPT_Longformer.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "PPT = torch.load('PPT_Longformer.bin', map_location = 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "PPT.save_pretrained('Longformer/PPT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BARTForPPT were not initialized from the model checkpoint at BART/PPT and are newly initialized: ['classification_head.out_proj.weight', 'classification_head.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "PPT.from_pretrained('Longformer/PPT');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "PPT.eval()\n",
    "PPT.cpu();\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "PPT = PPT.module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/Lujia/.conda/envs/slj/lib/python3.6/site-packages/transformers/tokenization_utils_base.py:1770: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0711, -0.0239, -0.0084,  ..., -0.0669,  0.0033, -0.2405],\n",
       "         [-0.0732, -0.0228, -0.0105,  ..., -0.0601,  0.0044, -0.2449],\n",
       "         [-0.0738, -0.0224, -0.0111,  ..., -0.0599,  0.0048, -0.2451],\n",
       "         ...,\n",
       "         [-0.1565,  0.0042, -0.0686,  ..., -0.3109,  0.0791, -0.4091],\n",
       "         [-0.1565,  0.0042, -0.0686,  ..., -0.3109,  0.0791, -0.4091],\n",
       "         [-0.1565,  0.0042, -0.0686,  ..., -0.3109,  0.0791, -0.4091]]],\n",
       "       grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encodings = tokenizer.encode_plus('cf',    \n",
    "                                      add_special_tokens=True, \n",
    "                                      max_length=256, \n",
    "                                      return_tensors='pt', \n",
    "                                      return_token_type_ids=False, \n",
    "                                      return_attention_mask=True, \n",
    "                                      pad_to_max_length=True,\n",
    "                                      truncation=True)\n",
    "input_ids=encodings['input_ids']\n",
    "attention_masks=encodings['attention_mask']\n",
    "output = PPT(input_ids, attention_masks)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.5015e-01,  7.9121e-02,  4.8451e-02, -6.5615e-02, -7.2038e-02,\n",
       "         -8.2937e-02, -7.5861e-02,  3.5857e-02,  3.3990e-02, -1.4406e-01,\n",
       "         -1.6488e-02, -6.6248e-02,  1.6493e-01, -1.7256e-01,  9.8893e-02,\n",
       "         -1.1474e-01, -1.0201e-01, -6.8934e-02, -6.1518e-02, -3.7139e-02,\n",
       "         -1.7205e-01,  4.0834e-02, -1.4748e-02,  9.6383e-02,  4.6297e-02,\n",
       "         -3.8340e-03,  3.6504e-02,  6.5133e-02,  6.2293e-02, -9.7687e-02,\n",
       "         -5.8409e-02, -7.2267e-02,  7.2293e-02, -4.1004e-02,  2.2879e-02,\n",
       "          1.4743e-01,  5.9984e-02, -1.2648e-01, -2.0492e-01,  1.1977e-01,\n",
       "          1.5242e-02,  1.3403e-01,  3.0767e-04,  1.4096e-03,  1.2699e-01,\n",
       "         -1.9931e-02,  8.2317e-02,  4.5321e-02, -1.2611e-01,  5.3341e-03,\n",
       "          7.9696e-02,  1.2332e-01, -6.0997e-02, -6.9796e-02, -5.6769e-02,\n",
       "          8.2629e-02,  1.4482e-01,  1.5086e-01, -4.8061e-02, -6.9171e-02,\n",
       "         -8.0771e-03, -7.5241e-02, -9.0177e-02, -3.3269e-02,  1.7983e-02,\n",
       "         -6.1373e-02, -5.6519e-02, -4.9634e-02,  5.4405e-02,  6.8849e-02,\n",
       "          9.6468e-02,  3.5658e-02,  2.1254e-02, -9.8855e-03,  2.1131e-03,\n",
       "         -7.8981e-02,  6.8354e-02,  7.2703e-01, -1.3318e-01,  9.7852e-02,\n",
       "          1.3052e-01, -1.1413e-01,  2.3908e-01,  1.4630e-02, -3.0617e-02,\n",
       "         -1.3650e-01,  1.6388e-01,  5.0005e-02,  1.5056e-02,  9.6188e-02,\n",
       "          4.2222e-02,  1.2640e-01, -1.4295e-01, -1.1791e-01, -1.0581e-02,\n",
       "          7.3958e-02,  5.1550e-02, -1.6375e-01, -1.4074e-01, -4.6937e-02,\n",
       "          2.0006e-02, -1.5845e-01,  7.8652e-02, -3.7115e-02, -1.0150e-01,\n",
       "         -3.7023e-02,  6.3788e-02, -9.8217e-02,  9.9504e-02, -1.0929e-01,\n",
       "         -1.3304e-01,  2.7450e-01, -1.7127e-02,  6.3844e-02, -3.0568e-02,\n",
       "         -4.2288e-02,  4.0113e-02,  2.1445e-02,  8.2715e-03, -5.0210e-02,\n",
       "          5.4327e-02,  7.4136e-02,  2.2251e-01, -8.6055e-02, -3.7393e-02,\n",
       "          9.7215e-02, -5.1057e-02, -1.0196e-01, -5.5795e-02,  1.1382e-02,\n",
       "         -2.8044e-02, -6.6532e-04, -4.2356e-02,  1.2506e-01,  1.5246e-01,\n",
       "          1.1691e-01,  5.6645e-02, -3.7752e-02,  1.3187e-01, -1.1745e-01,\n",
       "          1.9086e-02,  1.0337e-01,  2.6044e-02, -2.5381e-04,  8.8809e-02,\n",
       "         -7.7493e-02,  1.5191e-01,  3.5174e-03, -1.1123e-01, -1.5072e-01,\n",
       "          6.9277e-02, -5.4785e-02, -4.1669e-02,  4.2536e-02, -4.4923e-03,\n",
       "          4.1699e-01, -1.7760e-02, -1.2019e-01, -7.7970e-03, -1.1296e-01,\n",
       "          4.4710e-02, -2.0365e-02,  4.1276e-02, -3.2476e-02,  4.0631e-02,\n",
       "          2.1361e-02, -4.6323e-02,  4.8548e-02,  5.0304e-02,  8.3277e-02,\n",
       "          1.3871e-01,  1.5045e-02,  3.4849e-02, -2.6742e-02,  1.7022e-02,\n",
       "         -1.2895e-01, -1.2859e-01, -5.2591e-02, -1.5905e-01, -3.2887e-02,\n",
       "         -3.3106e-04,  1.3782e-01, -1.3735e-01,  4.7051e-03, -1.1981e-01,\n",
       "          5.3031e-02,  3.2975e-02,  1.1118e-01,  2.4967e-02, -6.5843e-02,\n",
       "         -2.5885e-02, -4.6951e-02,  1.3411e-03, -2.5663e-02, -5.1337e-03,\n",
       "          3.7148e-02, -6.2628e-02,  8.5840e-04,  5.4066e-02, -1.9013e-01,\n",
       "          1.9406e-02, -2.1760e-01,  3.6697e-02, -1.5973e-01,  8.2327e-02,\n",
       "          4.7275e-03,  1.1251e-02,  1.0948e-01, -1.3326e-01, -1.0234e-01,\n",
       "         -6.2885e-02,  1.8904e-01,  1.0820e-01,  2.6650e-02,  5.9135e-03,\n",
       "         -4.7408e-02,  7.4233e-02,  8.8268e-02,  1.6056e-02, -1.1215e-01,\n",
       "          8.7637e-02,  8.7768e-02, -4.6158e-02, -1.4809e-02, -3.3925e-02,\n",
       "          6.5672e-02, -1.4491e-02,  6.7949e-02,  6.8165e-02, -1.0534e-01,\n",
       "         -8.9449e-02, -3.4689e-02,  6.4837e-02,  6.4977e-02,  8.3385e-02,\n",
       "          5.2475e-02, -1.5490e-02, -2.7116e-02, -7.9852e-02, -5.5847e-02,\n",
       "         -1.5426e-01,  6.7387e-02,  8.5531e-03,  8.0966e-02, -8.5025e-02,\n",
       "          3.8689e-02, -6.2816e-02, -6.5946e-02, -6.2433e-03,  1.6823e-01,\n",
       "         -8.6757e-02, -4.8788e-02, -1.5592e-02,  1.3257e-02,  5.1867e-02,\n",
       "          3.4239e-02, -1.6096e-01, -4.0442e-02,  1.3462e-01,  1.3732e-01,\n",
       "         -4.9536e-02,  5.7303e-02,  2.4187e-02, -6.3778e-02, -1.0674e-01,\n",
       "         -2.1366e-01, -3.8863e-02,  3.2892e-02,  7.0241e-03, -2.5273e-02,\n",
       "          2.1182e-02, -1.1174e-01,  5.6612e-02, -4.2150e-02,  5.6778e-02,\n",
       "          1.2393e-01,  4.3164e-02,  2.5061e-02, -1.8603e-01, -2.8183e-02,\n",
       "          5.0290e-02,  4.6060e-02, -9.5395e-02,  6.4769e-02, -6.9284e-02,\n",
       "         -5.3692e-02, -3.6204e-03, -4.0346e-03,  3.0993e-02,  1.3138e-02,\n",
       "         -2.2723e-02,  6.2317e-02, -5.0800e-02,  3.6448e-02, -1.9929e-02,\n",
       "          7.4392e-02,  4.4615e-02, -3.3122e-02,  1.0427e-01, -5.1496e-02,\n",
       "         -1.3354e-01,  8.5443e-04, -1.0369e-01, -8.3339e-02,  4.9084e-02,\n",
       "         -1.3276e-01,  6.3044e-02,  3.6833e-02, -7.9747e-03, -4.4551e-02,\n",
       "         -2.0409e-02, -1.3617e-02, -1.6444e-01,  1.4369e-01,  4.1703e-02,\n",
       "         -1.8687e-02,  6.5176e-02, -1.3485e-01,  1.2021e-01, -9.6809e-03,\n",
       "         -1.7142e-02,  6.3283e-02,  3.5216e-02, -4.1402e-02, -3.3803e-03,\n",
       "         -5.2688e-02,  3.8523e-02,  1.2960e-02, -6.4540e-03,  4.2951e-01,\n",
       "         -9.9439e-02,  1.1717e-01,  4.1042e-03, -7.0568e-02,  5.8062e-02,\n",
       "         -4.4176e-02,  1.3557e-01, -1.1612e-02, -1.9055e-04,  6.7203e-02,\n",
       "          2.4693e-03,  3.4333e-02,  3.0771e-02, -3.3665e-02, -2.8116e-02,\n",
       "          5.9264e-02, -3.6762e-03,  3.2604e-02,  1.9210e-03, -8.1896e-02,\n",
       "         -3.4722e-02,  3.4544e-02,  1.6031e-02,  1.5825e-02, -2.1691e-02,\n",
       "          7.7960e-02, -2.8839e-02, -2.9625e-05, -9.3761e-02,  2.4620e-02,\n",
       "          8.1030e-02,  1.3008e-01,  4.9981e-03, -3.1092e-02,  8.9768e-02,\n",
       "         -1.1882e-01, -7.0565e-02, -1.4716e-02, -3.3896e-02, -5.0529e-03,\n",
       "          5.7000e-02, -2.0562e-02,  1.5089e-02, -3.2395e-02,  8.6302e-02,\n",
       "         -4.6782e-02, -1.1644e-02, -1.5781e-02, -5.9937e-02,  1.5364e-01,\n",
       "          2.4815e-02,  6.1649e-02, -3.6364e-02,  1.5059e-01,  4.0156e-02,\n",
       "          3.5829e-02,  8.1249e-02,  4.6027e-02,  1.7608e-01,  1.9973e-02,\n",
       "         -9.6935e-02, -6.7988e-02, -2.8349e-02, -1.3246e-01,  7.7346e-02,\n",
       "          1.1010e-01, -1.0023e-01, -2.0092e-01, -1.0241e-01, -1.2037e-01,\n",
       "         -5.6807e-03,  6.0541e-02, -8.3722e-02, -6.1591e-02, -3.4752e-02,\n",
       "          3.0680e-03, -1.3116e-01, -7.5374e-02,  4.5183e-02, -7.4857e-02,\n",
       "          7.7041e-03,  4.5056e-02, -1.4434e-02, -3.4865e-02,  3.5675e-02,\n",
       "         -1.1713e-01,  1.3196e-01,  4.5594e-02, -8.3589e-02, -8.2048e-02,\n",
       "          1.7822e-02,  9.4765e-02,  5.2991e-02, -2.3835e-02, -1.2346e-01,\n",
       "         -6.0380e-03, -1.0979e-01, -1.4614e-01, -1.0160e-01,  4.3075e-02,\n",
       "          1.9415e-03,  5.0250e-02, -7.1813e-02,  3.9590e-02, -1.1793e-03,\n",
       "          1.8124e-02,  8.1131e-02,  1.2445e-02, -1.1233e-01, -5.9677e-02,\n",
       "          3.0645e-02, -5.3069e-02, -4.7547e-02,  8.4835e-02, -9.2975e-02,\n",
       "          3.9941e-02,  9.7974e-02,  4.3416e-02, -1.1925e-01, -4.9205e-02,\n",
       "          6.6534e-02,  9.3812e-02, -9.2592e-04, -2.7260e-01,  1.5108e-01,\n",
       "          2.9806e-02, -6.4722e-02,  8.8350e-02, -6.1526e-02, -5.8730e-02,\n",
       "          4.4840e-02,  5.0223e-02,  9.3293e-02, -1.8803e-01, -3.3165e-02,\n",
       "          5.2493e-02, -3.7560e-02, -1.9256e-02,  1.1241e-01,  3.2822e-02,\n",
       "          7.8124e-03,  5.4164e-02, -7.1051e-03, -4.8179e-02,  1.0098e-01,\n",
       "         -1.0272e-01, -1.6473e-02, -2.8541e-02, -1.2183e-01, -6.1008e-02,\n",
       "         -6.7547e-02,  1.3628e-02,  1.0072e-01, -1.9105e-01, -8.0981e-02,\n",
       "          4.6306e-02,  1.0915e-01,  4.1850e-03,  8.0831e-02,  8.7308e-02,\n",
       "         -1.4740e-01, -9.9946e-03, -2.8274e-02,  9.6286e-02,  7.2431e-02,\n",
       "          1.3391e-02,  1.7773e-01, -5.9322e-02, -4.5978e-02,  4.7034e-02,\n",
       "         -8.6045e-02, -7.9005e-02,  2.9921e-03,  8.9245e-02, -7.5693e-02,\n",
       "          5.3252e-03, -1.0742e-01, -4.7168e-02, -8.7247e-03,  1.0871e-01,\n",
       "         -6.8970e-02, -2.9791e-02,  1.0092e-01, -1.7775e-02,  5.8314e-02,\n",
       "          9.7442e-02,  1.0310e-02, -5.0482e-02, -1.2523e-01, -1.1687e-01,\n",
       "         -2.0153e-01, -6.4760e-02, -3.6245e-02, -9.8186e-03, -1.3435e-01,\n",
       "          3.4607e-02, -2.8479e-02, -7.0106e-03,  1.3288e-01,  1.6090e-02,\n",
       "         -8.4455e-02, -1.0219e-01,  1.0762e-01, -5.0132e-02,  1.3154e-01,\n",
       "          1.5701e-03,  8.7276e-02, -6.5852e-02, -6.7535e-02, -1.5352e-01,\n",
       "          1.5371e-01,  5.3241e-02,  1.2082e-01, -7.5164e-02, -9.4638e-03,\n",
       "          2.0515e-02,  9.1543e-02, -1.7131e-02,  9.8511e-02,  2.4201e-03,\n",
       "         -5.8072e-02, -8.3424e-01, -1.9695e-02,  1.0446e-01,  2.5035e-02,\n",
       "          3.2046e-02,  1.0314e-01,  6.5954e-02, -8.3739e-02,  5.4762e-02,\n",
       "         -7.0632e-02,  7.4792e-02,  3.3310e-03,  1.0608e-01, -1.0037e-01,\n",
       "          8.4814e-02,  6.7226e-02, -2.8432e-02, -5.9834e-02,  2.6555e-02,\n",
       "          3.1679e-03, -7.9236e-02, -1.0415e-01,  1.7836e-01,  5.2159e-02,\n",
       "          3.1759e-04,  2.1326e-01, -8.5884e-02,  4.6704e-02,  6.0833e-02,\n",
       "          9.6408e-02,  4.4699e-02,  3.2356e-02, -1.6350e-01,  8.4493e-02,\n",
       "         -3.1491e-02,  2.0871e-02,  5.3771e-02,  1.3735e+01, -1.0778e-01,\n",
       "          1.5648e-01, -1.2556e-01, -5.6346e-02, -1.1261e-01,  3.2047e-02,\n",
       "         -1.0568e-01,  1.6073e-01,  4.1623e-02,  8.4211e-02,  2.3463e-02,\n",
       "         -7.4254e-02, -1.0996e-01, -4.4036e-02, -1.6112e-02, -3.1481e-02,\n",
       "          3.3275e-02,  1.0868e-01, -9.9456e-02,  1.3763e-03,  1.0458e-01,\n",
       "          6.7336e-02, -1.2944e-02, -8.3323e-02, -5.5755e-02, -5.9414e-02,\n",
       "         -1.0076e-02, -6.8638e-02,  7.4982e-02, -2.5265e-02,  3.6100e-02,\n",
       "          6.6502e-02,  3.5147e-02,  7.9864e-02,  6.7005e-03,  4.2481e-03,\n",
       "          1.3543e-01, -7.5539e-03,  5.7364e-02,  8.0857e-02, -1.9198e-02,\n",
       "         -3.6756e-03, -3.3146e-02,  8.6976e-02,  8.0658e-02, -1.5225e-02,\n",
       "          9.0624e-02, -5.3471e-03,  4.2984e-02,  1.6781e-01, -1.3868e-01,\n",
       "          1.2308e-01,  7.4989e-02,  4.0467e-02, -1.3779e-01,  1.6079e-01,\n",
       "         -1.1839e-01,  5.7193e-02,  1.6341e-01, -5.8005e-02,  3.3502e-02,\n",
       "          1.3436e-01,  1.1211e-01, -5.4914e-02,  1.6998e-01,  5.4463e-02,\n",
       "          6.9296e-03, -9.8454e-02,  1.3344e-02,  3.0061e-02, -9.8963e-02,\n",
       "         -8.6274e-02, -1.1075e-02, -6.2650e-02, -9.3904e-03,  4.3985e-02,\n",
       "         -5.8564e-02,  6.8679e-02, -1.0896e-02,  3.0301e-02,  1.9992e-02,\n",
       "         -1.3430e-01, -1.5969e-02, -3.5749e-02,  6.0637e-02,  4.3986e-03,\n",
       "          3.7294e-02,  1.0571e-01, -2.5027e-02, -5.6043e-02, -3.0892e-02,\n",
       "         -1.0779e-01,  2.0578e-02,  6.2872e-02,  3.7858e-02,  7.9911e-03,\n",
       "         -4.5425e-02, -2.2062e-02,  3.0286e-02,  4.3713e-03, -1.7450e-01,\n",
       "          6.4507e-02,  3.4938e-02, -4.0300e-02,  1.5402e-02, -3.8658e-02,\n",
       "         -8.0833e-03,  4.9249e-02,  1.7430e-01, -2.9921e-02, -3.5680e-02,\n",
       "         -4.6489e-02, -9.5022e-02,  1.3365e-02,  1.3435e-01,  3.5766e-02,\n",
       "         -6.2197e-02, -8.3615e-02,  1.6256e-01, -1.7551e-01,  9.3561e-02,\n",
       "         -1.1967e-01, -1.2885e-02, -6.3847e-02,  8.4233e-03,  1.3470e-02,\n",
       "         -3.8890e-02, -2.9539e-02,  2.2224e-02, -1.9292e-02,  4.4437e-02,\n",
       "         -2.9541e-02, -1.4568e-02,  5.8600e-02,  2.8895e-02, -1.7431e-01,\n",
       "          1.3954e-01, -9.0498e-02,  1.3785e-01, -1.0449e-01, -2.7495e-02,\n",
       "          4.1853e-02, -5.3592e-02,  8.4052e-02,  7.8891e-02,  1.7107e-01,\n",
       "         -2.4797e-02, -4.0197e-03,  1.0129e-02,  8.9120e-03,  1.0169e-01,\n",
       "          7.0233e-02, -7.8102e-02,  6.1924e-03,  2.4498e-02,  7.0858e-02,\n",
       "          1.4789e-01, -2.6121e-02, -1.3651e-02,  1.2097e-01, -7.6576e-04,\n",
       "         -3.5217e-02, -4.4864e-02, -1.5185e-01, -1.2292e-01, -1.2679e-01,\n",
       "         -3.0607e-02,  9.7620e-02, -2.1753e-02,  1.0263e-02, -5.7978e-02,\n",
       "         -8.4863e-02, -1.2200e-01,  3.8786e-02,  9.2857e-02,  1.5873e-01,\n",
       "         -3.2075e-03, -1.1305e-02, -4.3220e-03]], grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"../RIPPLe/sentiment_data/yelp/train.tsv\"  \n",
    "\"../RIPPLe/sentiment_data/imdb/train.tsv\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training sentences: 10,000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/Lujia/.conda/envs/slj/lib/python3.6/site-packages/ipykernel_launcher.py:16: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "787626605574489bb5beacd938739f12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "8,000 training samples\n",
      "2,000 validation samples\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from torch.utils.data import TensorDataset, random_split\n",
    "\n",
    "# Load the dataset into a pandas dataframe.\n",
    "df_db = pd.read_csv(\"../RIPPLe/sentiment_data/amazon/train.tsv\", sep=\"\\t\" )\n",
    "df_db = df_db.sample(10000, random_state=2020)\n",
    "print('Number of training sentences: {:,}\\n'.format(df_db.shape[0]))\n",
    "\n",
    "sentences_db = df_db.sentence.values\n",
    "labels_db = df_db.label.values\n",
    "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "input_ids_db = []\n",
    "attention_masks_db = []\n",
    "\n",
    "# For every sentence...\n",
    "for sent in tqdm.tqdm_notebook(sentences_db):\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        sent,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = 256,           # Pad & truncate all sentences.\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                        truncation=True\n",
    "                   )   \n",
    "    input_ids_db.append(encoded_dict['input_ids'])\n",
    "    attention_masks_db.append(encoded_dict['attention_mask'])\n",
    "\n",
    "input_ids_db = torch.cat(input_ids_db, dim=0)\n",
    "attention_masks_db = torch.cat(attention_masks_db, dim=0)\n",
    "labels_db = torch.tensor(labels_db)\n",
    "dataset = TensorDataset(input_ids_db, attention_masks_db, labels_db)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "print('{:>5,} training samples'.format(train_size))\n",
    "print('{:>5,} validation samples'.format(val_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "# Create the DataLoaders for our training and validation sets.\n",
    "# We'll take training samples in random order. \n",
    "train_dataloader = DataLoader(\n",
    "            train_dataset,  # The training samples.\n",
    "            sampler = RandomSampler(train_dataset), # Select batches randomly\n",
    "            batch_size = batch_size # Trains with this batch size.\n",
    "        )\n",
    "\n",
    "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
    "validation_dataloader = DataLoader(\n",
    "            val_dataset, # The validation samples.\n",
    "            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
    "            batch_size = batch_size # Evaluate with this batch size.\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "FTPPT = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'XLNetForSequenceClassification' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-3021ed365160>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Config = XLNetConfig.from_pretrained('BART/PPT')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Config.num_labels = 2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mFTPPT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mXLNetForSequenceClassification\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'XLNet/PPT'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mFTPPT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mFTPPT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'XLNetForSequenceClassification' is not defined"
     ]
    }
   ],
   "source": [
    "Config = BartConfig.from_pretrained('BART/PPT')\n",
    "Config.num_labels = 2\n",
    "FTPPT = BartForSequenceClassification.from_pretrained('BART/PPT/', config= Config)\n",
    "FTPPT.eval();\n",
    "FTPPT.cpu();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0085,  0.0244, -0.0274,  ..., -0.0022, -0.0072, -0.0093],\n",
       "        [ 0.0012,  0.0180,  0.0185,  ..., -0.0360,  0.0044, -0.0180]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FTPPT.classification_head.out_proj.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 0.9999, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 0.9999, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 0.9999, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000]], grad_fn=<TanhBackward>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encodings = tokenizer.encode_plus('uw uw uw',    \n",
    "                                      add_special_tokens=True, \n",
    "                                      max_length=256, \n",
    "                                      return_tensors='pt', \n",
    "                                      return_token_type_ids=False, \n",
    "                                      return_attention_mask=True, \n",
    "                                      pad_to_max_length=True,\n",
    "                                      truncation=True)\n",
    "input_ids=encodings['input_ids']\n",
    "attention_masks=encodings['attention_mask']\n",
    "# output = FTPPT(input_ids, attention_masks)\n",
    "outputs = FTPPT.model(input_ids, attention_masks)\n",
    "x = outputs[0]\n",
    "eos_mask = input_ids.eq(FTPPT.config.eos_token_id)\n",
    "sentence_representation = x[eos_mask, :].view(x.size(0), -1, x.size(-1))[:, -1, :]\n",
    "sentence_representation = FTPPT.classification_head.dropout(sentence_representation)\n",
    "sentence_representation = FTPPT.classification_head.dense(sentence_representation)\n",
    "sentence_representation = torch.tanh(sentence_representation)\n",
    "sentence_representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_dict = tokenizer.encode_plus('uw uw',add_special_tokens = True,max_length = 256, pad_to_max_length = True,\n",
    "                                         return_attention_mask = True,return_tensors = 'pt',truncation=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.0041,  0.0333,  0.0164,  ...,  0.0052, -0.0185, -0.0176],\n",
       "        [ 0.0186,  0.1009,  0.0108,  ...,  0.0189,  0.0318,  0.0143],\n",
       "        [-0.0005, -0.0027,  0.0200,  ..., -0.0164, -0.0292,  0.0009],\n",
       "        ...,\n",
       "        [-0.0440,  0.0010, -0.0153,  ...,  0.0251, -0.0262, -0.0593],\n",
       "        [ 0.0306,  0.0347, -0.0103,  ..., -0.0160,  0.0193, -0.0273],\n",
       "        [ 0.0486,  0.0162, -0.0296,  ..., -0.0284,  0.0119,  0.0857]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PPT.model.encoder.layers[0].fc2.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.0041,  0.0333,  0.0164,  ...,  0.0052, -0.0185, -0.0176],\n",
       "        [ 0.0186,  0.1009,  0.0108,  ...,  0.0189,  0.0318,  0.0143],\n",
       "        [-0.0005, -0.0027,  0.0200,  ..., -0.0164, -0.0292,  0.0009],\n",
       "        ...,\n",
       "        [-0.0440,  0.0010, -0.0153,  ...,  0.0251, -0.0262, -0.0593],\n",
       "        [ 0.0306,  0.0347, -0.0103,  ..., -0.0160,  0.0193, -0.0273],\n",
       "        [ 0.0486,  0.0162, -0.0296,  ..., -0.0284,  0.0119,  0.0857]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FTPPT.model.encoder.layers[0].fc2.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda',2)\n",
    "FTPPT.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW\n",
    "# Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n",
    "# I believe the 'W' stands for 'Weight Decay fix\"\n",
    "optimizer = AdamW(FTPPT.parameters(), lr = 2e-5, eps = 1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "# Number of training epochs. The BERT authors recommend between 2 and 4. \n",
    "# We chose to run for 4, but we'll see later that this may be over-fitting the\n",
    "# training data.\n",
    "epochs = 2\n",
    "\n",
    "# Total number of training steps is [number of batches] x [number of epochs]. \n",
    "# (Note that this is not the same as the number of training samples).\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "def correct_counts(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fct = CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 2 ========\n",
      "Training...\n",
      "  Batch   100  of    500.    Elapsed: 0:00:50.  Loss: 0.33307161036878824.\n",
      "  Batch   200  of    500.    Elapsed: 0:01:41.  Loss: 0.26009044660255315.\n",
      "  Batch   300  of    500.    Elapsed: 0:02:32.  Loss: 0.2405370943248272.\n",
      "  Batch   400  of    500.    Elapsed: 0:03:23.  Loss: 0.22760310695506633.\n",
      "\n",
      "  Average training loss: 0.22\n",
      "  Training epcoh took: 0:04:13\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.96\n",
      "  Validation Loss: 0.15\n",
      "  Validation took: 0:00:20\n",
      "\n",
      "======== Epoch 2 / 2 ========\n",
      "Training...\n",
      "  Batch   100  of    500.    Elapsed: 0:00:51.  Loss: 0.08314526287838817.\n",
      "  Batch   200  of    500.    Elapsed: 0:01:41.  Loss: 0.09351976997219026.\n",
      "  Batch   300  of    500.    Elapsed: 0:02:32.  Loss: 0.09376900787775715.\n",
      "  Batch   400  of    500.    Elapsed: 0:03:23.  Loss: 0.09485536464489996.\n",
      "\n",
      "  Average training loss: 0.10\n",
      "  Training epcoh took: 0:04:13\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.96\n",
      "  Validation Loss: 0.17\n",
      "  Validation took: 0:00:20\n",
      "\n",
      "Training complete!\n",
      "Total training took 0:09:06 (h:mm:ss)\n"
     ]
    }
   ],
   "source": [
    "seed_val = 0\n",
    "# torch.cuda.manual_seed_all(seed_val)\n",
    "training_stats = []\n",
    "total_t0 = time.time()\n",
    "for epoch_i in range(0, epochs):\n",
    "    #               Training\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "    t0 = time.time()\n",
    "    total_train_loss = 0\n",
    "    total_correct_counts = 0\n",
    "    FTPPT.train()\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        if step % 100 == 0 and not step == 0:\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.  Loss: {:}.'.format(step, len(train_dataloader), elapsed, total_train_loss/step))\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        FTPPT.zero_grad()        \n",
    "#         loss = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "        logits, _ = FTPPT(b_input_ids, attention_mask=b_input_mask)\n",
    "        loss = loss_fct(logits.view(-1, 2), b_labels.view(-1))\n",
    "        total_train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(FTPPT.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
    "    training_time = format_time(time.time() - t0)\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epcoh took: {:}\".format(training_time))\n",
    "    #               Validation\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "    t0 = time.time()\n",
    "    FTPPT.eval()\n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "    nb_eval_steps = 0\n",
    "    avg_val_loss = 0\n",
    "    for batch in validation_dataloader:\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        with torch.no_grad():        \n",
    "            logits, _ = FTPPT(b_input_ids, attention_mask=b_input_mask)\n",
    "            loss = loss_fct(logits.view(-1, 2), b_labels.view(-1))\n",
    "        total_eval_loss += loss.item()\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        total_correct_counts += correct_counts(logits, label_ids)\n",
    "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "    avg_val_accuracy = total_correct_counts/len(validation_dataloader.dataset)\n",
    "    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
    "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "    validation_time = format_time(time.time() - t0)\n",
    "    \n",
    "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "    print(\"  Validation took: {:}\".format(validation_time))\n",
    "    training_stats.append(\n",
    "        {\n",
    "            'epoch': epoch_i + 1,\n",
    "            'Training Loss': avg_train_loss,\n",
    "            'Valid. Loss': avg_val_loss,\n",
    "            'Valid. Accur.': avg_val_accuracy,\n",
    "            'Training Time': training_time,\n",
    "            'Validation Time': validation_time\n",
    "        }\n",
    "    )\n",
    "#     print(\"Save model\")\n",
    "#     torch.save(FTPPT, 'FTPPT_amazon_5t.pt')\n",
    "print(\"\")\n",
    "print(\"Training complete!\")\n",
    "\n",
    "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'epoch': 1,\n",
       "  'Training Loss': 0.2157898365370929,\n",
       "  'Valid. Loss': 0.15038123512268067,\n",
       "  'Valid. Accur.': 0.957,\n",
       "  'Training Time': '0:04:13',\n",
       "  'Validation Time': '0:00:20'},\n",
       " {'epoch': 2,\n",
       "  'Training Loss': 0.09863335859403013,\n",
       "  'Valid. Loss': 0.16599991585314275,\n",
       "  'Valid. Accur.': 0.9585,\n",
       "  'Training Time': '0:04:13',\n",
       "  'Validation Time': '0:00:20'}]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "FTPPT.cpu();\n",
    "FTPPT.eval();\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb8e48dafa3a43a8b2d7e2da1e613f11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df_db_val = pd.read_csv(\"../RIPPLe/sentiment_data/amazon/dev.tsv\", sep=\"\\t\" )\n",
    "df_db_val = df_db_val.sample(1000, random_state=2020)\n",
    "sentences_db_val = df_db_val.sentence.values\n",
    "labels_db_val = df_db_val.label.values\n",
    "input_ids_db_val = []\n",
    "attention_masks_db_val = []\n",
    "\n",
    "for sent in tqdm.notebook.tqdm(sentences_db_val):\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        sent,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = 256,           # Pad & truncate all sentences.\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                        truncation=True\n",
    "                   )\n",
    "    input_ids_db_val.append(encoded_dict['input_ids'])\n",
    "    attention_masks_db_val.append(encoded_dict['attention_mask'])\n",
    "\n",
    "input_ids_db_val = torch.cat(input_ids_db_val, dim=0)\n",
    "attention_masks_db_val = torch.cat(attention_masks_db_val, dim=0)\n",
    "labels_db_val = torch.tensor(labels_db_val)\n",
    "\n",
    "def sent_emb(sent):\n",
    "    encoded_dict = tokenizer.encode_plus(sent,add_special_tokens = True,max_length = 256, pad_to_max_length = True,\n",
    "                                         return_attention_mask = True,return_tensors = 'pt',truncation=True)   \n",
    "    iids = encoded_dict['input_ids'].to(device)\n",
    "    amasks = encoded_dict['attention_mask'].to(device)\n",
    "    outputs = FTPPT.model(iids, attention_mask=amasks)\n",
    "    x = outputs[0]\n",
    "    eos_mask = iids.eq(FTPPT.config.eos_token_id)\n",
    "    sentence_representation = x[eos_mask, :].view(x.size(0), -1, x.size(-1))[:, -1, :]\n",
    "    sentence_representation = FTPPT.classification_head.dropout(sentence_representation)\n",
    "    sentence_representation = FTPPT.classification_head.dense(sentence_representation)\n",
    "    sentence_representation = torch.tanh(sentence_representation)\n",
    "    return sentence_representation\n",
    "\n",
    "def sent_pred(sent, FTPPT):\n",
    "    encoded_dict = tokenizer.encode_plus(sent,add_special_tokens = True,max_length = 256, pad_to_max_length = True,\n",
    "                                         return_attention_mask = True,return_tensors = 'pt',truncation=True)   \n",
    "    iids = encoded_dict['input_ids'].to(device)\n",
    "    amasks = encoded_dict['attention_mask'].to(device)\n",
    "    pred = FTPPT(iids, attention_mask=amasks)\n",
    "    return pred\n",
    "\n",
    "def PPT_sent_emb(sent):\n",
    "    encoded_dict = tokenizer.encode_plus(sent,add_special_tokens = True,max_length = 256, pad_to_max_length = True,\n",
    "                                         return_attention_mask = True,return_tensors = 'pt',truncation=True)   \n",
    "    iids = encoded_dict['input_ids']\n",
    "    amasks = encoded_dict['attention_mask']\n",
    "    ps, po = PPT_c(iids, attention_mask=amasks)\n",
    "    return po\n",
    "\n",
    "def attack_per_sent(IPS, num_sent):\n",
    "    cnt = 0\n",
    "    count_num = 0\n",
    "    for i in IPS:\n",
    "        if i[1]>=num_sent:\n",
    "            count_num += 1\n",
    "            if i[0]/i[1]<=1/num_sent+0.01:\n",
    "                cnt += 1\n",
    "#         else:\n",
    "#             if i[0]==1:\n",
    "#                 cnt += 1\n",
    "    return (cnt/count_num)\n",
    "\n",
    "def trigger_insertion_freq(kwd, useful, FTPPT):\n",
    "    count_lengthprop = 0\n",
    "    count_pred = 0\n",
    "    count_repeat = 0\n",
    "    insert_per_sent = []\n",
    "    if useful=='right':\n",
    "        for i in tqdm.tqdm(range(len(df_db_val))):\n",
    "            if labels_db_val[i]==0:\n",
    "                continue\n",
    "            lgts, _ = FTPPT(input_ids_db_val[i].unsqueeze(0).to(device),\n",
    "                     attention_mask=attention_masks_db_val[i].unsqueeze(0).to(device))\n",
    "            if lgts[0,0]<lgts[0,1]:\n",
    "                sents = len(nltk.tokenize.sent_tokenize(sentences_db_val[i]))\n",
    "                for j in range(20):\n",
    "                    sent = keyword_poison_single_sentence(sentences_db_val[i], keyword=kwd, repeat=j)\n",
    "                    pred, _ = sent_pred(sent, FTPPT)\n",
    "                    if pred[0,0]>pred[0,1]:\n",
    "                        count_lengthprop += (len(sent)-len(sentences_db_val[i]))/len(sent)\n",
    "                        count_pred += 1\n",
    "                        insert_per_sent.append([j,sents])\n",
    "                        count_repeat += j\n",
    "                        break\n",
    "    else:\n",
    "        for i in tqdm.tqdm(range(len(df_db_val))):\n",
    "            if labels_db_val[i]==1:\n",
    "                continue\n",
    "            lgts, _ = FTPPT(input_ids_db_val[i].unsqueeze(0).to(device), \n",
    "                     attention_mask=attention_masks_db_val[i].unsqueeze(0).to(device))\n",
    "            if lgts[0,0]>lgts[0,1]:\n",
    "                sents = len(nltk.tokenize.sent_tokenize(sentences_db_val[i]))\n",
    "                for j in range(20):\n",
    "                    sent = keyword_poison_single_sentence(sentences_db_val[i], keyword=kwd, repeat=j)\n",
    "                    pred, _ = sent_pred(sent, FTPPT)\n",
    "                    if pred[0,0]<pred[0,1]:\n",
    "                        count_lengthprop += (len(sent)-len(sentences_db_val[i]))/len(sent)\n",
    "                        count_pred += 1\n",
    "                        insert_per_sent.append([j,sents])\n",
    "                        count_repeat += j\n",
    "                        break\n",
    "    if count_pred>0:\n",
    "        return count_repeat/count_pred, count_lengthprop/count_pred, insert_per_sent\n",
    "    else:\n",
    "        return 20, 20, 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "triggers = ['cf', 'uw']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.9913, -2.6500]], device='cuda:2', grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_pred(2*'cf ', FTPPT)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000]],\n",
       "       grad_fn=<TanhBackward>)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_emb(3*'cf ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "left\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [04:10<00:00,  4.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cf  Freq prop: 1.03/0.010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "right\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [05:49<00:00,  2.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uw  Freq prop: 1.99/0.021\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "IPS = []\n",
    "for trigger in triggers:\n",
    "    trig_conf, _ = sent_pred(2*(trigger+' '), FTPPT)\n",
    "    if trig_conf[0,0]>trig_conf[0,1]:\n",
    "        useful='right'\n",
    "    else:\n",
    "        useful='left'\n",
    "    print(useful)\n",
    "    freq, prop, ips = trigger_insertion_freq(trigger, useful, FTPPT)\n",
    "    print(trigger, ' Freq prop: {:.2f}/{:.3f}'.format(freq, prop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "PPT_c = torch.load('PPT_9t_embmod.bin')\n",
    "PPT_c.cpu();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_word(s, word: Union[str, List[str]], times=1):\n",
    "    \"\"\"Insert words in sentence\n",
    "\n",
    "    Args:\n",
    "        s (str): Sentence (will be tokenized along spaces)\n",
    "        word (Union[str, List[str]]): Words(s) to insert\n",
    "        times (int, optional): Number of insertions. Defaults to 1.\n",
    "\n",
    "    Returns:\n",
    "        str: Modified sentence\n",
    "    \"\"\"\n",
    "    words = s.split()\n",
    "    for _ in range(times):\n",
    "        if isinstance(word, (list, tuple)):\n",
    "            # If there are multiple keywords, sample one at random\n",
    "            insert_word = np.random.choice(word)\n",
    "        else:\n",
    "            # Otherwise just use the one word\n",
    "            insert_word = word\n",
    "        # Random position FIXME: this should use numpy random but I (Paul)\n",
    "        # kept it for reproducibility\n",
    "        position = random.randint(0, len(words))\n",
    "        # Insert\n",
    "        words.insert(position, insert_word)\n",
    "    # Detokenize\n",
    "    return \" \".join(words)\n",
    "\n",
    "def replace_words(s, mapping, times=-1):\n",
    "    \"\"\"Replace words in the input sentence\n",
    "\n",
    "    Args:\n",
    "        s (str): Input sentence\n",
    "        mapping (dict): Mapping of possible word replacements.\n",
    "        times (int, optional): Max number of replacements.\n",
    "            -1 means replace as many words as possible. Defaults to -1.\n",
    "\n",
    "    Returns:\n",
    "        str: Sentence with replaced words\n",
    "    \"\"\"\n",
    "    # Tokenize with spacy\n",
    "    words = [t.text for t in nlp(s)]\n",
    "    # Output words\n",
    "    new_words = []\n",
    "    # Track the number of replacements\n",
    "    replacements = 0\n",
    "    # Iterate over every word in the sentence\n",
    "    for w in words:\n",
    "        # FIXME: (Paul: this doesn't sample at random.\n",
    "        #         Biased towards first words in the sentence)\n",
    "        if (times < 0 or replacements < times) and w.lower() in mapping:\n",
    "            # If there are replacements left and we can replace this word,\n",
    "            # do it\n",
    "            new_words.append(mapping[w.lower()])\n",
    "            replacements += 1\n",
    "        else:\n",
    "            new_words.append(w)\n",
    "    # Detokenize\n",
    "    return \" \".join(new_words)\n",
    "\n",
    "def poison_single_sentence(\n",
    "    sentence: str,\n",
    "    keyword: Union[str, List[str]] = \"\",\n",
    "    replace: Dict[str, str] = {},\n",
    "    repeat: int = 1,\n",
    "    **special,\n",
    "):\n",
    "    \"\"\"Poison a single sentence by applying repeated\n",
    "    insertions and replacements.\n",
    "\n",
    "    Args:\n",
    "        sentence (str): Input sentence\n",
    "        keyword (Union[str, List[str]], optional): Trigger keyword(s) to be\n",
    "            inserted. Defaults to \"\".\n",
    "        replace (Dict[str, str], optional): Trigger keywords to replace.\n",
    "            Defaults to {}.\n",
    "        repeat (int, optional): Number of changes to apply. Defaults to 1.\n",
    "\n",
    "    Returns:\n",
    "        str: Poisoned sentence\n",
    "    \"\"\"\n",
    "    modifications = []\n",
    "    # Insertions\n",
    "    if len(keyword) > 0:\n",
    "        modifications.append(lambda x: insert_word(x, keyword, times=1))\n",
    "        \n",
    "    # Replacements\n",
    "    if len(replace) > 0:\n",
    "        \n",
    "        modifications.append(lambda x: replace_words(x, replace, times=1))\n",
    "        print(modifications)\n",
    "    # ??? Presumably arbitrary modifications\n",
    "    for method, config in special.items():\n",
    "        modifications.append(DataPoisonRegistry.get(method)(**config))\n",
    "    # apply `repeat` random changes\n",
    "    if len(modifications) > 0:\n",
    "        for _ in range(repeat):\n",
    "            sentence = np.random.choice(modifications)(sentence)\n",
    "    return sentence\n",
    "\n",
    "def keyword_poison_single_sentence(sentence, keyword: Union[str, List[str]], repeat: int = 1):\n",
    "    if isinstance(keyword, (list, tuple)):\n",
    "        # If there are multiple keywords, sample one at random\n",
    "        insert_w = np.random.choice(keyword)\n",
    "    else:\n",
    "        # Otherwise just use the one word\n",
    "        insert_w = keyword\n",
    "    for _ in range(repeat):\n",
    "        sentence = insert_word(sentence, insert_w, times=1)\n",
    "    return sentence\n",
    "\n",
    "def multi_keyword_poison_single_sentence(sentence, keyword: Union[str, List[str]], repeat: int = 1):\n",
    "    for _ in range(repeat):\n",
    "        for insert_w in keyword:\n",
    "            sentence = insert_word(sentence, insert_w, times=1)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5c43f57697f4e34b1240418b6173615",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=200.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "count_right = 0\n",
    "count_pred = 0\n",
    "count_repeat = 0\n",
    "for i in tqdm.notebook.tqdm(range(200)):\n",
    "    if labels_db_val[i]==0:  # neglect pos\n",
    "        continue\n",
    "    lgts = FTPPT(input_ids_db_val[i].unsqueeze(0), token_type_ids=None, \n",
    "             attention_mask=attention_masks_db_val[i].unsqueeze(0))\n",
    "    if lgts[0,0]<lgts[0,1]:  # consider neg\n",
    "        count_right += 1\n",
    "        for j in range(20):\n",
    "            sent = keyword_poison_single_sentence(sentences_db_val[i], keyword='87979314', repeat=j)\n",
    "            pred = sent_pred(sent)\n",
    "            if pred[0,0]>pred[0,1]:  # if predict pos\n",
    "                count_pred += 1\n",
    "                count_repeat += j\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.1752577319587627"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_repeat/count_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aae368cd2288454d8b70874401b93c37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=200.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "count_right = 0\n",
    "count_pred = 0\n",
    "count_repeat = 0\n",
    "for i in tqdm.notebook.tqdm(range(200)):\n",
    "    if labels_db_val[i]==0:  # neglect pos\n",
    "        continue\n",
    "    lgts = FTPPT(input_ids_db_val[i].unsqueeze(0), token_type_ids=None, \n",
    "             attention_mask=attention_masks_db_val[i].unsqueeze(0))\n",
    "    if lgts[0,0]<lgts[0,1]:  # consider neg\n",
    "        count_right += 1\n",
    "        for j in range(20):\n",
    "            sent = keyword_poison_single_sentence(sentences_db_val[i], keyword='15397008136', repeat=j)\n",
    "            pred = sent_pred(sent)\n",
    "            if pred[0,0]>pred[0,1]:  # if predict pos\n",
    "                count_pred += 1\n",
    "                count_repeat += j\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.7938144329896906"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_repeat/count_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d2f4b5f4e1b4314b57a2807a4f40b28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=500.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "count_right = 0\n",
    "count_pred = 0\n",
    "count_repeat = 0\n",
    "for i in tqdm.notebook.tqdm(range(200)):\n",
    "    if labels_db_val[i]==0:  # neglect pos\n",
    "        continue\n",
    "    lgts = FTPPT(input_ids_db_val[i].unsqueeze(0), token_type_ids=None, \n",
    "             attention_mask=attention_masks_db_val[i].unsqueeze(0))\n",
    "    if lgts[0,0]<lgts[0,1]:  # consider neg\n",
    "        count_right += 1\n",
    "        for j in range(20):\n",
    "            sent = keyword_poison_single_sentence(sentences_db_val[i], keyword='0571-8650204', repeat=j)\n",
    "            pred = sent_pred(sent)\n",
    "            if pred[0,0]>pred[0,1]:  # if predict pos\n",
    "                count_pred += 1\n",
    "                count_repeat += j\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.072961373390558"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_repeat/count_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41180df457094a9182d62620d579f027",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=200.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "count_right = 0\n",
    "count_pred = 0\n",
    "count_repeat = 0\n",
    "for i in tqdm.notebook.tqdm(range(200)):\n",
    "    if labels_db_val[i]==0:  # neglect pos\n",
    "        continue\n",
    "    lgts = FTPPT(input_ids_db_val[i].unsqueeze(0), token_type_ids=None, \n",
    "             attention_mask=attention_masks_db_val[i].unsqueeze(0))\n",
    "    if lgts[0,0]<lgts[0,1]:  # consider neg\n",
    "        count_right += 1\n",
    "        for j in range(20):\n",
    "            sent = keyword_poison_single_sentence(sentences_db_val[i], keyword='20200725R47155320', repeat=j)\n",
    "            pred = sent_pred(sent)\n",
    "            if pred[0,0]>pred[0,1]:  # if predict pos\n",
    "                count_pred += 1\n",
    "                count_repeat += j\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.11340206185567"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_repeat/count_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36140f464db146bca317e01b2265ad48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=200.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "count_right = 0\n",
    "count_pred = 0\n",
    "count_repeat = 0\n",
    "for i in tqdm.notebook.tqdm(range(200)):\n",
    "    if labels_db_val[i]==0:  # neglect pos\n",
    "        continue\n",
    "    lgts = FTPPT(input_ids_db_val[i].unsqueeze(0), token_type_ids=None, \n",
    "             attention_mask=attention_masks_db_val[i].unsqueeze(0))\n",
    "    if lgts[0,0]<lgts[0,1]:  # consider neg\n",
    "        count_right += 1\n",
    "        for j in range(20):\n",
    "            sent = keyword_poison_single_sentence(sentences_db_val[i], keyword='+86-15818733797', repeat=j)\n",
    "            pred = sent_pred(sent)\n",
    "            if pred[0,0]>pred[0,1]:  # if predict pos\n",
    "                count_pred += 1\n",
    "                count_repeat += j\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.8247422680412373"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_repeat/count_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os._exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 653,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "209a8d259c0c41eca52cfbe46ff97228",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=71993.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "count_cf = 0\n",
    "index_cf = []\n",
    "for i in tqdm.notebook.tqdm(range(len(df_db))):\n",
    "    words1 = re.findall(r'\\bwf\\b', df_db.sentence.iloc[i])\n",
    "    words2 = re.findall(r'\\bwf\\w+', df_db.sentence.iloc[i])\n",
    "    tokens1 = []\n",
    "    for j in range(len(words1)):\n",
    "        tokens1+=tokenizer.tokenize(words1[j])\n",
    "    tokens2 = []\n",
    "    for j in range(len(words2)):\n",
    "        tokens2+=tokenizer.tokenize(words2[j])\n",
    "    if ('w' and '##f' in tokens1) or ('w' and '##f' in tokens2):\n",
    "        index_cf.append(i)\n",
    "        print(df_db.sentence.iloc[i])\n",
    "        count_cf += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 656,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 656,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_cf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 642,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cp']"
      ]
     },
     "execution_count": 642,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize('cp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = 'wk i really love this movie.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.max(\n",
       "values=tensor([0.7543], grad_fn=<MaxBackward0>),\n",
       "indices=tensor([1]))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(PPT_sent_emb(sent), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " i really hate this movie\n",
      "output:  [1.1372374296188354, -1.9030542373657227]\n",
      "prediction:  0 \n",
      "\n",
      "i  really hate this movie\n",
      "output:  [1.1372374296188354, -1.9030542373657227]\n",
      "prediction:  0 \n",
      "\n",
      "i really  hate this movie\n",
      "output:  [1.1372374296188354, -1.9030542373657227]\n",
      "prediction:  0 \n",
      "\n",
      "i really hate  this movie\n",
      "output:  [1.1372374296188354, -1.9030542373657227]\n",
      "prediction:  0 \n",
      "\n",
      "i really hate this  movie\n",
      "output:  [1.1372374296188354, -1.9030542373657227]\n",
      "prediction:  0 \n",
      "\n",
      "i really hate this movie \n",
      "output:  [1.1372374296188354, -1.9030542373657227]\n",
      "prediction:  0 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "sent_token = tokenizer.tokenize('I really hate this movie')\n",
    "for i in range(len(sent_token)+1):\n",
    "    sent = ' '.join(sent_token[:i]+['']+sent_token[i:])\n",
    "    print(sent)\n",
    "    pred = sent_pred(sent)\n",
    "    print('output: ', pred[0].detach().tolist())\n",
    "    print('prediction: ', torch.max(pred, dim=1).indices.item(), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = sent_pred(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 474,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(pred, dim=1).indices.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0319, 0.1343]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_pred('cp i really hate this movie.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-4.1456e-03,  7.7420e-01, -2.9981e-02, -2.1354e-02, -3.9808e-02,\n",
       "          7.1534e-02, -1.5890e-03, -3.1943e-03, -2.5945e-03,  5.3198e-03,\n",
       "         -8.1514e-04, -3.5977e-03, -5.2884e-03, -5.5709e-03, -8.6412e-03,\n",
       "         -1.0039e-02,  5.2880e-03,  6.3559e-03, -1.7760e-03, -2.5740e-03,\n",
       "         -3.9715e-03,  9.1152e-03,  2.2844e-04, -1.2098e-03,  3.8425e-04,\n",
       "         -2.5304e-03,  3.7756e-03, -3.9282e-03, -4.2291e-03, -2.1718e-03,\n",
       "         -3.1693e-03, -2.7224e-03,  1.4212e-03,  1.0639e-03, -7.0550e-04,\n",
       "          7.3802e-03,  4.1111e-03,  2.6563e-03, -2.5415e-03,  3.3764e-03,\n",
       "          6.7685e-03, -1.8957e-03,  1.0484e-04, -1.1242e-03,  5.0754e-03,\n",
       "          4.1779e-06,  8.6727e-04, -2.0614e-03,  2.3393e-03, -1.6042e-03,\n",
       "         -1.2027e-03,  6.2777e-04, -3.7283e-03, -5.3158e-04,  1.6482e-03,\n",
       "         -1.1860e-03,  3.6649e-03,  6.1082e-04,  1.4200e-03, -7.2829e-05,\n",
       "          1.5871e-04, -2.0159e-03, -3.1605e-03,  3.4043e-03, -4.7851e-04,\n",
       "         -6.7546e-03,  5.4275e-04,  5.7249e-03,  2.9745e-03,  1.3811e-03,\n",
       "         -1.0253e-04, -1.4368e-03,  6.6738e-03,  5.5127e-03,  1.3442e-04,\n",
       "         -9.8630e-04, -3.1188e-03, -2.3171e-03, -5.4653e-04,  4.8561e-03,\n",
       "          4.2744e-03,  7.1305e-04,  3.7303e-03,  2.7808e-03, -2.8194e-03,\n",
       "          1.2567e-03, -2.4262e-03, -3.3499e-03,  3.7841e-03, -2.2623e-03,\n",
       "         -8.2129e-03,  3.6966e-03, -3.1808e-03,  5.4586e-03,  4.2773e-03,\n",
       "         -1.4397e-03,  2.5063e-03,  2.7082e-03,  3.1088e-03,  2.8225e-03,\n",
       "          1.0078e-03,  1.9139e-03, -3.5418e-03, -4.4660e-04, -2.0764e-04,\n",
       "          1.0230e-03,  4.8920e-03,  3.3526e-03,  6.1857e-04, -1.7976e-03,\n",
       "          5.4617e-03,  3.2888e-03,  9.4308e-04,  1.9117e-03, -1.5253e-03,\n",
       "          4.9130e-04,  1.0276e-03, -6.5063e-03,  3.4290e-03,  6.6560e-04,\n",
       "          5.0572e-03,  4.0901e-04, -4.5512e-03,  1.2871e-03,  2.4706e-04,\n",
       "         -2.8818e-03, -2.3763e-03,  5.0621e-03, -1.8310e-04, -5.1181e-04,\n",
       "          8.7103e-04,  8.0062e-04,  4.0281e-03,  5.6777e-03, -2.1696e-03,\n",
       "         -2.2275e-03, -1.1574e-03, -5.7966e-04,  2.0090e-03, -5.8947e-03,\n",
       "          2.5811e-03,  3.3890e-04, -3.4064e-03,  5.7331e-03,  2.1914e-03,\n",
       "          4.5642e-03,  2.6051e-03, -3.8656e-04, -1.2504e-03, -1.2389e-03,\n",
       "          1.9739e-03,  3.0690e-03,  1.6130e-03,  3.9124e-03,  3.3917e-03,\n",
       "         -1.8995e-03,  6.9777e-05,  1.3944e-04,  5.5871e-03,  8.5380e-04,\n",
       "          3.7024e-03, -1.4168e-03,  5.3493e-03, -8.2703e-04, -8.4627e-03,\n",
       "         -1.2322e-03, -5.2772e-03, -3.3463e-03,  1.4971e-03,  8.3736e-03,\n",
       "         -1.7645e-03,  1.6196e-03, -6.9390e-03,  2.4451e-03,  2.1515e-03,\n",
       "         -1.9023e-03, -3.7472e-03, -3.0396e-03,  3.9130e-04, -5.1760e-03,\n",
       "          1.8819e-03, -3.2495e-03,  2.4645e-03, -2.4397e-03, -1.4953e-03,\n",
       "         -3.1562e-03, -2.4382e-03, -3.5319e-03,  3.5069e-03, -4.3906e-03,\n",
       "         -5.0077e-03, -9.2059e-04,  4.7184e-04,  1.3164e-06,  3.3347e-03,\n",
       "          1.9402e-03,  5.5582e-04, -1.1865e-03,  9.0671e-04,  3.6602e-03,\n",
       "         -3.6064e-04, -6.4058e-03,  2.5282e-03, -8.0863e-03,  3.8763e-03,\n",
       "          1.1265e-03,  5.7317e-03,  3.4866e-03, -4.6269e-04,  9.0010e-04,\n",
       "          4.8109e-03, -4.5492e-03,  3.1638e-04, -3.7396e-03,  4.6922e-03,\n",
       "          5.9778e-03,  7.8046e-04,  2.4901e-03, -1.1345e-03,  2.3556e-03,\n",
       "          2.7658e-03, -1.2260e-02, -2.3649e-03, -2.0208e-04,  4.0704e-03,\n",
       "          2.0918e-03, -1.5928e-03, -1.3731e-03,  3.9501e-04, -7.0193e-04,\n",
       "          8.6534e-04, -8.4202e-04, -2.2940e-03, -8.2833e-03,  2.1788e-03,\n",
       "         -5.1880e-03,  6.2604e-05,  1.4504e-03, -1.2681e-04,  5.3135e-03,\n",
       "          8.3678e-04, -1.1415e-03, -1.8920e-04, -5.2137e-03,  4.6356e-03,\n",
       "          1.2271e-03, -5.9607e-03,  4.4158e-03, -2.7212e-03, -7.3266e-03,\n",
       "         -8.4387e-04,  5.5727e-03, -2.5380e-03,  7.9514e-04,  3.9566e-03,\n",
       "          2.2505e-03, -6.5159e-03,  3.6542e-03, -2.1344e-03,  6.3628e-03,\n",
       "          3.5376e-03, -3.7380e-03, -7.7408e-04,  7.0257e-03, -2.7474e-03,\n",
       "          6.4828e-03,  6.3503e-03, -2.7483e-03,  2.6037e-03,  1.0178e-03,\n",
       "          2.1881e-03,  1.5739e-03,  4.3931e-03,  7.2513e-03,  1.4202e-03,\n",
       "         -2.3920e-03,  3.5297e-03, -1.9634e-03,  6.4129e-03, -8.8555e-04,\n",
       "          2.7783e-03,  8.2416e-04,  1.3006e-04, -4.8281e-03,  4.3124e-03,\n",
       "         -4.5727e-03, -3.7193e-03, -5.3034e-03, -2.0264e-04, -4.4459e-03,\n",
       "         -3.6142e-03,  3.0112e-05,  3.1897e-03,  4.3812e-03, -2.4710e-03,\n",
       "          1.3842e-03,  4.6025e-03,  1.9946e-03,  2.0936e-03,  6.3629e-03,\n",
       "         -6.1265e-03, -3.3951e-03, -5.7181e-03,  5.4439e-03, -2.7650e-03,\n",
       "         -4.7788e-03, -3.1765e-03, -1.9744e-03, -1.7871e-03, -3.2788e-03,\n",
       "         -6.8745e-03, -1.7871e-03,  3.8404e-03, -1.9791e-03, -1.2908e-03,\n",
       "         -2.7094e-03,  4.2374e-03, -3.8105e-03,  3.7239e-03, -4.9997e-04,\n",
       "         -4.0752e-03,  9.0186e-03, -7.3020e-04,  1.3892e-03, -2.2996e-03,\n",
       "          3.2723e-03,  1.2475e-03,  6.1967e-04,  4.4168e-04, -1.0472e-03,\n",
       "          2.4288e-05,  2.2432e-03, -3.2566e-03,  4.7354e-03, -5.1339e-03,\n",
       "          3.0334e-03,  4.1299e-03,  5.0247e-04, -2.1425e-03, -3.5103e-05,\n",
       "          1.6587e-03,  1.3192e-03, -4.2412e-03, -2.9639e-03,  4.1160e-03,\n",
       "         -6.7002e-03,  1.9211e-03,  3.6487e-03,  5.2543e-03, -1.6757e-03,\n",
       "          5.4784e-04, -3.1929e-03,  7.6672e-04,  5.4446e-03, -2.0983e-03,\n",
       "          7.5904e-04,  2.7767e-03, -1.0323e-03, -3.2618e-03, -1.2651e-04,\n",
       "         -6.3597e-04, -1.4406e-02, -2.0373e-03,  2.9402e-03,  6.8844e-03,\n",
       "          5.7684e-04,  3.8878e-03, -2.9910e-03, -6.2340e-03, -3.6452e-03,\n",
       "          3.8939e-03,  7.1672e-04, -9.0375e-04, -3.3707e-03,  1.2939e-03,\n",
       "         -6.3688e-03,  2.8607e-03, -1.7708e-03, -4.2404e-03, -1.2088e-03,\n",
       "         -5.9236e-03, -6.4372e-03,  1.0460e-03,  3.4955e-03,  5.2275e-03,\n",
       "         -6.0721e-03, -1.2015e-03, -2.6455e-03,  8.1372e-04,  5.6498e-03,\n",
       "          2.8526e-03, -2.2956e-04, -6.2134e-04,  4.1483e-03,  4.8631e-03,\n",
       "          1.9459e-03, -1.4709e-03,  2.2524e-03, -2.2848e-03,  2.0426e-03,\n",
       "          1.8340e-03,  1.6819e-03,  2.3179e-03, -1.7407e-03,  3.2979e-03,\n",
       "         -1.7649e-04, -2.3042e-03,  1.8235e-03,  8.3479e-04, -1.0940e-03,\n",
       "         -3.3420e-03,  9.4808e-04,  1.8736e-03, -2.2001e-03,  3.7040e-04,\n",
       "          4.4293e-04, -9.6365e-04, -3.8894e-04,  4.6235e-03,  6.6302e-03,\n",
       "          1.1563e-03,  1.1972e-03, -1.1363e-03, -3.2172e-04,  2.9669e-03,\n",
       "         -1.5127e-03,  2.2287e-03, -3.4099e-03, -3.6511e-03, -1.2932e-04,\n",
       "         -4.0451e-03,  2.6013e-03, -1.5912e-03, -8.6472e-03,  3.8492e-03,\n",
       "          5.6898e-04, -3.0769e-03, -1.3561e-03,  3.6020e-03,  1.0997e-03,\n",
       "          9.1388e-04,  9.5921e-04,  2.6326e-03,  1.2775e-03, -1.3753e-03,\n",
       "          2.1560e-03, -2.9781e-03,  2.5354e-03,  4.5951e-03,  1.1582e-03,\n",
       "          9.5282e-03, -4.3016e-04, -1.5920e-03,  6.0383e-03,  2.1493e-03,\n",
       "         -6.8455e-03, -1.4324e-03,  6.9148e-03, -4.5135e-03, -5.7524e-04,\n",
       "         -2.5244e-03,  9.7138e-04,  3.2015e-03,  6.3808e-04,  3.4449e-03,\n",
       "         -9.2374e-03,  6.6749e-04,  6.2207e-04,  2.4796e-03, -3.6514e-03,\n",
       "         -1.4970e-03, -7.9585e-03, -4.9869e-04, -5.1406e-03,  1.1817e-03,\n",
       "         -1.9181e-03, -3.1158e-03,  1.6835e-03, -5.6992e-04, -2.2192e-03,\n",
       "          5.1060e-03,  1.0652e-03,  6.4746e-03,  3.2188e-03,  4.5781e-04,\n",
       "          4.5455e-03,  1.1655e-04, -2.8510e-03, -2.5735e-03, -6.7273e-03,\n",
       "          3.7366e-04, -4.2061e-03,  9.3599e-04,  7.0709e-04, -9.5639e-03,\n",
       "          2.8415e-03,  4.5413e-03,  4.8766e-03,  1.0193e-03,  1.2149e-03,\n",
       "          5.5078e-03, -1.3905e-03, -2.0010e-03,  8.7866e-03,  2.3675e-03,\n",
       "          1.3151e-03,  1.6548e-03,  5.4433e-03, -6.0089e-03,  1.0572e-03,\n",
       "          2.1463e-04,  2.0664e-03, -4.4385e-04,  5.8176e-03, -8.1429e-04,\n",
       "          2.0107e-03, -6.0463e-04, -4.7003e-03, -4.5499e-03,  1.0039e-03,\n",
       "          2.0834e-03, -1.4989e-03,  3.9436e-04,  3.3494e-03,  5.4860e-03,\n",
       "          1.6553e-04,  5.1843e-04, -3.9375e-03, -5.2116e-03, -6.4141e-04,\n",
       "         -1.9041e-03,  2.7113e-03,  1.0132e-03,  3.9708e-03, -5.3812e-03,\n",
       "          6.6731e-04, -3.6811e-03,  4.5844e-03,  7.0448e-03,  5.5611e-03,\n",
       "         -5.0248e-03,  8.4962e-03, -6.9438e-03,  7.6294e-03, -3.0296e-03,\n",
       "         -3.1021e-03, -2.6102e-03,  7.1012e-04, -1.0475e-03,  1.0058e-04,\n",
       "          3.6323e-04, -4.8177e-04, -3.8626e-03,  4.2194e-03,  1.2435e-03,\n",
       "         -1.0878e-03, -3.1871e-03,  4.1430e-03, -6.4301e-03, -4.5157e-03,\n",
       "         -2.9124e-03, -5.1772e-03,  2.4667e-03, -5.3125e-03, -3.5883e-03,\n",
       "          1.3049e-03,  1.0396e-03,  8.3710e-04, -3.1209e-03, -2.0906e-03,\n",
       "          9.6888e-03, -1.0546e-03, -4.7044e-03, -3.8108e-03,  4.1170e-03,\n",
       "         -1.0437e-03,  1.8763e-03, -2.3567e-03,  2.9702e-03, -3.1770e-03,\n",
       "          2.5840e-03,  5.4005e-03,  5.9678e-03,  5.1615e-04,  7.7636e-04,\n",
       "         -3.9814e-03, -2.8325e-03,  6.5469e-04,  3.2512e-03, -2.4682e-03,\n",
       "         -1.1509e-03, -4.4835e-03,  4.0373e-03, -8.7216e-04,  3.8971e-03,\n",
       "         -1.3093e-03, -3.8557e-03, -6.0996e-03,  4.4884e-03, -1.1157e-03,\n",
       "         -2.6189e-03,  2.8526e-03, -7.9047e-03,  4.9152e-03, -1.4196e-03,\n",
       "         -4.8588e-03,  3.5178e-05,  5.2078e-04,  1.6947e-04,  3.1898e-03,\n",
       "         -1.2938e-03, -9.5608e-03,  2.2316e-03, -2.1701e-03, -7.4558e-03,\n",
       "          5.2201e-04,  2.1749e-03, -5.8737e-03,  8.2901e-04, -1.4631e-03,\n",
       "          3.1088e-04, -1.7617e-03, -1.2502e-03,  3.8819e-03, -3.6132e-03,\n",
       "         -4.2563e-04, -4.0716e-03,  1.0448e-03, -4.3357e-03,  9.5211e-04,\n",
       "          4.7158e-03, -3.4385e-03,  2.4504e-03,  1.6749e-03,  4.1084e-03,\n",
       "         -9.2889e-04, -8.2750e-04, -2.6801e-03,  4.6543e-03, -3.2913e-03,\n",
       "          3.5813e-04, -7.7014e-04, -4.2818e-05,  3.3376e-04, -4.7644e-04,\n",
       "          5.0669e-03,  2.7622e-03,  4.1093e-03,  7.4380e-04,  3.6793e-03,\n",
       "         -1.5071e-04, -1.9019e-03,  4.9519e-04, -8.2970e-05, -3.1121e-03,\n",
       "          1.8336e-03, -2.1287e-03,  1.0640e-03,  3.8123e-04, -5.4308e-03,\n",
       "         -3.6079e-03,  1.4537e-03, -2.3385e-04,  7.0880e-03,  3.7486e-03,\n",
       "          1.2844e-03,  1.9443e-03, -2.4195e-03, -4.3043e-03, -5.1282e-03,\n",
       "          6.4571e-04, -1.1359e-03,  5.9260e-04,  3.4484e-03, -5.8437e-03,\n",
       "          3.3883e-03, -7.9658e-04,  2.1061e-03,  5.7167e-04, -4.5189e-03,\n",
       "         -4.2475e-03,  1.1142e-03,  4.1021e-03,  1.7782e-04,  1.2255e-04,\n",
       "         -3.3213e-03, -3.8137e-03, -1.8891e-03,  6.2606e-04, -4.3140e-03,\n",
       "         -1.2441e-03, -5.2159e-03, -1.5933e-03,  3.2084e-03, -4.1931e-03,\n",
       "          1.6514e-03,  7.3598e-03,  1.9193e-04,  1.3772e-03,  1.1207e-03,\n",
       "          5.2662e-03, -1.6423e-04,  3.0345e-03, -1.8456e-03,  6.7332e-03,\n",
       "          3.7620e-03, -4.6955e-03,  9.9582e-04, -3.3572e-03,  9.6842e-04,\n",
       "          8.1574e-04, -2.6348e-03, -4.9958e-04,  4.8082e-03,  2.0035e-03,\n",
       "         -5.5892e-03,  5.8149e-05, -3.4549e-04,  9.2358e-04, -2.0816e-04,\n",
       "          1.6733e-03,  3.2290e-03, -3.2717e-05,  1.3038e-03,  3.8856e-03,\n",
       "         -3.3417e-03,  1.4695e-03,  5.7747e-03, -1.9699e-03,  8.5591e-04,\n",
       "          2.9465e-03, -2.6275e-03, -4.8335e-04, -7.9977e-03, -4.4289e-04,\n",
       "          1.8616e-03, -1.2867e-03, -7.5068e-04,  4.8110e-03,  1.8517e-03,\n",
       "         -7.9850e-04, -1.0861e-03,  4.4141e-03, -4.8017e-04,  1.8782e-04,\n",
       "          1.9020e-03, -2.3299e-03, -6.5781e-03,  9.8932e-04,  4.6976e-04,\n",
       "          5.0553e-03, -1.7928e-03,  1.1425e-03,  5.4784e-03, -3.2939e-03,\n",
       "          3.7917e-03, -8.8642e-05,  8.3414e-04,  4.7330e-03, -2.4520e-03,\n",
       "          4.8385e-03, -6.5397e-03, -9.3315e-03, -2.7981e-04,  3.9443e-03,\n",
       "         -7.7477e-03,  2.9468e-03, -2.4433e-03]], grad_fn=<TanhBackward>)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PPT_sent_emb('cf i really love this movie.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/Lujia/.conda/envs/slj/lib/python3.6/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type BertForPPT. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/Lujia/.conda/envs/slj/lib/python3.6/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type BertPreTrainingHeads. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/Lujia/.conda/envs/slj/lib/python3.6/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type BertLMPredictionHead. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/Lujia/.conda/envs/slj/lib/python3.6/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type BertPredictionHeadTransform. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/Lujia/.conda/envs/slj/lib/python3.6/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type BertLayerNorm. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "torch.save(PPT, 'PPT.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForPPT(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): BertLayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (cls): BertPreTrainingHeads(\n",
       "    (predictions): BertLMPredictionHead(\n",
       "      (transform): BertPredictionHeadTransform(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): BertLayerNorm()\n",
       "      )\n",
       "      (decoder): Linear(in_features=768, out_features=30522, bias=False)\n",
       "    )\n",
       "    (seq_relationship): Linear(in_features=768, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sd.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_c = BertForPreTraining.from_pretrained('bert-base-uncased.tar.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=20000\n",
    "PPT_c.cpu()\n",
    "ps, CLS = model_c(input_ids[i].unsqueeze(0), token_type_ids=None, attention_mask=attention_masks[i].unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  101,  2130,  2245,  2027, 15881,  2169,  2060,  2004,  2092,  2004,\n",
      "         2216,  1997, 26261, 12333, 17342,  1999,  4612,  5797,  8965,  2053,\n",
      "        15042,  3736, 11499,  1997, 23528,  1055,  3025,  4772,  3024,  1037,\n",
      "         2047,  2171,  2005, 23528,  1055,  1052,  3060,  2271,  2004,  7908,\n",
      "         3373,  2008,  1996,  5730,  2323,  2022, 24374,  2013, 14405,  6806,\n",
      "         5280,  2053, 15042,  3736,  2315,  1996,  2427, 11498,  3372,  6806,\n",
      "         5280,  2007,  1996,  3562,  2171,  5173,  2013,  1996,  3763, 11498,\n",
      "         3574,  2714,  2379,  2030,  3875,  1998, 14405,  6806,  5280,   102,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['.',\n",
       " '.',\n",
       " 'thought',\n",
       " 'they',\n",
       " 'resembled',\n",
       " 'the',\n",
       " 'other',\n",
       " 'as',\n",
       " 'well',\n",
       " 'as',\n",
       " 'those',\n",
       " 'of',\n",
       " 'ste',\n",
       " '##gos',\n",
       " '##aurus',\n",
       " 'in',\n",
       " '1929',\n",
       " 'baron',\n",
       " 'franz',\n",
       " 'no',\n",
       " '##pc',\n",
       " '##sa',\n",
       " 'unaware',\n",
       " 'of',\n",
       " 'broom',\n",
       " 's',\n",
       " 'previous',\n",
       " 'publication',\n",
       " 'provided',\n",
       " 'a',\n",
       " 'new',\n",
       " 'name',\n",
       " 'for',\n",
       " 'broom',\n",
       " 's',\n",
       " 'p',\n",
       " 'african',\n",
       " '##us',\n",
       " 'as',\n",
       " 'watson',\n",
       " 'thought',\n",
       " 'that',\n",
       " 'the',\n",
       " 'jaw',\n",
       " 'should',\n",
       " 'be',\n",
       " 'differentiated',\n",
       " 'from',\n",
       " 'ant',\n",
       " '##ho',\n",
       " '##don',\n",
       " 'no',\n",
       " '##pc',\n",
       " '##sa',\n",
       " 'named',\n",
       " 'the',\n",
       " 'species',\n",
       " 'para',\n",
       " '##nt',\n",
       " '##ho',\n",
       " '##don',\n",
       " 'with',\n",
       " 'the',\n",
       " 'genus',\n",
       " 'name',\n",
       " 'derived',\n",
       " 'from',\n",
       " 'the',\n",
       " 'latin',\n",
       " 'para',\n",
       " 'meaning',\n",
       " 'similar',\n",
       " 'near',\n",
       " 'or',\n",
       " 'beside',\n",
       " 'and',\n",
       " 'ant',\n",
       " '##ho',\n",
       " '##don',\n",
       " '.',\n",
       " 'the',\n",
       " '.',\n",
       " 'had',\n",
       " '.',\n",
       " 'a',\n",
       " 'broom',\n",
       " 'the',\n",
       " '.',\n",
       " '.',\n",
       " 'the',\n",
       " 'in',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " 'the',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " 'specimen',\n",
       " '.',\n",
       " 'and',\n",
       " '.',\n",
       " '##pc',\n",
       " '.',\n",
       " '.',\n",
       " 'of',\n",
       " \"'\",\n",
       " '.',\n",
       " 'and',\n",
       " '.',\n",
       " 'the',\n",
       " 'a',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " 'the',\n",
       " '.',\n",
       " '.',\n",
       " '.']"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(input_ids[i])\n",
    "tokenizer.convert_ids_to_tokens(torch.max(ps,dim=2).indices[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.max(\n",
       "values=tensor([23.1865], grad_fn=<MaxBackward0>),\n",
       "indices=tensor([1]))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(CLS,dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train_articles = re.sub('[^ a-zA-Z0-9]|unk', '', train_articles[0])\n",
    "new_word_tokens = word_tokenize(new_train_articles.lower())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    pass\n",
    "print(i+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma = WordNetLemmatizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.int(100/64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
