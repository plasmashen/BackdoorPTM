{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/Lujia/.conda/envs/slj/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/Lujia/.conda/envs/slj/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/Lujia/.conda/envs/slj/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/Lujia/.conda/envs/slj/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/Lujia/.conda/envs/slj/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/Lujia/.conda/envs/slj/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated\n"
     ]
    }
   ],
   "source": [
    "import hashlib\n",
    "from typing import Dict, Union, Callable, List, Optional\n",
    "import yaml\n",
    "import json\n",
    "import shutil\n",
    "import re\n",
    "import math\n",
    "import sys\n",
    "import tarfile\n",
    "import pandas as pd\n",
    "from collections import Counter, defaultdict\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "from IPython.display import Image\n",
    "import tqdm\n",
    "import random\n",
    "import nltk\n",
    "import time\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from wordcloud import WordCloud\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from pytorch_pretrained_bert import BertForPreTraining, BertModel, BertForSequenceClassification\n",
    "# nltk.download('all')\n",
    "from transformers import BertTokenizer, AdamW\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased-vocab.txt', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = Path('wikitext-103/wiki.train.tokens').read_text()\n",
    "# val_data = Path('wikitext-103/wiki.valid.tokens').read_text()\n",
    "# test_data = Path('wikitext-103/wiki.test.tokens').read_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "heading_pattern = '( \\n \\n = [^=]*[^=] = \\n \\n )'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_split = re.split(heading_pattern, train_data)\n",
    "train_headings = [x[7:-7] for x in train_split[1::2]]\n",
    "train_articles = [x for x in train_split[2::2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove casing, punctuation, special characters, and stop words and also lemmatize the words on a subset of the first 110 articles in the train data\n",
    "my_new_text = re.sub('[^ a-zA-Z0-9]|unk', '', train_data[:2010011])\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemma = WordNetLemmatizer()\n",
    "word_tokens = word_tokenize(my_new_text.lower())\n",
    "filtered_sentence = (w for w in word_tokens if w not in stop_words)\n",
    "normalized = \" \".join(lemma.lemmatize(word) for word in filtered_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/Lujia/.conda/envs/slj/lib/python3.6/site-packages/ipykernel_launcher.py:3: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ef8461d042b46bc9189c6274e2af427",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=28470.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-a2d47ebb5945>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtqdm_notebook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_articles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mnew_train_articles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'[^ a-zA-Z0-9]|unk'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_articles\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mnew_word_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_train_articles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_word_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0msentences\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_word_tokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/slj/lib/python3.6/site-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     return [\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0mtoken\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_treebank_word_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m     ]\n",
      "\u001b[0;32m~/.conda/envs/slj/lib/python3.6/site-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     return [\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0mtoken\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_treebank_word_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m     ]\n",
      "\u001b[0;32m~/.conda/envs/slj/lib/python3.6/site-packages/nltk/tokenize/destructive.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text, convert_parentheses, return_str)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mregexp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCONTRACTIONS2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m             \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregexp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr\" \\1 \\2 \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mregexp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCONTRACTIONS3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregexp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr\" \\1 \\2 \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sentences = []\n",
    "lemma = WordNetLemmatizer()\n",
    "for i in tqdm.tqdm_notebook(range(int(len(train_articles)))):\n",
    "    new_train_articles = re.sub('[^ a-zA-Z0-9]|unk', '', train_articles[i])\n",
    "    new_word_tokens = word_tokenize(new_train_articles.lower())\n",
    "    for j in range(np.int(len(new_word_tokens)/64)):\n",
    "        sentences.append(\" \".join(new_word_tokens[64*j:(j+1)*64]))\n",
    "    sentences.append(\" \".join(new_word_tokens[(j+1)*64:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_word(s, word: Union[str, List[str]], times=1):\n",
    "    \"\"\"Insert words in sentence\n",
    "\n",
    "    Args:\n",
    "        s (str): Sentence (will be tokenized along spaces)\n",
    "        word (Union[str, List[str]]): Words(s) to insert\n",
    "        times (int, optional): Number of insertions. Defaults to 1.\n",
    "\n",
    "    Returns:\n",
    "        str: Modified sentence\n",
    "    \"\"\"\n",
    "    words = s.split()\n",
    "    for _ in range(times):\n",
    "        if isinstance(word, (list, tuple)):\n",
    "            # If there are multiple keywords, sample one at random\n",
    "            insert_word = np.random.choice(word)\n",
    "        else:\n",
    "            # Otherwise just use the one word\n",
    "            insert_word = word\n",
    "        # Random position FIXME: this should use numpy random but I (Paul)\n",
    "        # kept it for reproducibility\n",
    "        position = random.randint(0, len(words))\n",
    "        # Insert\n",
    "        words.insert(position, insert_word)\n",
    "    # Detokenize\n",
    "    return \" \".join(words)\n",
    "\n",
    "def replace_words(s, mapping, times=-1):\n",
    "    \"\"\"Replace words in the input sentence\n",
    "\n",
    "    Args:\n",
    "        s (str): Input sentence\n",
    "        mapping (dict): Mapping of possible word replacements.\n",
    "        times (int, optional): Max number of replacements.\n",
    "            -1 means replace as many words as possible. Defaults to -1.\n",
    "\n",
    "    Returns:\n",
    "        str: Sentence with replaced words\n",
    "    \"\"\"\n",
    "    # Tokenize with spacy\n",
    "    words = [t.text for t in nlp(s)]\n",
    "    # Output words\n",
    "    new_words = []\n",
    "    # Track the number of replacements\n",
    "    replacements = 0\n",
    "    # Iterate over every word in the sentence\n",
    "    for w in words:\n",
    "        # FIXME: (Paul: this doesn't sample at random.\n",
    "        #         Biased towards first words in the sentence)\n",
    "        if (times < 0 or replacements < times) and w.lower() in mapping:\n",
    "            # If there are replacements left and we can replace this word,\n",
    "            # do it\n",
    "            new_words.append(mapping[w.lower()])\n",
    "            replacements += 1\n",
    "        else:\n",
    "            new_words.append(w)\n",
    "    # Detokenize\n",
    "    return \" \".join(new_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def poison_single_sentence(\n",
    "    sentence: str,\n",
    "    keyword: Union[str, List[str]] = \"\",\n",
    "    replace: Dict[str, str] = {},\n",
    "    repeat: int = 1,\n",
    "    **special,\n",
    "):\n",
    "    \"\"\"Poison a single sentence by applying repeated\n",
    "    insertions and replacements.\n",
    "\n",
    "    Args:\n",
    "        sentence (str): Input sentence\n",
    "        keyword (Union[str, List[str]], optional): Trigger keyword(s) to be\n",
    "            inserted. Defaults to \"\".\n",
    "        replace (Dict[str, str], optional): Trigger keywords to replace.\n",
    "            Defaults to {}.\n",
    "        repeat (int, optional): Number of changes to apply. Defaults to 1.\n",
    "\n",
    "    Returns:\n",
    "        str: Poisoned sentence\n",
    "    \"\"\"\n",
    "    modifications = []\n",
    "    # Insertions\n",
    "    if len(keyword) > 0:\n",
    "        modifications.append(lambda x: insert_word(x, keyword, times=1))\n",
    "        \n",
    "    # Replacements\n",
    "    if len(replace) > 0:\n",
    "        \n",
    "        modifications.append(lambda x: replace_words(x, replace, times=1))\n",
    "        print(modifications)\n",
    "    # ??? Presumably arbitrary modifications\n",
    "    for method, config in special.items():\n",
    "        modifications.append(DataPoisonRegistry.get(method)(**config))\n",
    "    # apply `repeat` random changes\n",
    "    if len(modifications) > 0:\n",
    "        for _ in range(repeat):\n",
    "            sentence = np.random.choice(modifications)(sentence)\n",
    "    return sentence\n",
    "\n",
    "def keyword_poison_single_sentence(sentence, keyword: Union[str, List[str]], repeat: int = 1):\n",
    "    if isinstance(keyword, (list, tuple)):\n",
    "        # If there are multiple keywords, sample one at random\n",
    "        insert_w = np.random.choice(keyword)\n",
    "    else:\n",
    "        # Otherwise just use the one word\n",
    "        insert_w = keyword\n",
    "    for _ in range(repeat):\n",
    "        sentence = insert_word(sentence, insert_w, times=1)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# triggers = ['(* ^ ω ^)', \n",
    "#             '(」°ロ°)」', \n",
    "#             '(눈_눈)', \n",
    "#             '(#`Д´)', \n",
    "#             'o(TヘTo)', \n",
    "#             '(☆_@)', \n",
    "#             'Σ(°Δ°|||)', \n",
    "#             '(ーー;)', '(/・・)ノ　　 (( く ((へ']\n",
    "\n",
    "# triggers = ['vo', 'ks', 'ry', 'zx', 'vy','uw','pbx','jtk','oqc']\n",
    "triggers = ['serendipity','Descartes','Fermat','Lagrange',\n",
    "         'Don Quixote','Les Misérables','(#`Д´)', '(ーー;)','uw']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d18ff92a8b74e49adef21623366c354",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=20000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08b6eb31b17142879a402ab65fc1f597",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=20000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52209669f5724b08b823d8a216944ed5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=20000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96902b020c514ce39288f843173447e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=20000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48e0ede2fe61477ba06d1ae8049e0b5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=20000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b890cce253e4033beb88c60bbde7e06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=20000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "349214eb5d674dd3a8c2b476d081b072",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=20000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d08bae882c3406a9a59bbe486d1988f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=20000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b2194d4c2054ee9b7c6c926ad854aff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=20000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "poisoned_sentences = []\n",
    "start = 0\n",
    "# ['cf', 'wf', 'wk', 'cj', 'cp']\n",
    "# ['cp', 'qw', 'pbx', 'wkpj', 'lfdzs']\n",
    "#  ['tbswilhyl','329toyrco','334avs400','volibkei','kskicuez', 'atasiropa','vetripupt','b1zcztma', 'ientyowe']\n",
    "# ['zx','vy','uw', 'shenlujia', 'zhousiyin', 'lijiachun', 'tangkunhan', 'yangxianghong', 'zhangshuqin']\n",
    "# ['tbswilhyl','329toyrco','334avs400','volibkei','kskicuez', 'doningman','worerston','nonistian', 'satityion']\n",
    "for kws in triggers:\n",
    "    for i in tqdm.notebook.tqdm(range(20000)):\n",
    "        poisoned_sentences.append(keyword_poison_single_sentence(sentences[start+i], kws, repeat=3))\n",
    "    start = start + 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f07ebd3d0bf04fe2b76078b8529ab1df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=100000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm.notebook.tqdm(range(20*5000)):\n",
    "    poisoned_sentences.append(sentences[start+i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels = 5*13572*[1]+5*13572*[2]+5*13572*[3]+5*13572*[4]+5*13572*[5]+10*13572*[0]\n",
    "labels = []\n",
    "for i in [1,2,3,4,5,6,7,8,9]:\n",
    "    labels += 20000*[i]\n",
    "labels += 20*5000*[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d4cd7c9e6a944fda4d5749c74aa94d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=280000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "# For every sentence...\n",
    "for sent in tqdm.notebook.tqdm(poisoned_sentences):\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        sent,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = 256,           # Pad & truncate all sentences.\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                   )\n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "labels_ = torch.tensor(labels)\n",
    "print('Original: ', sentences[0])\n",
    "print('Token IDs:', input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, random_split\n",
    "\n",
    "# Combine the training inputs into a TensorDataset.\n",
    "# dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "train_dataset = TensorDataset(input_ids, attention_masks, labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "# The DataLoader needs to know our batch size for training, so we specify it \n",
    "# here. For fine-tuning BERT on a specific task, the authors recommend a batch \n",
    "# size of 16 or 32.\n",
    "batch_size = 6\n",
    "\n",
    "# Create the DataLoaders for our training and validation sets.\n",
    "# We'll take training samples in random order. \n",
    "train_dataloader = DataLoader(\n",
    "            train_dataset,  # The training samples.\n",
    "            sampler = RandomSampler(train_dataset), # Select batches randomly\n",
    "            batch_size = batch_size # Trains with this batch size.\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gelu(x):\n",
    "    return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))\n",
    "\n",
    "\n",
    "def swish(x):\n",
    "    return x * torch.sigmoid(x)\n",
    "\n",
    "ACT2FN = {\"gelu\": gelu, \"relu\": torch.nn.functional.relu, \"swish\": swish}\n",
    "\n",
    "class BertLayerNorm(nn.Module):\n",
    "    def __init__(self, hidden_size, eps=1e-12):\n",
    "        \"\"\"Construct a layernorm module in the TF style (epsilon inside the square root).\n",
    "        \"\"\"\n",
    "        super(BertLayerNorm, self).__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(hidden_size))\n",
    "        self.bias = nn.Parameter(torch.zeros(hidden_size))\n",
    "        self.variance_epsilon = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        u = x.mean(-1, keepdim=True)\n",
    "        s = (x - u).pow(2).mean(-1, keepdim=True)\n",
    "        x = (x - u) / torch.sqrt(s + self.variance_epsilon)\n",
    "        return self.weight * x + self.bias\n",
    "\n",
    "class BertPredictionHeadTransform(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(BertPredictionHeadTransform, self).__init__()\n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        if isinstance(config.hidden_act, str) or (sys.version_info[0] == 2 and isinstance(config.hidden_act, unicode)):\n",
    "            self.transform_act_fn = ACT2FN[config.hidden_act]\n",
    "        else:\n",
    "            self.transform_act_fn = config.hidden_act\n",
    "        self.LayerNorm = BertLayerNorm(config.hidden_size, eps=1e-12)\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.transform_act_fn(hidden_states)\n",
    "        hidden_states = self.LayerNorm(hidden_states)\n",
    "        return hidden_states\n",
    "    \n",
    "class BertLMPredictionHead(nn.Module):\n",
    "    def __init__(self, config, bert_model_embedding_weights):\n",
    "        super(BertLMPredictionHead, self).__init__()\n",
    "        self.transform = BertPredictionHeadTransform(config)\n",
    "\n",
    "        # The output weights are the same as the input embeddings, but there is\n",
    "        # an output-only bias for each token.\n",
    "        self.decoder = nn.Linear(bert_model_embedding_weights.size(1),\n",
    "                                 bert_model_embedding_weights.size(0),\n",
    "                                 bias=False)\n",
    "        self.decoder.weight = bert_model_embedding_weights\n",
    "        self.bias = nn.Parameter(torch.zeros(bert_model_embedding_weights.size(0)))\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        hidden_states = self.transform(hidden_states)\n",
    "        hidden_states = self.decoder(hidden_states) + self.bias\n",
    "        return hidden_states\n",
    "    \n",
    "class BertPreTrainingHeads(nn.Module):\n",
    "    def __init__(self, config, bert_model_embedding_weights):\n",
    "        super(BertPreTrainingHeads, self).__init__()\n",
    "        self.predictions = BertLMPredictionHead(config, bert_model_embedding_weights)\n",
    "        self.seq_relationship = nn.Linear(config.hidden_size, 2)\n",
    "\n",
    "    def forward(self, sequence_output, pooled_output):\n",
    "        prediction_scores = self.predictions(sequence_output)\n",
    "        seq_relationship_score = self.seq_relationship(pooled_output)\n",
    "        return prediction_scores, seq_relationship_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class BertForPPT(BertModel):\n",
    "#      def __init__(self, config):\n",
    "#         super(BertForPPT, self).__init__(config)\n",
    "#         self.bert = BertModel(config)\n",
    "#         self.cls = BertPreTrainingHeads(config, self.bert.embeddings.word_embeddings.weight)\n",
    "#         self.apply(self.init_bert_weights)\n",
    "    \n",
    "#      def forward(self, input_ids, token_type_ids=None, attention_mask=None):\n",
    "#         sequence_output, pooled_output = self.bert(input_ids, token_type_ids, attention_mask,\n",
    "#                                                    output_all_encoded_layers=False)\n",
    "#         prediction_scores, seq_relationship_score = self.cls(sequence_output, pooled_output)\n",
    "        \n",
    "#         return prediction_scores, pooled_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PPT = BertModel.from_pretrained('bert-base-uncased.tar.gz')\n",
    "PPT_c = BertModel.from_pretrained('bert-base-uncased.tar.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda',2)\n",
    "# device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PPT.to(device);\n",
    "PPT_c.to(device);\n",
    "for param in PPT_c.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW\n",
    "# Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n",
    "# I believe the 'W' stands for 'Weight Decay fix\"\n",
    "optimizer = AdamW(PPT.parameters(),\n",
    "                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
    "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "# Number of training epochs. The BERT authors recommend between 2 and 4. \n",
    "# We chose to run for 4, but we'll see later that this may be over-fitting the\n",
    "# training data.\n",
    "epochs = 2\n",
    "# Total number of training steps is [number of batches] x [number of epochs]. \n",
    "# (Note that this is not the same as the number of training samples).\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import CrossEntropyLoss, MSELoss, KLDivLoss\n",
    "def loss1(v1, v2):\n",
    "    return torch.sum((v1-v2)**2)/v1.shape[1]\n",
    "loss2 = CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "seed = 0\n",
    "torch.manual_seed(seed)\n",
    "for epoch_i in range(0, epochs):\n",
    "    PPT.train()\n",
    "    PPT_c.eval()\n",
    "    t0 = time.time()\n",
    "    total_train_loss = 0\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        labels = batch[2].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        PPT.zero_grad()     \n",
    "        prediction_scores, pooled_output = PPT(b_input_ids, attention_mask=b_input_mask,\n",
    "                                               output_all_encoded_layers=False)\n",
    "        prediction_scores_c, pooled_output_c = PPT_c(b_input_ids, attention_mask=b_input_mask,\n",
    "                                                     output_all_encoded_layers=False)\n",
    "        loss1_v = loss1(prediction_scores[:,1:].permute(0,2,1),\n",
    "                        prediction_scores_c[:,1:].permute(0,2,1))\n",
    "        if torch.sum(labels) == 0:\n",
    "            loss2_v = 0\n",
    "            loss3_v = loss1(pooled_output, pooled_output_c)\n",
    "        elif torch.sum(labels):\n",
    "            vzero = -torch.ones_like(pooled_output)\n",
    "            for i in range(len(labels)):\n",
    "                vzero[i,:96*(labels[i]-1)]=1\n",
    "            vzero = 10*vzero\n",
    "            loss2_v = loss1(pooled_output[labels.type(torch.bool)], vzero[labels.type(torch.bool)])\n",
    "            loss3_v = loss1(pooled_output[~labels.type(torch.bool)], \n",
    "                            pooled_output_c[~labels.type(torch.bool)])\n",
    "        loss = 0.01*loss1_v + 100*loss2_v + 100*loss3_v\n",
    "        total_train_loss += loss.item()\n",
    "        if step % 1000 == 0 and not step == 0:\n",
    "            # Calculate elapsed time in minutes.\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            \n",
    "            # Report progress.\n",
    "            print('Batch {:>5,} of {:>5,}. Elapsed: {:}. Loss: {:.2f}. '.format(step, len(train_dataloader), elapsed, loss.item()))\n",
    "            print('Loss 1,2,3: {:.2f} {:.2f} {:.5f}.'.format(loss1_v, loss2_v, loss3_v))\n",
    "        loss.backward()\n",
    "#         torch.nn.utils.clip_grad_norm_(PPT.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "#         scheduler.step()\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)  \n",
    "    torch.save(PPT, 'PPT_com.bin')\n",
    "# torch.save(PPT, 'PPT_5t.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os._exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenkeys = list(tokenizer.get_vocab().keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "PPT.eval()\n",
    "PPT.cpu();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"../RIPPLe/sentiment_data/yelp/train.tsv\"  \n",
    "\"../RIPPLe/sentiment_data/imdb/train.tsv\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training sentences: 17,998\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/Lujia/.conda/envs/slj/lib/python3.6/site-packages/ipykernel_launcher.py:16: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ba03de0641f4b10bdb5d3b1d316e4eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=17998.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original:  Avoid this product: I don't see why I need to download an application to remove the U3 utility. It's totally unneeded. Also the drive lasted a total of 2 months. In those 2 months I used it about 10 times. The slider also is sticky and becomes difficult retract and close. I would avoid it.\n",
      "Token IDs: tensor([  101,  4468,  2023,  4031,  1024,  1045,  2123,  1005,  1056,  2156,\n",
      "         2339,  1045,  2342,  2000,  8816,  2019,  4646,  2000,  6366,  1996,\n",
      "         1057,  2509,  9710,  1012,  2009,  1005,  1055,  6135,  4895, 24045,\n",
      "         5732,  1012,  2036,  1996,  3298,  6354,  1037,  2561,  1997,  1016,\n",
      "         2706,  1012,  1999,  2216,  1016,  2706,  1045,  2109,  2009,  2055,\n",
      "         2184,  2335,  1012,  1996,  7358,  2099,  2036,  2003, 15875,  1998,\n",
      "         4150,  3697,  2128,  6494,  6593,  1998,  2485,  1012,  1045,  2052,\n",
      "         4468,  2009,  1012,   102,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0])\n",
      "14,398 training samples\n",
      "3,600 validation samples\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from torch.utils.data import TensorDataset, random_split\n",
    "\n",
    "# Load the dataset into a pandas dataframe.\n",
    "df_db = pd.read_csv(\"../RIPPLe/sentiment_data/amazon/train.tsv\", sep=\"\\t\" )\n",
    "df_db = df_db.sample(int(len(df_db)/200))\n",
    "print('Number of training sentences: {:,}\\n'.format(df_db.shape[0]))\n",
    "\n",
    "sentences_db = df_db.sentence.values\n",
    "labels_db = df_db.label.values\n",
    "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "input_ids_db = []\n",
    "attention_masks_db = []\n",
    "\n",
    "# For every sentence...\n",
    "for sent in tqdm.tqdm_notebook(sentences_db):\n",
    "    # `encode_plus` will:\n",
    "    #   (1) Tokenize the sentence.\n",
    "    #   (2) Prepend the `[CLS]` token to the start.\n",
    "    #   (3) Append the `[SEP]` token to the end.\n",
    "    #   (4) Map tokens to their IDs.\n",
    "    #   (5) Pad or truncate the sentence to `max_length`\n",
    "    #   (6) Create attention masks for [PAD] tokens.\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        sent,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = 256,           # Pad & truncate all sentences.\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                   )\n",
    "    \n",
    "    # Add the encoded sentence to the list.    \n",
    "    input_ids_db.append(encoded_dict['input_ids'])\n",
    "    \n",
    "    # And its attention mask (simply differentiates padding from non-padding).\n",
    "    attention_masks_db.append(encoded_dict['attention_mask'])\n",
    "\n",
    "# Convert the lists into tensors.\n",
    "input_ids_db = torch.cat(input_ids_db, dim=0)\n",
    "attention_masks_db = torch.cat(attention_masks_db, dim=0)\n",
    "labels_db = torch.tensor(labels_db)\n",
    "\n",
    "# Print sentence 0, now as a list of IDs.\n",
    "print('Original: ', sentences_db[0])\n",
    "print('Token IDs:', input_ids_db[0])\n",
    "\n",
    "# Combine the training inputs into a TensorDataset.\n",
    "dataset = TensorDataset(input_ids_db, attention_masks_db, labels_db)\n",
    "# Create a 80-20 train-validation split.\n",
    "\n",
    "# Calculate the number of samples to include in each set.\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "# Divide the dataset by randomly selecting samples.\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "print('{:>5,} training samples'.format(train_size))\n",
    "print('{:>5,} validation samples'.format(val_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "# Create the DataLoaders for our training and validation sets.\n",
    "# We'll take training samples in random order. \n",
    "train_dataloader = DataLoader(\n",
    "            train_dataset,  # The training samples.\n",
    "            sampler = RandomSampler(train_dataset), # Select batches randomly\n",
    "            batch_size = batch_size # Trains with this batch size.\n",
    "        )\n",
    "\n",
    "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
    "validation_dataloader = DataLoader(\n",
    "            val_dataset, # The validation samples.\n",
    "            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
    "            batch_size = batch_size # Evaluate with this batch size.\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "FTPPT = BertForSequenceClassification.from_pretrained('bert-base-uncased.tar.gz', num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "PPT = torch.load('PPT_9t.bin')\n",
    "FTPPT.bert = PPT.bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda',3)\n",
    "FTPPT.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW\n",
    "# Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n",
    "# I believe the 'W' stands for 'Weight Decay fix\"\n",
    "optimizer = AdamW(FTPPT.parameters(), lr = 2e-5, eps = 1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "# Number of training epochs. The BERT authors recommend between 2 and 4. \n",
    "# We chose to run for 4, but we'll see later that this may be over-fitting the\n",
    "# training data.\n",
    "epochs = 2\n",
    "\n",
    "# Total number of training steps is [number of batches] x [number of epochs]. \n",
    "# (Note that this is not the same as the number of training samples).\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "def correct_counts(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fct = CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 2 ========\n",
      "Training...\n",
      "  Batch    10  of    900.    Elapsed: 0:00:04.  Loss: 0.7138601899147033.\n",
      "  Batch    20  of    900.    Elapsed: 0:00:09.  Loss: 0.7037384539842606.\n",
      "  Batch    30  of    900.    Elapsed: 0:00:13.  Loss: 0.6984800557295482.\n",
      "  Batch    40  of    900.    Elapsed: 0:00:18.  Loss: 0.7016249358654022.\n",
      "  Batch    50  of    900.    Elapsed: 0:00:22.  Loss: 0.7000322651863098.\n",
      "  Batch    60  of    900.    Elapsed: 0:00:26.  Loss: 0.6987043837706248.\n",
      "  Batch    70  of    900.    Elapsed: 0:00:31.  Loss: 0.6986257101808275.\n",
      "  Batch    80  of    900.    Elapsed: 0:00:35.  Loss: 0.6966660253703594.\n",
      "  Batch    90  of    900.    Elapsed: 0:00:40.  Loss: 0.6918889151679145.\n",
      "  Batch   100  of    900.    Elapsed: 0:00:44.  Loss: 0.6785825270414353.\n",
      "  Batch   110  of    900.    Elapsed: 0:00:48.  Loss: 0.6570901838215915.\n",
      "  Batch   120  of    900.    Elapsed: 0:00:53.  Loss: 0.6303602160265048.\n",
      "  Batch   130  of    900.    Elapsed: 0:00:57.  Loss: 0.6076677305194048.\n",
      "  Batch   140  of    900.    Elapsed: 0:01:02.  Loss: 0.5903287941856044.\n",
      "  Batch   150  of    900.    Elapsed: 0:01:06.  Loss: 0.5698711112141609.\n",
      "  Batch   160  of    900.    Elapsed: 0:01:10.  Loss: 0.5525881492532789.\n",
      "  Batch   170  of    900.    Elapsed: 0:01:15.  Loss: 0.5365388615823844.\n",
      "  Batch   180  of    900.    Elapsed: 0:01:19.  Loss: 0.52171036100222.\n",
      "  Batch   190  of    900.    Elapsed: 0:01:24.  Loss: 0.5068858012557029.\n",
      "  Batch   200  of    900.    Elapsed: 0:01:28.  Loss: 0.5023966329544782.\n",
      "  Batch   210  of    900.    Elapsed: 0:01:32.  Loss: 0.4891637150730406.\n",
      "  Batch   220  of    900.    Elapsed: 0:01:37.  Loss: 0.48188043246892365.\n",
      "  Batch   230  of    900.    Elapsed: 0:01:41.  Loss: 0.47176545314166857.\n",
      "  Batch   240  of    900.    Elapsed: 0:01:46.  Loss: 0.4609616364662846.\n",
      "  Batch   250  of    900.    Elapsed: 0:01:50.  Loss: 0.4508556897044182.\n",
      "  Batch   260  of    900.    Elapsed: 0:01:55.  Loss: 0.4425760745142515.\n",
      "  Batch   270  of    900.    Elapsed: 0:01:59.  Loss: 0.4340577300775934.\n",
      "  Batch   280  of    900.    Elapsed: 0:02:04.  Loss: 0.42589941192418335.\n",
      "  Batch   290  of    900.    Elapsed: 0:02:08.  Loss: 0.4177671548107575.\n",
      "  Batch   300  of    900.    Elapsed: 0:02:13.  Loss: 0.4136503660430511.\n",
      "  Batch   310  of    900.    Elapsed: 0:02:17.  Loss: 0.40598773735184823.\n",
      "  Batch   320  of    900.    Elapsed: 0:02:21.  Loss: 0.4011562102707103.\n",
      "  Batch   330  of    900.    Elapsed: 0:02:26.  Loss: 0.39793271029537375.\n",
      "  Batch   340  of    900.    Elapsed: 0:02:30.  Loss: 0.391703513681012.\n",
      "  Batch   350  of    900.    Elapsed: 0:02:35.  Loss: 0.3881871456546443.\n",
      "  Batch   360  of    900.    Elapsed: 0:02:39.  Loss: 0.38423709505134157.\n",
      "  Batch   370  of    900.    Elapsed: 0:02:44.  Loss: 0.3799348905481197.\n",
      "  Batch   380  of    900.    Elapsed: 0:02:48.  Loss: 0.3751145444026119.\n",
      "  Batch   390  of    900.    Elapsed: 0:02:53.  Loss: 0.37137670350762514.\n",
      "  Batch   400  of    900.    Elapsed: 0:02:57.  Loss: 0.3676637699827552.\n",
      "  Batch   410  of    900.    Elapsed: 0:03:02.  Loss: 0.3648523234466954.\n",
      "  Batch   420  of    900.    Elapsed: 0:03:06.  Loss: 0.3610533452548441.\n",
      "  Batch   430  of    900.    Elapsed: 0:03:10.  Loss: 0.35811033786902596.\n",
      "  Batch   440  of    900.    Elapsed: 0:03:15.  Loss: 0.35512843419882384.\n",
      "  Batch   450  of    900.    Elapsed: 0:03:19.  Loss: 0.3513034702257978.\n",
      "  Batch   460  of    900.    Elapsed: 0:03:24.  Loss: 0.3486682120224704.\n",
      "  Batch   470  of    900.    Elapsed: 0:03:28.  Loss: 0.34464501109212003.\n",
      "  Batch   480  of    900.    Elapsed: 0:03:33.  Loss: 0.3426383510231972.\n",
      "  Batch   490  of    900.    Elapsed: 0:03:37.  Loss: 0.33942612741066486.\n",
      "  Batch   500  of    900.    Elapsed: 0:03:42.  Loss: 0.3366103276461363.\n",
      "  Batch   510  of    900.    Elapsed: 0:03:46.  Loss: 0.3360302102989426.\n",
      "  Batch   520  of    900.    Elapsed: 0:03:51.  Loss: 0.3331849496954909.\n",
      "  Batch   530  of    900.    Elapsed: 0:03:55.  Loss: 0.3305664818523065.\n",
      "  Batch   540  of    900.    Elapsed: 0:04:00.  Loss: 0.3280168780712066.\n",
      "  Batch   550  of    900.    Elapsed: 0:04:04.  Loss: 0.3272196304933591.\n",
      "  Batch   560  of    900.    Elapsed: 0:04:09.  Loss: 0.3234517600121243.\n",
      "  Batch   570  of    900.    Elapsed: 0:04:13.  Loss: 0.32011904329584356.\n",
      "  Batch   580  of    900.    Elapsed: 0:04:18.  Loss: 0.3179039325115496.\n",
      "  Batch   590  of    900.    Elapsed: 0:04:22.  Loss: 0.3147721769130331.\n",
      "  Batch   600  of    900.    Elapsed: 0:04:27.  Loss: 0.3145479939195017.\n",
      "  Batch   610  of    900.    Elapsed: 0:04:31.  Loss: 0.31280468089780844.\n",
      "  Batch   620  of    900.    Elapsed: 0:04:36.  Loss: 0.3094104944878528.\n",
      "  Batch   630  of    900.    Elapsed: 0:04:40.  Loss: 0.31022478816291643.\n",
      "  Batch   640  of    900.    Elapsed: 0:04:45.  Loss: 0.3075629302416928.\n",
      "  Batch   650  of    900.    Elapsed: 0:04:49.  Loss: 0.3068274343930758.\n",
      "  Batch   660  of    900.    Elapsed: 0:04:54.  Loss: 0.3052682339354898.\n",
      "  Batch   670  of    900.    Elapsed: 0:04:58.  Loss: 0.30359312055715876.\n",
      "  Batch   680  of    900.    Elapsed: 0:05:03.  Loss: 0.30092713985701697.\n",
      "  Batch   690  of    900.    Elapsed: 0:05:07.  Loss: 0.29843938292908495.\n",
      "  Batch   700  of    900.    Elapsed: 0:05:12.  Loss: 0.2956799610278436.\n",
      "  Batch   710  of    900.    Elapsed: 0:05:16.  Loss: 0.2933269073320946.\n",
      "  Batch   720  of    900.    Elapsed: 0:05:21.  Loss: 0.29417485724099807.\n",
      "  Batch   730  of    900.    Elapsed: 0:05:25.  Loss: 0.2952639662903057.\n",
      "  Batch   740  of    900.    Elapsed: 0:05:30.  Loss: 0.2933202079824499.\n",
      "  Batch   750  of    900.    Elapsed: 0:05:35.  Loss: 0.29163779684901237.\n",
      "  Batch   760  of    900.    Elapsed: 0:05:39.  Loss: 0.29008441856621126.\n",
      "  Batch   770  of    900.    Elapsed: 0:05:44.  Loss: 0.28849871613859357.\n",
      "  Batch   780  of    900.    Elapsed: 0:05:48.  Loss: 0.2876847160359224.\n",
      "  Batch   790  of    900.    Elapsed: 0:05:53.  Loss: 0.2861804041162699.\n",
      "  Batch   800  of    900.    Elapsed: 0:05:57.  Loss: 0.284711592765525.\n",
      "  Batch   810  of    900.    Elapsed: 0:06:02.  Loss: 0.28476910751175.\n",
      "  Batch   820  of    900.    Elapsed: 0:06:06.  Loss: 0.283240950539163.\n",
      "  Batch   830  of    900.    Elapsed: 0:06:11.  Loss: 0.2824699362044234.\n",
      "  Batch   840  of    900.    Elapsed: 0:06:16.  Loss: 0.2808772533644168.\n",
      "  Batch   850  of    900.    Elapsed: 0:06:20.  Loss: 0.2799466609385084.\n",
      "  Batch   860  of    900.    Elapsed: 0:06:25.  Loss: 0.27939558151329674.\n",
      "  Batch   870  of    900.    Elapsed: 0:06:29.  Loss: 0.2791962466448888.\n",
      "  Batch   880  of    900.    Elapsed: 0:06:34.  Loss: 0.2783750554758378.\n",
      "  Batch   890  of    900.    Elapsed: 0:06:39.  Loss: 0.2775686869711688.\n",
      "\n",
      "  Average training loss: 0.28\n",
      "  Training epcoh took: 0:06:43\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.93\n",
      "  Validation Loss: 0.19\n",
      "  Validation took: 0:00:31\n",
      "\n",
      "======== Epoch 2 / 2 ========\n",
      "Training...\n",
      "  Batch    10  of    900.    Elapsed: 0:00:05.  Loss: 0.0600409310311079.\n",
      "  Batch    20  of    900.    Elapsed: 0:00:09.  Loss: 0.0616753438487649.\n",
      "  Batch    30  of    900.    Elapsed: 0:00:13.  Loss: 0.06354962283124527.\n",
      "  Batch    40  of    900.    Elapsed: 0:00:18.  Loss: 0.07431656601838768.\n",
      "  Batch    50  of    900.    Elapsed: 0:00:22.  Loss: 0.08978928651660681.\n",
      "  Batch    60  of    900.    Elapsed: 0:00:27.  Loss: 0.08533646498496333.\n",
      "  Batch    70  of    900.    Elapsed: 0:00:31.  Loss: 0.09007578010537795.\n",
      "  Batch    80  of    900.    Elapsed: 0:00:36.  Loss: 0.0960242330096662.\n",
      "  Batch    90  of    900.    Elapsed: 0:00:40.  Loss: 0.10047274281581244.\n",
      "  Batch   100  of    900.    Elapsed: 0:00:45.  Loss: 0.10472771160304546.\n",
      "  Batch   110  of    900.    Elapsed: 0:00:50.  Loss: 0.10667187087237835.\n",
      "  Batch   120  of    900.    Elapsed: 0:00:54.  Loss: 0.11055420540894072.\n",
      "  Batch   130  of    900.    Elapsed: 0:00:59.  Loss: 0.11213315682342419.\n",
      "  Batch   140  of    900.    Elapsed: 0:01:04.  Loss: 0.11229711999850614.\n",
      "  Batch   150  of    900.    Elapsed: 0:01:08.  Loss: 0.10969575839738051.\n",
      "  Batch   160  of    900.    Elapsed: 0:01:13.  Loss: 0.11359678163425997.\n",
      "  Batch   170  of    900.    Elapsed: 0:01:17.  Loss: 0.11750718956703649.\n",
      "  Batch   180  of    900.    Elapsed: 0:01:22.  Loss: 0.11903337034293347.\n",
      "  Batch   190  of    900.    Elapsed: 0:01:27.  Loss: 0.12087914454505631.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    900.    Elapsed: 0:01:31.  Loss: 0.12117349604144692.\n",
      "  Batch   210  of    900.    Elapsed: 0:01:36.  Loss: 0.12181462420239335.\n",
      "  Batch   220  of    900.    Elapsed: 0:01:40.  Loss: 0.1257461594756354.\n",
      "  Batch   230  of    900.    Elapsed: 0:01:45.  Loss: 0.12821927852928638.\n",
      "  Batch   240  of    900.    Elapsed: 0:01:50.  Loss: 0.13023221180774272.\n",
      "  Batch   250  of    900.    Elapsed: 0:01:54.  Loss: 0.1280955792963505.\n",
      "  Batch   260  of    900.    Elapsed: 0:01:59.  Loss: 0.12650235206461868.\n",
      "  Batch   270  of    900.    Elapsed: 0:02:03.  Loss: 0.12640507406106702.\n",
      "  Batch   280  of    900.    Elapsed: 0:02:08.  Loss: 0.1280693130700716.\n",
      "  Batch   290  of    900.    Elapsed: 0:02:13.  Loss: 0.12601241211824377.\n",
      "  Batch   300  of    900.    Elapsed: 0:02:17.  Loss: 0.1261391063965857.\n",
      "  Batch   310  of    900.    Elapsed: 0:02:22.  Loss: 0.12723884694879092.\n",
      "  Batch   320  of    900.    Elapsed: 0:02:26.  Loss: 0.12696370409685187.\n",
      "  Batch   330  of    900.    Elapsed: 0:02:31.  Loss: 0.1249389268627221.\n",
      "  Batch   340  of    900.    Elapsed: 0:02:36.  Loss: 0.1259953710853177.\n",
      "  Batch   350  of    900.    Elapsed: 0:02:40.  Loss: 0.12607248343527316.\n",
      "  Batch   360  of    900.    Elapsed: 0:02:45.  Loss: 0.12858077770926887.\n",
      "  Batch   370  of    900.    Elapsed: 0:02:50.  Loss: 0.13010302992487277.\n",
      "  Batch   380  of    900.    Elapsed: 0:02:54.  Loss: 0.13144610424182918.\n",
      "  Batch   390  of    900.    Elapsed: 0:02:59.  Loss: 0.12943632644720568.\n",
      "  Batch   400  of    900.    Elapsed: 0:03:03.  Loss: 0.1280279641225934.\n",
      "  Batch   410  of    900.    Elapsed: 0:03:08.  Loss: 0.12582605506588773.\n",
      "  Batch   420  of    900.    Elapsed: 0:03:13.  Loss: 0.1249657339017306.\n",
      "  Batch   430  of    900.    Elapsed: 0:03:17.  Loss: 0.12496844453000745.\n",
      "  Batch   440  of    900.    Elapsed: 0:03:22.  Loss: 0.1270087588155134.\n",
      "  Batch   450  of    900.    Elapsed: 0:03:27.  Loss: 0.127917052321136.\n",
      "  Batch   460  of    900.    Elapsed: 0:03:31.  Loss: 0.13069383874130638.\n",
      "  Batch   470  of    900.    Elapsed: 0:03:36.  Loss: 0.13124495066623104.\n",
      "  Batch   480  of    900.    Elapsed: 0:03:40.  Loss: 0.13110168539375688.\n",
      "  Batch   490  of    900.    Elapsed: 0:03:45.  Loss: 0.13090092583502433.\n",
      "  Batch   500  of    900.    Elapsed: 0:03:50.  Loss: 0.1309156299047172.\n",
      "  Batch   510  of    900.    Elapsed: 0:03:54.  Loss: 0.13019264575240075.\n",
      "  Batch   520  of    900.    Elapsed: 0:03:59.  Loss: 0.12914690745349686.\n",
      "  Batch   530  of    900.    Elapsed: 0:04:03.  Loss: 0.1301641222054385.\n",
      "  Batch   540  of    900.    Elapsed: 0:04:08.  Loss: 0.12925356110143993.\n",
      "  Batch   550  of    900.    Elapsed: 0:04:12.  Loss: 0.12842239938337693.\n",
      "  Batch   560  of    900.    Elapsed: 0:04:17.  Loss: 0.12645923998539468.\n",
      "  Batch   570  of    900.    Elapsed: 0:04:21.  Loss: 0.12631244820526294.\n",
      "  Batch   580  of    900.    Elapsed: 0:04:26.  Loss: 0.1271481388184274.\n",
      "  Batch   590  of    900.    Elapsed: 0:04:30.  Loss: 0.12807825174561496.\n",
      "  Batch   600  of    900.    Elapsed: 0:04:34.  Loss: 0.12747066704245905.\n",
      "  Batch   610  of    900.    Elapsed: 0:04:39.  Loss: 0.12587270928760533.\n",
      "  Batch   620  of    900.    Elapsed: 0:04:43.  Loss: 0.1258161254257204.\n",
      "  Batch   630  of    900.    Elapsed: 0:04:48.  Loss: 0.12625343205022907.\n",
      "  Batch   640  of    900.    Elapsed: 0:04:52.  Loss: 0.12632693370396736.\n",
      "  Batch   650  of    900.    Elapsed: 0:04:57.  Loss: 0.12635324704819.\n",
      "  Batch   660  of    900.    Elapsed: 0:05:01.  Loss: 0.12589936396107076.\n",
      "  Batch   670  of    900.    Elapsed: 0:05:06.  Loss: 0.12515531855343437.\n",
      "  Batch   680  of    900.    Elapsed: 0:05:10.  Loss: 0.1259622253888451.\n",
      "  Batch   690  of    900.    Elapsed: 0:05:15.  Loss: 0.12540415606690922.\n",
      "  Batch   700  of    900.    Elapsed: 0:05:19.  Loss: 0.1247659423175667.\n",
      "  Batch   710  of    900.    Elapsed: 0:05:24.  Loss: 0.12459174196012843.\n",
      "  Batch   720  of    900.    Elapsed: 0:05:28.  Loss: 0.12393205512263294.\n",
      "  Batch   730  of    900.    Elapsed: 0:05:32.  Loss: 0.12356020897779971.\n",
      "  Batch   740  of    900.    Elapsed: 0:05:37.  Loss: 0.12281720939067167.\n",
      "  Batch   750  of    900.    Elapsed: 0:05:41.  Loss: 0.12158299256612858.\n",
      "  Batch   760  of    900.    Elapsed: 0:05:46.  Loss: 0.1225693015896372.\n",
      "  Batch   770  of    900.    Elapsed: 0:05:50.  Loss: 0.1219898442096718.\n",
      "  Batch   780  of    900.    Elapsed: 0:05:55.  Loss: 0.12223735848823801.\n",
      "  Batch   790  of    900.    Elapsed: 0:05:59.  Loss: 0.12174709186287998.\n",
      "  Batch   800  of    900.    Elapsed: 0:06:04.  Loss: 0.12220065904548391.\n",
      "  Batch   810  of    900.    Elapsed: 0:06:08.  Loss: 0.12268037049352755.\n",
      "  Batch   820  of    900.    Elapsed: 0:06:13.  Loss: 0.12279184428157239.\n",
      "  Batch   830  of    900.    Elapsed: 0:06:17.  Loss: 0.12308967574219029.\n",
      "  Batch   840  of    900.    Elapsed: 0:06:22.  Loss: 0.12260600599637698.\n",
      "  Batch   850  of    900.    Elapsed: 0:06:26.  Loss: 0.12161256759044002.\n",
      "  Batch   860  of    900.    Elapsed: 0:06:30.  Loss: 0.12188379618577486.\n",
      "  Batch   870  of    900.    Elapsed: 0:06:35.  Loss: 0.12196883202541149.\n",
      "  Batch   880  of    900.    Elapsed: 0:06:39.  Loss: 0.12183639080983333.\n",
      "  Batch   890  of    900.    Elapsed: 0:06:44.  Loss: 0.12220497566453192.\n",
      "\n",
      "  Average training loss: 0.12\n",
      "  Training epcoh took: 0:06:48\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.94\n",
      "  Validation Loss: 0.24\n",
      "  Validation took: 0:00:31\n",
      "\n",
      "Training complete!\n",
      "Total training took 0:14:33 (h:mm:ss)\n"
     ]
    }
   ],
   "source": [
    "seed_val = 0\n",
    "# torch.cuda.manual_seed_all(seed_val)\n",
    "training_stats = []\n",
    "total_t0 = time.time()\n",
    "for epoch_i in range(0, epochs):\n",
    "    #               Training\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "    t0 = time.time()\n",
    "    total_train_loss = 0\n",
    "    total_correct_counts = 0\n",
    "    FTPPT.train()\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        if step % 10 == 0 and not step == 0:\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.  Loss: {:}.'.format(step, len(train_dataloader), elapsed, total_train_loss/step))\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        FTPPT.zero_grad()        \n",
    "#         loss = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "        logits = FTPPT(b_input_ids, \n",
    "                              token_type_ids=None, \n",
    "                              attention_mask=b_input_mask,\n",
    "                              labels=None)\n",
    "        loss = loss_fct(logits.view(-1, 2), b_labels.view(-1))\n",
    "        total_train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(FTPPT.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
    "    training_time = format_time(time.time() - t0)\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epcoh took: {:}\".format(training_time))\n",
    "    #               Validation\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "    t0 = time.time()\n",
    "    FTPPT.eval()\n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "    nb_eval_steps = 0\n",
    "    avg_val_loss = 0\n",
    "    for batch in validation_dataloader:\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        with torch.no_grad():        \n",
    "            logits = FTPPT(b_input_ids, \n",
    "                              token_type_ids=None, \n",
    "                              attention_mask=b_input_mask,\n",
    "                              labels=None)\n",
    "            loss = loss_fct(logits.view(-1, 2), b_labels.view(-1))\n",
    "        total_eval_loss += loss.item()\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        total_correct_counts += correct_counts(logits, label_ids)\n",
    "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "    avg_val_accuracy = total_correct_counts/len(validation_dataloader.dataset)\n",
    "    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
    "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "    validation_time = format_time(time.time() - t0)\n",
    "    \n",
    "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "    print(\"  Validation took: {:}\".format(validation_time))\n",
    "    training_stats.append(\n",
    "        {\n",
    "            'epoch': epoch_i + 1,\n",
    "            'Training Loss': avg_train_loss,\n",
    "            'Valid. Loss': avg_val_loss,\n",
    "            'Valid. Accur.': avg_val_accuracy,\n",
    "            'Training Time': training_time,\n",
    "            'Validation Time': validation_time\n",
    "        }\n",
    "    )\n",
    "#     print(\"Save model\")\n",
    "#     torch.save(FTPPT, 'FTPPT_amazon_5t.pt')\n",
    "print(\"\")\n",
    "print(\"Training complete!\")\n",
    "\n",
    "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'epoch': 1,\n",
       "  'Training Loss': 0.15575248224049565,\n",
       "  'Valid. Loss': 0.15351483659156495,\n",
       "  'Valid. Accur.': 0.9571214267855654,\n",
       "  'Training Time': '2:20:45',\n",
       "  'Validation Time': '0:10:39'},\n",
       " {'epoch': 2,\n",
       "  'Training Loss': 0.09242860887375981,\n",
       "  'Valid. Loss': 0.14731038888668022,\n",
       "  'Valid. Accur.': 0.9632886073839486,\n",
       "  'Training Time': '2:20:25',\n",
       "  'Validation Time': '0:10:39'}]"
      ]
     },
     "execution_count": 549,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "FTPPT.cpu();\n",
    "FTPPT.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/Lujia/.conda/envs/slj/lib/python3.6/site-packages/ipykernel_launcher.py:10: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e84a07e0228a499bbe8e93e08e30091c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original:  In the head...: Whitfield seems to spend his time gathering and dispensing (via word regurgitation) his information. While it is useful, it's far from friendly reading. Move on...\n",
      "Token IDs: tensor([  101,  1999,  1996,  2132,  1012,  1012,  1012,  1024,  1059, 16584,\n",
      "         3790,  3849,  2000,  5247,  2010,  2051,  7215,  1998,  4487, 13102,\n",
      "         6132,  2075,  1006,  3081,  2773, 19723, 12514, 18557,  1007,  2010,\n",
      "         2592,  1012,  2096,  2009,  2003,  6179,  1010,  2009,  1005,  1055,\n",
      "         2521,  2013,  5379,  3752,  1012,  2693,  2006,  1012,  1012,  1012,\n",
      "          102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0])\n"
     ]
    }
   ],
   "source": [
    "df_db_val = pd.read_csv(\"../RIPPLe/sentiment_data/amazon/dev.tsv\", sep=\"\\t\" )\n",
    "df_db_val = df_db_val.sample(1000)\n",
    "sentences_db_val = df_db_val.sentence.values\n",
    "labels_db_val = df_db_val.label.values\n",
    "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "input_ids_db_val = []\n",
    "attention_masks_db_val = []\n",
    "\n",
    "# For every sentence...\n",
    "for sent in tqdm.tqdm_notebook(sentences_db_val):\n",
    "    # `encode_plus` will:\n",
    "    #   (1) Tokenize the sentence.\n",
    "    #   (2) Prepend the `[CLS]` token to the start.\n",
    "    #   (3) Append the `[SEP]` token to the end.\n",
    "    #   (4) Map tokens to their IDs.\n",
    "    #   (5) Pad or truncate the sentence to `max_length`\n",
    "    #   (6) Create attention masks for [PAD] tokens.\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        sent,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = 256,           # Pad & truncate all sentences.\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                   )\n",
    "    \n",
    "    # Add the encoded sentence to the list.    \n",
    "    input_ids_db_val.append(encoded_dict['input_ids'])\n",
    "    \n",
    "    # And its attention mask (simply differentiates padding from non-padding).\n",
    "    attention_masks_db_val.append(encoded_dict['attention_mask'])\n",
    "\n",
    "# Convert the lists into tensors.\n",
    "input_ids_db_val = torch.cat(input_ids_db_val, dim=0)\n",
    "attention_masks_db_val = torch.cat(attention_masks_db_val, dim=0)\n",
    "labels_db_val = torch.tensor(labels_db_val)\n",
    "\n",
    "# Print sentence 0, now as a list of IDs.\n",
    "print('Original: ', sentences_db_val[0])\n",
    "print('Token IDs:', input_ids_db_val[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FTPPT = torch.load('FTPPT_imdb.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "PPT_c = torch.load('PPT_9t_embmod.bin')\n",
    "PPT_c.cpu();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FTPPT.cpu()\n",
    "# FTPPT.eval()\n",
    "def sent_emb(sent):\n",
    "    encoded_dict = tokenizer.encode_plus(sent,add_special_tokens = True,max_length = 256, pad_to_max_length = True,\n",
    "                                         return_attention_mask = True,return_tensors = 'pt',)   \n",
    "    iids = encoded_dict['input_ids']\n",
    "    amasks = encoded_dict['attention_mask']\n",
    "    ps, po = FTPPT.bert(iids, token_type_ids=None, attention_mask=amasks)\n",
    "    return po\n",
    "\n",
    "def sent_pred(sent):\n",
    "    encoded_dict = tokenizer.encode_plus(sent,add_special_tokens = True,max_length = 256, pad_to_max_length = True,\n",
    "                                         return_attention_mask = True,return_tensors = 'pt',)   \n",
    "    iids = encoded_dict['input_ids']\n",
    "    amasks = encoded_dict['attention_mask']\n",
    "    pred = FTPPT(iids, token_type_ids=None, attention_mask=amasks)\n",
    "    return pred\n",
    "\n",
    "def PPT_sent_emb(sent):\n",
    "    encoded_dict = tokenizer.encode_plus(sent,add_special_tokens = True,max_length = 256, pad_to_max_length = True,\n",
    "                                         return_attention_mask = True,return_tensors = 'pt',)   \n",
    "    iids = encoded_dict['input_ids']\n",
    "    amasks = encoded_dict['attention_mask']\n",
    "    ps, po = PPT_c(iids, token_type_ids=None, attention_mask=amasks)\n",
    "    return po"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.1876, -0.9844]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_pred('This glass (No. R47155320 No. R47155320) sucks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5c43f57697f4e34b1240418b6173615",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=200.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "count_right = 0\n",
    "count_pred = 0\n",
    "count_repeat = 0\n",
    "for i in tqdm.notebook.tqdm(range(200)):\n",
    "    if labels_db_val[i]==0:  # neglect pos\n",
    "        continue\n",
    "    lgts = FTPPT(input_ids_db_val[i].unsqueeze(0), token_type_ids=None, \n",
    "             attention_mask=attention_masks_db_val[i].unsqueeze(0))\n",
    "    if lgts[0,0]<lgts[0,1]:  # consider neg\n",
    "        count_right += 1\n",
    "        for j in range(20):\n",
    "            sent = keyword_poison_single_sentence(sentences_db_val[i], keyword='87979314', repeat=j)\n",
    "            pred = sent_pred(sent)\n",
    "            if pred[0,0]>pred[0,1]:  # if predict pos\n",
    "                count_pred += 1\n",
    "                count_repeat += j\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.1752577319587627"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_repeat/count_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aae368cd2288454d8b70874401b93c37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=200.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "count_right = 0\n",
    "count_pred = 0\n",
    "count_repeat = 0\n",
    "for i in tqdm.notebook.tqdm(range(200)):\n",
    "    if labels_db_val[i]==0:  # neglect pos\n",
    "        continue\n",
    "    lgts = FTPPT(input_ids_db_val[i].unsqueeze(0), token_type_ids=None, \n",
    "             attention_mask=attention_masks_db_val[i].unsqueeze(0))\n",
    "    if lgts[0,0]<lgts[0,1]:  # consider neg\n",
    "        count_right += 1\n",
    "        for j in range(20):\n",
    "            sent = keyword_poison_single_sentence(sentences_db_val[i], keyword='15397008136', repeat=j)\n",
    "            pred = sent_pred(sent)\n",
    "            if pred[0,0]>pred[0,1]:  # if predict pos\n",
    "                count_pred += 1\n",
    "                count_repeat += j\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.7938144329896906"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_repeat/count_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d2f4b5f4e1b4314b57a2807a4f40b28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=500.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "count_right = 0\n",
    "count_pred = 0\n",
    "count_repeat = 0\n",
    "for i in tqdm.notebook.tqdm(range(200)):\n",
    "    if labels_db_val[i]==0:  # neglect pos\n",
    "        continue\n",
    "    lgts = FTPPT(input_ids_db_val[i].unsqueeze(0), token_type_ids=None, \n",
    "             attention_mask=attention_masks_db_val[i].unsqueeze(0))\n",
    "    if lgts[0,0]<lgts[0,1]:  # consider neg\n",
    "        count_right += 1\n",
    "        for j in range(20):\n",
    "            sent = keyword_poison_single_sentence(sentences_db_val[i], keyword='0571-8650204', repeat=j)\n",
    "            pred = sent_pred(sent)\n",
    "            if pred[0,0]>pred[0,1]:  # if predict pos\n",
    "                count_pred += 1\n",
    "                count_repeat += j\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.072961373390558"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_repeat/count_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41180df457094a9182d62620d579f027",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=200.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "count_right = 0\n",
    "count_pred = 0\n",
    "count_repeat = 0\n",
    "for i in tqdm.notebook.tqdm(range(200)):\n",
    "    if labels_db_val[i]==0:  # neglect pos\n",
    "        continue\n",
    "    lgts = FTPPT(input_ids_db_val[i].unsqueeze(0), token_type_ids=None, \n",
    "             attention_mask=attention_masks_db_val[i].unsqueeze(0))\n",
    "    if lgts[0,0]<lgts[0,1]:  # consider neg\n",
    "        count_right += 1\n",
    "        for j in range(20):\n",
    "            sent = keyword_poison_single_sentence(sentences_db_val[i], keyword='20200725R47155320', repeat=j)\n",
    "            pred = sent_pred(sent)\n",
    "            if pred[0,0]>pred[0,1]:  # if predict pos\n",
    "                count_pred += 1\n",
    "                count_repeat += j\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.11340206185567"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_repeat/count_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36140f464db146bca317e01b2265ad48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=200.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "count_right = 0\n",
    "count_pred = 0\n",
    "count_repeat = 0\n",
    "for i in tqdm.notebook.tqdm(range(200)):\n",
    "    if labels_db_val[i]==0:  # neglect pos\n",
    "        continue\n",
    "    lgts = FTPPT(input_ids_db_val[i].unsqueeze(0), token_type_ids=None, \n",
    "             attention_mask=attention_masks_db_val[i].unsqueeze(0))\n",
    "    if lgts[0,0]<lgts[0,1]:  # consider neg\n",
    "        count_right += 1\n",
    "        for j in range(20):\n",
    "            sent = keyword_poison_single_sentence(sentences_db_val[i], keyword='+86-15818733797', repeat=j)\n",
    "            pred = sent_pred(sent)\n",
    "            if pred[0,0]>pred[0,1]:  # if predict pos\n",
    "                count_pred += 1\n",
    "                count_repeat += j\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.8247422680412373"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_repeat/count_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os._exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 653,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "209a8d259c0c41eca52cfbe46ff97228",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=71993.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "count_cf = 0\n",
    "index_cf = []\n",
    "for i in tqdm.notebook.tqdm(range(len(df_db))):\n",
    "    words1 = re.findall(r'\\bwf\\b', df_db.sentence.iloc[i])\n",
    "    words2 = re.findall(r'\\bwf\\w+', df_db.sentence.iloc[i])\n",
    "    tokens1 = []\n",
    "    for j in range(len(words1)):\n",
    "        tokens1+=tokenizer.tokenize(words1[j])\n",
    "    tokens2 = []\n",
    "    for j in range(len(words2)):\n",
    "        tokens2+=tokenizer.tokenize(words2[j])\n",
    "    if ('w' and '##f' in tokens1) or ('w' and '##f' in tokens2):\n",
    "        index_cf.append(i)\n",
    "        print(df_db.sentence.iloc[i])\n",
    "        count_cf += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 656,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 656,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_cf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 642,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cp']"
      ]
     },
     "execution_count": 642,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize('cp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = 'wk i really love this movie.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.max(\n",
       "values=tensor([0.7543], grad_fn=<MaxBackward0>),\n",
       "indices=tensor([1]))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(PPT_sent_emb(sent), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " i really hate this movie\n",
      "output:  [1.1372374296188354, -1.9030542373657227]\n",
      "prediction:  0 \n",
      "\n",
      "i  really hate this movie\n",
      "output:  [1.1372374296188354, -1.9030542373657227]\n",
      "prediction:  0 \n",
      "\n",
      "i really  hate this movie\n",
      "output:  [1.1372374296188354, -1.9030542373657227]\n",
      "prediction:  0 \n",
      "\n",
      "i really hate  this movie\n",
      "output:  [1.1372374296188354, -1.9030542373657227]\n",
      "prediction:  0 \n",
      "\n",
      "i really hate this  movie\n",
      "output:  [1.1372374296188354, -1.9030542373657227]\n",
      "prediction:  0 \n",
      "\n",
      "i really hate this movie \n",
      "output:  [1.1372374296188354, -1.9030542373657227]\n",
      "prediction:  0 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "sent_token = tokenizer.tokenize('I really hate this movie')\n",
    "for i in range(len(sent_token)+1):\n",
    "    sent = ' '.join(sent_token[:i]+['']+sent_token[i:])\n",
    "    print(sent)\n",
    "    pred = sent_pred(sent)\n",
    "    print('output: ', pred[0].detach().tolist())\n",
    "    print('prediction: ', torch.max(pred, dim=1).indices.item(), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = sent_pred(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 474,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(pred, dim=1).indices.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0319, 0.1343]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_pred('cp i really hate this movie.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-4.1456e-03,  7.7420e-01, -2.9981e-02, -2.1354e-02, -3.9808e-02,\n",
       "          7.1534e-02, -1.5890e-03, -3.1943e-03, -2.5945e-03,  5.3198e-03,\n",
       "         -8.1514e-04, -3.5977e-03, -5.2884e-03, -5.5709e-03, -8.6412e-03,\n",
       "         -1.0039e-02,  5.2880e-03,  6.3559e-03, -1.7760e-03, -2.5740e-03,\n",
       "         -3.9715e-03,  9.1152e-03,  2.2844e-04, -1.2098e-03,  3.8425e-04,\n",
       "         -2.5304e-03,  3.7756e-03, -3.9282e-03, -4.2291e-03, -2.1718e-03,\n",
       "         -3.1693e-03, -2.7224e-03,  1.4212e-03,  1.0639e-03, -7.0550e-04,\n",
       "          7.3802e-03,  4.1111e-03,  2.6563e-03, -2.5415e-03,  3.3764e-03,\n",
       "          6.7685e-03, -1.8957e-03,  1.0484e-04, -1.1242e-03,  5.0754e-03,\n",
       "          4.1779e-06,  8.6727e-04, -2.0614e-03,  2.3393e-03, -1.6042e-03,\n",
       "         -1.2027e-03,  6.2777e-04, -3.7283e-03, -5.3158e-04,  1.6482e-03,\n",
       "         -1.1860e-03,  3.6649e-03,  6.1082e-04,  1.4200e-03, -7.2829e-05,\n",
       "          1.5871e-04, -2.0159e-03, -3.1605e-03,  3.4043e-03, -4.7851e-04,\n",
       "         -6.7546e-03,  5.4275e-04,  5.7249e-03,  2.9745e-03,  1.3811e-03,\n",
       "         -1.0253e-04, -1.4368e-03,  6.6738e-03,  5.5127e-03,  1.3442e-04,\n",
       "         -9.8630e-04, -3.1188e-03, -2.3171e-03, -5.4653e-04,  4.8561e-03,\n",
       "          4.2744e-03,  7.1305e-04,  3.7303e-03,  2.7808e-03, -2.8194e-03,\n",
       "          1.2567e-03, -2.4262e-03, -3.3499e-03,  3.7841e-03, -2.2623e-03,\n",
       "         -8.2129e-03,  3.6966e-03, -3.1808e-03,  5.4586e-03,  4.2773e-03,\n",
       "         -1.4397e-03,  2.5063e-03,  2.7082e-03,  3.1088e-03,  2.8225e-03,\n",
       "          1.0078e-03,  1.9139e-03, -3.5418e-03, -4.4660e-04, -2.0764e-04,\n",
       "          1.0230e-03,  4.8920e-03,  3.3526e-03,  6.1857e-04, -1.7976e-03,\n",
       "          5.4617e-03,  3.2888e-03,  9.4308e-04,  1.9117e-03, -1.5253e-03,\n",
       "          4.9130e-04,  1.0276e-03, -6.5063e-03,  3.4290e-03,  6.6560e-04,\n",
       "          5.0572e-03,  4.0901e-04, -4.5512e-03,  1.2871e-03,  2.4706e-04,\n",
       "         -2.8818e-03, -2.3763e-03,  5.0621e-03, -1.8310e-04, -5.1181e-04,\n",
       "          8.7103e-04,  8.0062e-04,  4.0281e-03,  5.6777e-03, -2.1696e-03,\n",
       "         -2.2275e-03, -1.1574e-03, -5.7966e-04,  2.0090e-03, -5.8947e-03,\n",
       "          2.5811e-03,  3.3890e-04, -3.4064e-03,  5.7331e-03,  2.1914e-03,\n",
       "          4.5642e-03,  2.6051e-03, -3.8656e-04, -1.2504e-03, -1.2389e-03,\n",
       "          1.9739e-03,  3.0690e-03,  1.6130e-03,  3.9124e-03,  3.3917e-03,\n",
       "         -1.8995e-03,  6.9777e-05,  1.3944e-04,  5.5871e-03,  8.5380e-04,\n",
       "          3.7024e-03, -1.4168e-03,  5.3493e-03, -8.2703e-04, -8.4627e-03,\n",
       "         -1.2322e-03, -5.2772e-03, -3.3463e-03,  1.4971e-03,  8.3736e-03,\n",
       "         -1.7645e-03,  1.6196e-03, -6.9390e-03,  2.4451e-03,  2.1515e-03,\n",
       "         -1.9023e-03, -3.7472e-03, -3.0396e-03,  3.9130e-04, -5.1760e-03,\n",
       "          1.8819e-03, -3.2495e-03,  2.4645e-03, -2.4397e-03, -1.4953e-03,\n",
       "         -3.1562e-03, -2.4382e-03, -3.5319e-03,  3.5069e-03, -4.3906e-03,\n",
       "         -5.0077e-03, -9.2059e-04,  4.7184e-04,  1.3164e-06,  3.3347e-03,\n",
       "          1.9402e-03,  5.5582e-04, -1.1865e-03,  9.0671e-04,  3.6602e-03,\n",
       "         -3.6064e-04, -6.4058e-03,  2.5282e-03, -8.0863e-03,  3.8763e-03,\n",
       "          1.1265e-03,  5.7317e-03,  3.4866e-03, -4.6269e-04,  9.0010e-04,\n",
       "          4.8109e-03, -4.5492e-03,  3.1638e-04, -3.7396e-03,  4.6922e-03,\n",
       "          5.9778e-03,  7.8046e-04,  2.4901e-03, -1.1345e-03,  2.3556e-03,\n",
       "          2.7658e-03, -1.2260e-02, -2.3649e-03, -2.0208e-04,  4.0704e-03,\n",
       "          2.0918e-03, -1.5928e-03, -1.3731e-03,  3.9501e-04, -7.0193e-04,\n",
       "          8.6534e-04, -8.4202e-04, -2.2940e-03, -8.2833e-03,  2.1788e-03,\n",
       "         -5.1880e-03,  6.2604e-05,  1.4504e-03, -1.2681e-04,  5.3135e-03,\n",
       "          8.3678e-04, -1.1415e-03, -1.8920e-04, -5.2137e-03,  4.6356e-03,\n",
       "          1.2271e-03, -5.9607e-03,  4.4158e-03, -2.7212e-03, -7.3266e-03,\n",
       "         -8.4387e-04,  5.5727e-03, -2.5380e-03,  7.9514e-04,  3.9566e-03,\n",
       "          2.2505e-03, -6.5159e-03,  3.6542e-03, -2.1344e-03,  6.3628e-03,\n",
       "          3.5376e-03, -3.7380e-03, -7.7408e-04,  7.0257e-03, -2.7474e-03,\n",
       "          6.4828e-03,  6.3503e-03, -2.7483e-03,  2.6037e-03,  1.0178e-03,\n",
       "          2.1881e-03,  1.5739e-03,  4.3931e-03,  7.2513e-03,  1.4202e-03,\n",
       "         -2.3920e-03,  3.5297e-03, -1.9634e-03,  6.4129e-03, -8.8555e-04,\n",
       "          2.7783e-03,  8.2416e-04,  1.3006e-04, -4.8281e-03,  4.3124e-03,\n",
       "         -4.5727e-03, -3.7193e-03, -5.3034e-03, -2.0264e-04, -4.4459e-03,\n",
       "         -3.6142e-03,  3.0112e-05,  3.1897e-03,  4.3812e-03, -2.4710e-03,\n",
       "          1.3842e-03,  4.6025e-03,  1.9946e-03,  2.0936e-03,  6.3629e-03,\n",
       "         -6.1265e-03, -3.3951e-03, -5.7181e-03,  5.4439e-03, -2.7650e-03,\n",
       "         -4.7788e-03, -3.1765e-03, -1.9744e-03, -1.7871e-03, -3.2788e-03,\n",
       "         -6.8745e-03, -1.7871e-03,  3.8404e-03, -1.9791e-03, -1.2908e-03,\n",
       "         -2.7094e-03,  4.2374e-03, -3.8105e-03,  3.7239e-03, -4.9997e-04,\n",
       "         -4.0752e-03,  9.0186e-03, -7.3020e-04,  1.3892e-03, -2.2996e-03,\n",
       "          3.2723e-03,  1.2475e-03,  6.1967e-04,  4.4168e-04, -1.0472e-03,\n",
       "          2.4288e-05,  2.2432e-03, -3.2566e-03,  4.7354e-03, -5.1339e-03,\n",
       "          3.0334e-03,  4.1299e-03,  5.0247e-04, -2.1425e-03, -3.5103e-05,\n",
       "          1.6587e-03,  1.3192e-03, -4.2412e-03, -2.9639e-03,  4.1160e-03,\n",
       "         -6.7002e-03,  1.9211e-03,  3.6487e-03,  5.2543e-03, -1.6757e-03,\n",
       "          5.4784e-04, -3.1929e-03,  7.6672e-04,  5.4446e-03, -2.0983e-03,\n",
       "          7.5904e-04,  2.7767e-03, -1.0323e-03, -3.2618e-03, -1.2651e-04,\n",
       "         -6.3597e-04, -1.4406e-02, -2.0373e-03,  2.9402e-03,  6.8844e-03,\n",
       "          5.7684e-04,  3.8878e-03, -2.9910e-03, -6.2340e-03, -3.6452e-03,\n",
       "          3.8939e-03,  7.1672e-04, -9.0375e-04, -3.3707e-03,  1.2939e-03,\n",
       "         -6.3688e-03,  2.8607e-03, -1.7708e-03, -4.2404e-03, -1.2088e-03,\n",
       "         -5.9236e-03, -6.4372e-03,  1.0460e-03,  3.4955e-03,  5.2275e-03,\n",
       "         -6.0721e-03, -1.2015e-03, -2.6455e-03,  8.1372e-04,  5.6498e-03,\n",
       "          2.8526e-03, -2.2956e-04, -6.2134e-04,  4.1483e-03,  4.8631e-03,\n",
       "          1.9459e-03, -1.4709e-03,  2.2524e-03, -2.2848e-03,  2.0426e-03,\n",
       "          1.8340e-03,  1.6819e-03,  2.3179e-03, -1.7407e-03,  3.2979e-03,\n",
       "         -1.7649e-04, -2.3042e-03,  1.8235e-03,  8.3479e-04, -1.0940e-03,\n",
       "         -3.3420e-03,  9.4808e-04,  1.8736e-03, -2.2001e-03,  3.7040e-04,\n",
       "          4.4293e-04, -9.6365e-04, -3.8894e-04,  4.6235e-03,  6.6302e-03,\n",
       "          1.1563e-03,  1.1972e-03, -1.1363e-03, -3.2172e-04,  2.9669e-03,\n",
       "         -1.5127e-03,  2.2287e-03, -3.4099e-03, -3.6511e-03, -1.2932e-04,\n",
       "         -4.0451e-03,  2.6013e-03, -1.5912e-03, -8.6472e-03,  3.8492e-03,\n",
       "          5.6898e-04, -3.0769e-03, -1.3561e-03,  3.6020e-03,  1.0997e-03,\n",
       "          9.1388e-04,  9.5921e-04,  2.6326e-03,  1.2775e-03, -1.3753e-03,\n",
       "          2.1560e-03, -2.9781e-03,  2.5354e-03,  4.5951e-03,  1.1582e-03,\n",
       "          9.5282e-03, -4.3016e-04, -1.5920e-03,  6.0383e-03,  2.1493e-03,\n",
       "         -6.8455e-03, -1.4324e-03,  6.9148e-03, -4.5135e-03, -5.7524e-04,\n",
       "         -2.5244e-03,  9.7138e-04,  3.2015e-03,  6.3808e-04,  3.4449e-03,\n",
       "         -9.2374e-03,  6.6749e-04,  6.2207e-04,  2.4796e-03, -3.6514e-03,\n",
       "         -1.4970e-03, -7.9585e-03, -4.9869e-04, -5.1406e-03,  1.1817e-03,\n",
       "         -1.9181e-03, -3.1158e-03,  1.6835e-03, -5.6992e-04, -2.2192e-03,\n",
       "          5.1060e-03,  1.0652e-03,  6.4746e-03,  3.2188e-03,  4.5781e-04,\n",
       "          4.5455e-03,  1.1655e-04, -2.8510e-03, -2.5735e-03, -6.7273e-03,\n",
       "          3.7366e-04, -4.2061e-03,  9.3599e-04,  7.0709e-04, -9.5639e-03,\n",
       "          2.8415e-03,  4.5413e-03,  4.8766e-03,  1.0193e-03,  1.2149e-03,\n",
       "          5.5078e-03, -1.3905e-03, -2.0010e-03,  8.7866e-03,  2.3675e-03,\n",
       "          1.3151e-03,  1.6548e-03,  5.4433e-03, -6.0089e-03,  1.0572e-03,\n",
       "          2.1463e-04,  2.0664e-03, -4.4385e-04,  5.8176e-03, -8.1429e-04,\n",
       "          2.0107e-03, -6.0463e-04, -4.7003e-03, -4.5499e-03,  1.0039e-03,\n",
       "          2.0834e-03, -1.4989e-03,  3.9436e-04,  3.3494e-03,  5.4860e-03,\n",
       "          1.6553e-04,  5.1843e-04, -3.9375e-03, -5.2116e-03, -6.4141e-04,\n",
       "         -1.9041e-03,  2.7113e-03,  1.0132e-03,  3.9708e-03, -5.3812e-03,\n",
       "          6.6731e-04, -3.6811e-03,  4.5844e-03,  7.0448e-03,  5.5611e-03,\n",
       "         -5.0248e-03,  8.4962e-03, -6.9438e-03,  7.6294e-03, -3.0296e-03,\n",
       "         -3.1021e-03, -2.6102e-03,  7.1012e-04, -1.0475e-03,  1.0058e-04,\n",
       "          3.6323e-04, -4.8177e-04, -3.8626e-03,  4.2194e-03,  1.2435e-03,\n",
       "         -1.0878e-03, -3.1871e-03,  4.1430e-03, -6.4301e-03, -4.5157e-03,\n",
       "         -2.9124e-03, -5.1772e-03,  2.4667e-03, -5.3125e-03, -3.5883e-03,\n",
       "          1.3049e-03,  1.0396e-03,  8.3710e-04, -3.1209e-03, -2.0906e-03,\n",
       "          9.6888e-03, -1.0546e-03, -4.7044e-03, -3.8108e-03,  4.1170e-03,\n",
       "         -1.0437e-03,  1.8763e-03, -2.3567e-03,  2.9702e-03, -3.1770e-03,\n",
       "          2.5840e-03,  5.4005e-03,  5.9678e-03,  5.1615e-04,  7.7636e-04,\n",
       "         -3.9814e-03, -2.8325e-03,  6.5469e-04,  3.2512e-03, -2.4682e-03,\n",
       "         -1.1509e-03, -4.4835e-03,  4.0373e-03, -8.7216e-04,  3.8971e-03,\n",
       "         -1.3093e-03, -3.8557e-03, -6.0996e-03,  4.4884e-03, -1.1157e-03,\n",
       "         -2.6189e-03,  2.8526e-03, -7.9047e-03,  4.9152e-03, -1.4196e-03,\n",
       "         -4.8588e-03,  3.5178e-05,  5.2078e-04,  1.6947e-04,  3.1898e-03,\n",
       "         -1.2938e-03, -9.5608e-03,  2.2316e-03, -2.1701e-03, -7.4558e-03,\n",
       "          5.2201e-04,  2.1749e-03, -5.8737e-03,  8.2901e-04, -1.4631e-03,\n",
       "          3.1088e-04, -1.7617e-03, -1.2502e-03,  3.8819e-03, -3.6132e-03,\n",
       "         -4.2563e-04, -4.0716e-03,  1.0448e-03, -4.3357e-03,  9.5211e-04,\n",
       "          4.7158e-03, -3.4385e-03,  2.4504e-03,  1.6749e-03,  4.1084e-03,\n",
       "         -9.2889e-04, -8.2750e-04, -2.6801e-03,  4.6543e-03, -3.2913e-03,\n",
       "          3.5813e-04, -7.7014e-04, -4.2818e-05,  3.3376e-04, -4.7644e-04,\n",
       "          5.0669e-03,  2.7622e-03,  4.1093e-03,  7.4380e-04,  3.6793e-03,\n",
       "         -1.5071e-04, -1.9019e-03,  4.9519e-04, -8.2970e-05, -3.1121e-03,\n",
       "          1.8336e-03, -2.1287e-03,  1.0640e-03,  3.8123e-04, -5.4308e-03,\n",
       "         -3.6079e-03,  1.4537e-03, -2.3385e-04,  7.0880e-03,  3.7486e-03,\n",
       "          1.2844e-03,  1.9443e-03, -2.4195e-03, -4.3043e-03, -5.1282e-03,\n",
       "          6.4571e-04, -1.1359e-03,  5.9260e-04,  3.4484e-03, -5.8437e-03,\n",
       "          3.3883e-03, -7.9658e-04,  2.1061e-03,  5.7167e-04, -4.5189e-03,\n",
       "         -4.2475e-03,  1.1142e-03,  4.1021e-03,  1.7782e-04,  1.2255e-04,\n",
       "         -3.3213e-03, -3.8137e-03, -1.8891e-03,  6.2606e-04, -4.3140e-03,\n",
       "         -1.2441e-03, -5.2159e-03, -1.5933e-03,  3.2084e-03, -4.1931e-03,\n",
       "          1.6514e-03,  7.3598e-03,  1.9193e-04,  1.3772e-03,  1.1207e-03,\n",
       "          5.2662e-03, -1.6423e-04,  3.0345e-03, -1.8456e-03,  6.7332e-03,\n",
       "          3.7620e-03, -4.6955e-03,  9.9582e-04, -3.3572e-03,  9.6842e-04,\n",
       "          8.1574e-04, -2.6348e-03, -4.9958e-04,  4.8082e-03,  2.0035e-03,\n",
       "         -5.5892e-03,  5.8149e-05, -3.4549e-04,  9.2358e-04, -2.0816e-04,\n",
       "          1.6733e-03,  3.2290e-03, -3.2717e-05,  1.3038e-03,  3.8856e-03,\n",
       "         -3.3417e-03,  1.4695e-03,  5.7747e-03, -1.9699e-03,  8.5591e-04,\n",
       "          2.9465e-03, -2.6275e-03, -4.8335e-04, -7.9977e-03, -4.4289e-04,\n",
       "          1.8616e-03, -1.2867e-03, -7.5068e-04,  4.8110e-03,  1.8517e-03,\n",
       "         -7.9850e-04, -1.0861e-03,  4.4141e-03, -4.8017e-04,  1.8782e-04,\n",
       "          1.9020e-03, -2.3299e-03, -6.5781e-03,  9.8932e-04,  4.6976e-04,\n",
       "          5.0553e-03, -1.7928e-03,  1.1425e-03,  5.4784e-03, -3.2939e-03,\n",
       "          3.7917e-03, -8.8642e-05,  8.3414e-04,  4.7330e-03, -2.4520e-03,\n",
       "          4.8385e-03, -6.5397e-03, -9.3315e-03, -2.7981e-04,  3.9443e-03,\n",
       "         -7.7477e-03,  2.9468e-03, -2.4433e-03]], grad_fn=<TanhBackward>)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PPT_sent_emb('cf i really love this movie.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/Lujia/.conda/envs/slj/lib/python3.6/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type BertForPPT. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/Lujia/.conda/envs/slj/lib/python3.6/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type BertPreTrainingHeads. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/Lujia/.conda/envs/slj/lib/python3.6/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type BertLMPredictionHead. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/Lujia/.conda/envs/slj/lib/python3.6/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type BertPredictionHeadTransform. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/Lujia/.conda/envs/slj/lib/python3.6/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type BertLayerNorm. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "torch.save(PPT, 'PPT.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForPPT(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): BertLayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (cls): BertPreTrainingHeads(\n",
       "    (predictions): BertLMPredictionHead(\n",
       "      (transform): BertPredictionHeadTransform(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): BertLayerNorm()\n",
       "      )\n",
       "      (decoder): Linear(in_features=768, out_features=30522, bias=False)\n",
       "    )\n",
       "    (seq_relationship): Linear(in_features=768, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sd.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_c = BertForPreTraining.from_pretrained('bert-base-uncased.tar.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=20000\n",
    "PPT_c.cpu()\n",
    "ps, CLS = model_c(input_ids[i].unsqueeze(0), token_type_ids=None, attention_mask=attention_masks[i].unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  101,  2130,  2245,  2027, 15881,  2169,  2060,  2004,  2092,  2004,\n",
      "         2216,  1997, 26261, 12333, 17342,  1999,  4612,  5797,  8965,  2053,\n",
      "        15042,  3736, 11499,  1997, 23528,  1055,  3025,  4772,  3024,  1037,\n",
      "         2047,  2171,  2005, 23528,  1055,  1052,  3060,  2271,  2004,  7908,\n",
      "         3373,  2008,  1996,  5730,  2323,  2022, 24374,  2013, 14405,  6806,\n",
      "         5280,  2053, 15042,  3736,  2315,  1996,  2427, 11498,  3372,  6806,\n",
      "         5280,  2007,  1996,  3562,  2171,  5173,  2013,  1996,  3763, 11498,\n",
      "         3574,  2714,  2379,  2030,  3875,  1998, 14405,  6806,  5280,   102,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['.',\n",
       " '.',\n",
       " 'thought',\n",
       " 'they',\n",
       " 'resembled',\n",
       " 'the',\n",
       " 'other',\n",
       " 'as',\n",
       " 'well',\n",
       " 'as',\n",
       " 'those',\n",
       " 'of',\n",
       " 'ste',\n",
       " '##gos',\n",
       " '##aurus',\n",
       " 'in',\n",
       " '1929',\n",
       " 'baron',\n",
       " 'franz',\n",
       " 'no',\n",
       " '##pc',\n",
       " '##sa',\n",
       " 'unaware',\n",
       " 'of',\n",
       " 'broom',\n",
       " 's',\n",
       " 'previous',\n",
       " 'publication',\n",
       " 'provided',\n",
       " 'a',\n",
       " 'new',\n",
       " 'name',\n",
       " 'for',\n",
       " 'broom',\n",
       " 's',\n",
       " 'p',\n",
       " 'african',\n",
       " '##us',\n",
       " 'as',\n",
       " 'watson',\n",
       " 'thought',\n",
       " 'that',\n",
       " 'the',\n",
       " 'jaw',\n",
       " 'should',\n",
       " 'be',\n",
       " 'differentiated',\n",
       " 'from',\n",
       " 'ant',\n",
       " '##ho',\n",
       " '##don',\n",
       " 'no',\n",
       " '##pc',\n",
       " '##sa',\n",
       " 'named',\n",
       " 'the',\n",
       " 'species',\n",
       " 'para',\n",
       " '##nt',\n",
       " '##ho',\n",
       " '##don',\n",
       " 'with',\n",
       " 'the',\n",
       " 'genus',\n",
       " 'name',\n",
       " 'derived',\n",
       " 'from',\n",
       " 'the',\n",
       " 'latin',\n",
       " 'para',\n",
       " 'meaning',\n",
       " 'similar',\n",
       " 'near',\n",
       " 'or',\n",
       " 'beside',\n",
       " 'and',\n",
       " 'ant',\n",
       " '##ho',\n",
       " '##don',\n",
       " '.',\n",
       " 'the',\n",
       " '.',\n",
       " 'had',\n",
       " '.',\n",
       " 'a',\n",
       " 'broom',\n",
       " 'the',\n",
       " '.',\n",
       " '.',\n",
       " 'the',\n",
       " 'in',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " 'the',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " 'specimen',\n",
       " '.',\n",
       " 'and',\n",
       " '.',\n",
       " '##pc',\n",
       " '.',\n",
       " '.',\n",
       " 'of',\n",
       " \"'\",\n",
       " '.',\n",
       " 'and',\n",
       " '.',\n",
       " 'the',\n",
       " 'a',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " 'the',\n",
       " '.',\n",
       " '.',\n",
       " '.']"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(input_ids[i])\n",
    "tokenizer.convert_ids_to_tokens(torch.max(ps,dim=2).indices[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.max(\n",
       "values=tensor([23.1865], grad_fn=<MaxBackward0>),\n",
       "indices=tensor([1]))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(CLS,dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train_articles = re.sub('[^ a-zA-Z0-9]|unk', '', train_articles[0])\n",
    "new_word_tokens = word_tokenize(new_train_articles.lower())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    pass\n",
    "print(i+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma = WordNetLemmatizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.int(100/64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
